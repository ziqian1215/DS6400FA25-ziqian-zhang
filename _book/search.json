[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML1 Portoflio",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-performance-measures.html",
    "href": "01-performance-measures.html",
    "title": "2  Measures of model performance",
    "section": "",
    "text": "2.1 Random Forest\nLet the dataset be\n\\[\nD=\\{(x_i,y_i)\\}_{i=1}^N,\\quad x_i\\in\\mathbb{R}^p,\\ y_i\\in\\{0,1\\}.\n\\]\nA Random Forest is an ensemble of \\(B\\) classification trees \\(\\{T_b\\}_{b=1}^B\\). Each tree is trained using:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Measures of model performance</span>"
    ]
  },
  {
    "objectID": "01-performance-measures.html#random-forest",
    "href": "01-performance-measures.html#random-forest",
    "title": "2  Measures of model performance",
    "section": "",
    "text": "Note. Random Forest can be applied to both classification and regression tasks.\nIn this chapter, we focus only on the classification setting, where the target variable \\(y_i \\in \\{0,1\\}\\) and each tree outputs class probabilities. The formulas and evaluation metrics described below are all for classification.\n\n\n\n\n\n2.1.1 Bootstrap sampling\nDraw a bootstrap sample \\(D_b^*\\) from \\(D\\) by sampling \\(N\\) observations with replacement.\n\\[\nD_b^* = \\{(x_{i_1},y_{i_1}),\\dots,(x_{i_N},y_{i_N})\\},\\quad i_j \\sim \\text{Unif}\\{1,\\dots,N\\}.\n\\]\n\n\n2.1.2 Random feature selection\nAt each internal node, randomly select \\(m_{\\text{try}}\\) features from the full set \\(\\{1,\\dots,p\\}\\).\nAmong the possible splits on these features, choose the one that maximizes the impurity decrease:\n\\[\n\\Delta \\mathcal{I}\n= \\mathcal{I}(\\text{parent})\n-\\sum_{c\\in\\{\\text{left},\\text{right}\\}}\n\\frac{n_c}{n_{\\text{parent}}}\\mathcal{I}(c).\n\\]\nA common impurity measure is the Gini impurity:\n\\[\n\\mathcal{I}_{\\text{Gini}}(\\text{node}) = 1 - \\sum_{k\\in\\{0,1\\}}\\hat{\\pi}_k^2,\n\\qquad\n\\hat{\\pi}_k = \\frac{1}{n_{\\text{node}}}\\sum_{i\\in\\text{node}}\\mathbb{1}(y_i=k).\n\\]\n\n\n2.1.3 Prediction\nEach tree \\(T_b\\) produces a probability estimate for the positive class:\n\\[\nT_b(x) = \\hat{p}_b(y=1\\mid x).\n\\]\nThe Random Forest averages these:\n\\[\n\\hat{p}(x) = \\frac{1}{B}\\sum_{b=1}^B T_b(x),\n\\qquad\n\\hat{y}(x) = \\mathbb{1}\\{\\hat{p}(x) \\ge 0.5\\}.\n\\]\nThis reduces variance and improves generalization.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Measures of model performance</span>"
    ]
  },
  {
    "objectID": "01-performance-measures.html#ten-fold-cross-validation",
    "href": "01-performance-measures.html#ten-fold-cross-validation",
    "title": "2  Measures of model performance",
    "section": "2.2 Ten-Fold Cross-Validation",
    "text": "2.2 Ten-Fold Cross-Validation\nWe split \\(D\\) into 10 disjoint folds of (approximately) equal size:\n\\[\nD = \\bigcup_{k=1}^{10} D_k,\\qquad D_i\\cap D_j = \\varnothing \\ (i\\neq j).\n\\]\nFor each fold \\(k\\):\n\nTraining set: \\(D^{(-k)} = D \\setminus D_k\\)\n\nTest set: \\(D_k\\)\n\nTrain the Random Forest on \\(D^{(-k)}\\) to obtain a fitted model \\(f_k(\\cdot)\\).\nUse it to produce predictions on all samples in \\(D_k\\):\n\\[\n\\{(\\hat{p}_k(x_i),\\hat{y}_k(x_i),y_i)\\}_{(x_i,y_i)\\in D_k}.\n\\]\nLet \\(M(\\cdot)\\) be a performance metric (Accuracy, F1, etc).\nDefine the fold-wise performance as\n\\[\nM_k = M\\big(\\{(\\hat{y}_k(x_i),y_i)\\}_{(x_i,y_i)\\in D_k}\\big).\n\\]\nThe cross-validated estimate is\n\\[\n\\overline{M} = \\frac{1}{10}\\sum_{k=1}^{10} M_k.\n\\]\n\n2.2.1 Standard error of the 10-fold CV mean (derivation)\nLet \\(M_1,\\dots,M_n\\) be the performance values (e.g., accuracy, F1, AUC, or MSE) computed on each of the \\(n\\) CV folds. For 10-fold CV, \\(n=10\\).\nStep 1 (fold mean).\nDefine the mean across folds \\[\n\\overline{M} \\;=\\; \\frac{1}{n}\\sum_{k=1}^n M_k \\, .\n\\]\nStep 2 (sample variance across folds).\nUse the unbiased sample variance with Bessel’s correction \\[\ns^2 \\;=\\; \\frac{1}{n-1}\\sum_{k=1}^n \\big(M_k-\\overline{M}\\big)^2 \\, .\n\\]\nStep 3 (standard error of the mean).\nThe standard error of the mean is the standard deviation of \\(\\overline{M}\\).\nWe estimate it by plugging in the sample standard deviation \\(s\\): \\[\n\\widehat{\\mathrm{SE}}(\\overline{M}) \\;=\\; \\frac{s}{\\sqrt{n}}\n\\;=\\; \\sqrt{\\frac{s^2}{n}} \\, .\n\\]\nStep 4 (combine Steps 2 and 3).\nSubstitute \\(s^2\\) from Step 2: \\[\n\\widehat{\\mathrm{SE}}(\\overline{M})\n\\;=\\;\n\\sqrt{ \\frac{1}{n}\\cdot \\frac{1}{n-1} \\sum_{k=1}^n \\big(M_k-\\overline{M}\\big)^2 }\n\\;=\\;\n\\sqrt{ \\frac{1}{n(n-1)} \\sum_{k=1}^n \\big(M_k-\\overline{M}\\big)^2 } \\, .\n\\]\nFor 10-fold CV (\\(n=10\\)), \\[\n\\widehat{\\mathrm{SE}}(\\overline{M})\n\\;=\\;\n\\sqrt{ \\frac{1}{10(10-1)} \\sum_{k=1}^{10} \\big(M_k-\\overline{M}\\big)^2 } \\, .\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Measures of model performance</span>"
    ]
  },
  {
    "objectID": "01-performance-measures.html#performance-metrics",
    "href": "01-performance-measures.html#performance-metrics",
    "title": "2  Measures of model performance",
    "section": "2.3 Performance Metrics",
    "text": "2.3 Performance Metrics\nOn a given test set, define confusion matrix counts:\n\\[\nTP=\\sum \\mathbb{1}\\{y_i=1,\\hat{y}_i=1\\},\\quad\nFP=\\sum \\mathbb{1}\\{y_i=0,\\hat{y}_i=1\\},\\quad\nTN=\\sum \\mathbb{1}\\{y_i=0,\\hat{y}_i=0\\},\\quad\nFN=\\sum \\mathbb{1}\\{y_i=1,\\hat{y}_i=0\\}.\n\\]\nThen:\n\\[\n\\mathrm{Accuracy}=\\frac{TP+TN}{TP+TN+FP+FN},\n\\qquad\n\\mathrm{Precision}=\\frac{TP}{TP+FP},\n\\qquad\n\\mathrm{Recall}=\\frac{TP}{TP+FN},\n\\]\n\\[\n\\mathrm{F1}=2\\frac{\\mathrm{Precision}\\cdot\\mathrm{Recall}}{\\mathrm{Precision}+\\mathrm{Recall}}.\n\\]\nAUC is the area under the ROC curve:\n\\[\n\\mathrm{AUC} = \\Pr(S^+ &gt; S^-)\n\\]\nwhere \\(S^+\\) and \\(S^-\\) are the scores from randomly drawn positive and negative examples.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Measures of model performance</span>"
    ]
  },
  {
    "objectID": "01-performance-measures.html#combining-rf-and-cv",
    "href": "01-performance-measures.html#combining-rf-and-cv",
    "title": "2  Measures of model performance",
    "section": "2.4 Combining RF and CV",
    "text": "2.4 Combining RF and CV\nFor each fold \\(k\\), compute the metrics on the test set \\(D_k\\).\nFor example, cross-validated accuracy is\n\\[\n\\overline{\\mathrm{Acc}} = \\frac{1}{10}\\sum_{k=1}^{10}\n\\frac{TP_k+TN_k}{TP_k+TN_k+FP_k+FN_k}.\n\\]\nDo the same for Precision, Recall, F1, and AUC.\nThese \\(\\overline{M}\\) values represent the estimated generalization performance of the Random Forest model.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Measures of model performance</span>"
    ]
  },
  {
    "objectID": "01-performance-measures.html#simulation-study",
    "href": "01-performance-measures.html#simulation-study",
    "title": "2  Measures of model performance",
    "section": "2.5 Simulation study",
    "text": "2.5 Simulation study\nThe following part shows the simulation study comparing LOO vs 10-fold CV for LDA & Random Forest.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Measures of model performance</span>"
    ]
  },
  {
    "objectID": "01-performance-measures.html#data-generation-model",
    "href": "01-performance-measures.html#data-generation-model",
    "title": "2  Measures of model performance",
    "section": "2.6 Data generation model",
    "text": "2.6 Data generation model\nWe simulate i.i.d. data \\(\\{(\\mathbf{T}_i, Y_i)\\}_{i=1}^n\\) as:\n\\[\nY_i \\sim \\mathrm{Bin}(1, 0.5), \\qquad\n\\mathbf{T}_i \\mid Y_i = y_i \\;\\sim\\; N_2\\!\\left(\n\\begin{bmatrix}\ny_i - \\tfrac{1}{2} \\\\\n0\n\\end{bmatrix},\n\\, I_2\n\\right),\n\\]\nwhere \\(I_2\\) is the \\(2\\times2\\) identity and \\(N_2\\) denotes the bivariate normal. Equivalently, conditional on \\(Y_i\\), the two features are independent with unit variance;\nthe first feature’s mean shifts by \\(y_i-0.5\\), making it informative for \\(Y\\).\n\n2.6.1 R implementation of data generation\n\n# Data generation function\ngenerate_data &lt;- function(N, seed = NULL) {\n  if (!is.null(seed)) set.seed(seed)\n  y  &lt;- rbinom(N, size = 1, prob = 0.5)                 # Bernoulli(0.5)\n  x1 &lt;- rnorm(N, mean = y - 0.5, sd = 1)                # mean depends on y\n  x2 &lt;- rnorm(N, mean = 0,       sd = 1)                # mean 0, unit sd\n  data.frame(\n    x1 = x1,\n    x2 = x2,\n    y  = factor(y, levels = c(0,1))\n  )\n}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Measures of model performance</span>"
    ]
  },
  {
    "objectID": "01-performance-measures.html#methods-evaluated",
    "href": "01-performance-measures.html#methods-evaluated",
    "title": "2  Measures of model performance",
    "section": "2.7 Methods evaluated",
    "text": "2.7 Methods evaluated\nWe evaluate two classifiers:\n\nLinear Discriminant Analysis (LDA)\nRandom Forest (RF)\n\nEach model’s generalization error is estimated using two cross-validation approaches:\n\nLeave-one-out cross-validation (LOO-CV):\n\\[\n\\widehat{R}_{\\text{LOO}} = \\frac{1}{n} \\sum_{i=1}^n \\ell(y_i, \\widehat{f}^{(-i)}(x_i)),\n\\] where \\(\\widehat{f}^{(-i)}\\) is trained without observation \\(i\\).\n10-fold cross-validation:\n\\[\n\\widehat{R}_{10\\text{-fold}} =\n\\frac{1}{10}\\sum_{k=1}^{10}\\frac{1}{|\\mathcal{I}_k|}\\sum_{i\\in\\mathcal{I}_k} \\ell(y_i,\\widehat{f}^{(-k)}(x_i)),\n\\] where folds \\(\\mathcal{I}_1,\\dots,\\mathcal{I}_{10}\\) partition the sample.\n\nThe loss function \\(\\ell\\) is 0–1 loss (misclassification error).\nWe then compare each estimator to the true error (approximated using a very large independent test set) and summarize performance using:\n\nBias: mean difference between estimated CV error and true error.\n\nVariance: variability of CV estimates across Monte Carlo replications.\n\nMSE: mean squared error, decomposed as\n\\[\n\\text{MSE} = (\\text{Bias})^2 + \\text{Variance}.\n\\]\n\n\n2.7.1 Libraries\n\nsuppressPackageStartupMessages({\n  library(MASS)          # for LDA\n  library(randomForest)  # for Random Forest\n  library(dplyr)\n  library(tibble)\n  library(purrr)\n  library(ggplot2)\n})\n\n\n\n2.7.2 Stratified folds, model wrappers, and error computation\n\nmake_stratified_folds &lt;- function(y, K = 10, seed = 1) {\n  set.seed(seed)\n  y &lt;- factor(y, levels = c(\"0\",\"1\"))\n  folds &lt;- integer(length(y))\n  for (lvl in levels(y)) {\n    idx &lt;- which(y == lvl)\n    idx &lt;- sample(idx)\n    splits &lt;- cut(seq_along(idx), breaks = K, labels = FALSE)\n    folds[idx] &lt;- splits\n  }\n  folds\n}\n\npredict_model &lt;- function(model_type, train_df, test_df, y_col = \"y\",\n                          rf_ntree = 500, rf_mtry = NULL) {\n  if (!is.factor(train_df[[y_col]])) train_df[[y_col]] &lt;- factor(train_df[[y_col]], levels = c(\"0\",\"1\"))\n  if (!is.factor(test_df[[y_col]]))  test_df[[y_col]]  &lt;- factor(test_df[[y_col]],  levels = c(\"0\",\"1\"))\n\n  if (model_type == \"lda\") {\n    fit &lt;- MASS::lda(as.formula(paste(y_col, \"~ .\")), data = train_df)\n    post &lt;- predict(fit, newdata = test_df)\n    pred &lt;- factor(ifelse(post$posterior[, \"1\"] &gt;= 0.5, \"1\", \"0\"), levels = c(\"0\",\"1\"))\n  } else if (model_type == \"rf\") {\n    if (is.null(rf_mtry)) rf_mtry &lt;- floor(sqrt(ncol(train_df) - 1))\n    fit &lt;- randomForest::randomForest(as.formula(paste(y_col, \"~ .\")),\n                                      data = train_df, ntree = rf_ntree, mtry = rf_mtry)\n    pred &lt;- predict(fit, newdata = test_df, type = \"response\")\n  } else stop(\"Unknown model_type\")\n\n  pred\n}\n\ncompute_error &lt;- function(y_true, pred) {\n  mean(pred != y_true)\n}\n\n\n\n2.7.3 Cross-validation evaluators\n\nevaluate_loo &lt;- function(df, model_type, rf_ntree = 500, rf_mtry = NULL) {\n  n &lt;- nrow(df)\n  errs &lt;- numeric(n)\n  for (i in seq_len(n)) {\n    test_idx  &lt;- i\n    train_idx &lt;- setdiff(seq_len(n), i)\n    train_df  &lt;- df[train_idx, , drop = FALSE]\n    test_df   &lt;- df[test_idx,  , drop = FALSE]\n    pred &lt;- predict_model(model_type, train_df, test_df, rf_ntree = rf_ntree, rf_mtry = rf_mtry)\n    errs[i] &lt;- compute_error(test_df$y, pred)\n  }\n  mean(errs)\n}\n\nevaluate_kfold &lt;- function(df, model_type, K = 10, seed = 1, rf_ntree = 500, rf_mtry = NULL) {\n  folds &lt;- make_stratified_folds(df$y, K = K, seed = seed)\n  errs &lt;- numeric(K)\n  for (k in seq_len(K)) {\n    test_idx  &lt;- which(folds == k)\n    train_idx &lt;- which(folds != k)\n    train_df  &lt;- df[train_idx, , drop = FALSE]\n    test_df   &lt;- df[test_idx,  , drop = FALSE]\n    pred &lt;- predict_model(model_type, train_df, test_df, rf_ntree = rf_ntree, rf_mtry = rf_mtry)\n    errs[k] &lt;- compute_error(test_df$y, pred)\n  }\n  mean(errs)\n}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Measures of model performance</span>"
    ]
  },
  {
    "objectID": "01-performance-measures.html#true-error-and-monte-carlo-evaluation",
    "href": "01-performance-measures.html#true-error-and-monte-carlo-evaluation",
    "title": "2  Measures of model performance",
    "section": "2.8 True error and Monte Carlo evaluation",
    "text": "2.8 True error and Monte Carlo evaluation\nTo evaluate the accuracy of cross-validation estimators, we need a benchmark:\nthe true generalization error of each model.\nFor a fitted classifier \\(f\\), the true error is defined as \\[\nR(f) = \\Pr\\{ f(\\mathbf{X}) \\neq Y \\},\n\\] where \\((\\mathbf{X}, Y)\\) is a new independent draw from the same distribution.\nSince this probability cannot be computed exactly, we approximate it using a very large independent test set (e.g., \\(N_\\text{test} = 50{,}000\\)).\nWe train the model on the entire observed dataset, then evaluate its misclassification rate on the test set.\n\n\n2.8.1 Function to estimate true error\n\nestimate_true_error &lt;- function(model_type, train_df, N_test = 50000,\n                                rf_ntree = 500, rf_mtry = NULL) {\n  # Generate large independent test set\n  test_df &lt;- generate_data(N_test)\n  pred &lt;- predict_model(model_type, train_df, test_df, rf_ntree = rf_ntree, rf_mtry = rf_mtry)\n  compute_error(test_df$y, pred)\n}\n\n\none_replication &lt;- function(N, seed = NULL) {\n  if (!is.null(seed)) set.seed(seed)\n  df &lt;- generate_data(N)\n\n  # LDA\n  lda_loo   &lt;- evaluate_loo(df, \"lda\")\n  lda_k10   &lt;- evaluate_kfold(df, \"lda\", K = 10, seed = 1)\n  lda_true  &lt;- estimate_true_error(\"lda\", df)\n\n  # RF\n  rf_loo    &lt;- evaluate_loo(df, \"rf\")\n  rf_k10    &lt;- evaluate_kfold(df, \"rf\", K = 10, seed = 1)\n  rf_true   &lt;- estimate_true_error(\"rf\", df)\n\n  tibble::tibble(\n    N        = N,\n    model    = c(\"LDA\",\"LDA\",\"RF\",\"RF\"),\n    method   = c(\"LOO\",\"10-fold\",\"LOO\",\"10-fold\"),\n    est_err  = c(lda_loo, lda_k10, rf_loo, rf_k10),\n    true_err = c(lda_true, lda_true, rf_true, rf_true)\n  )\n}\n\n\nset.seed(20250925)\none_replication(40)\n\n# A tibble: 4 × 5\n      N model method  est_err true_err\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1    40 LDA   LOO       0.325    0.323\n2    40 LDA   10-fold   0.285    0.323\n3    40 RF    LOO       0.425    0.333\n4    40 RF    10-fold   0.38     0.333",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Measures of model performance</span>"
    ]
  },
  {
    "objectID": "01-performance-measures.html#simulation-setup",
    "href": "01-performance-measures.html#simulation-setup",
    "title": "2  Measures of model performance",
    "section": "2.9 Simulation Setup",
    "text": "2.9 Simulation Setup\nThe code below runs a Monte Carlo simulation across multiple sample sizes. It defines a set of sample sizes (Ns) and the number of replications (R=20) to run for each, sets a random seed for reproducibility, and initializes an empty list to store results. For each sample size (N) and replication (r), the function one_replication(N) is called to perform a single experiment, the output is tagged with the replication index, and then appended to the results list. In the end, results_list contains all simulated outcomes across every combination of sample size and replication, providing the data needed for further summarization and analysis.\n\nNs &lt;- c( 20, 30, 40, 50, 75, 100)\nR  &lt;- 20  # increase for tighter Monte Carlo precision\nset.seed(123)\nresults_list &lt;- list()\n\nfor (N in Ns) {\n  for (r in 1:R) {\n    res &lt;- one_replication(N)\n    res$rep &lt;- r\n    results_list &lt;- append(results_list, list(res))\n  }\n}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Measures of model performance</span>"
    ]
  },
  {
    "objectID": "01-performance-measures.html#summary",
    "href": "01-performance-measures.html#summary",
    "title": "2  Measures of model performance",
    "section": "2.10 Summary",
    "text": "2.10 Summary\n\nresults &lt;- dplyr::bind_rows(results_list)\nggplot(results, aes(x = factor(N), y = est_err, color = method)) +\n  geom_boxplot(outlier.alpha = 0.3, position = position_dodge(width = 0.8)) +\n  stat_summary(fun = mean, geom = \"line\", aes(group = method),\n               position = position_dodge(width = 0.8)) +\n  stat_summary(fun = mean, geom = \"point\", aes(group = method),\n               position = position_dodge(width = 0.8), size = 2) +\n  facet_wrap(~ model) +\n  labs(\n    title = \"Distribution of CV Estimates vs. True Error\",\n    x = \"Sample size N\",\n    y = \"Estimated error rate\",\n    color = \"CV Method\"\n  ) +\n  theme_minimal(base_size = 12)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Measures of model performance</span>"
    ]
  },
  {
    "objectID": "02-partial-effect-plots.html",
    "href": "02-partial-effect-plots.html",
    "title": "3  Model Visualization",
    "section": "",
    "text": "3.1 Data Generation\nWe generate data from the model:\n\\[\nE[Y \\mid X] = \\sqrt{5}\\,\\sigma(X_1 + X_3) + \\sqrt{5}\\,\\sigma(X_2)\\,X_3, \\quad\nV[Y \\mid X]=1,\n\\]\nwhere \\(X_1,X_2 \\sim \\mathcal N(0,1)\\), \\(X_3 \\sim \\text{Bernoulli}(0.4)\\), and \\(\\sigma(x)=1/(1+e^{-x})\\).\nlibrary(tidyverse)\n\nset.seed(42)\nn  &lt;- 2000\nX1 &lt;- rnorm(n)\nX2 &lt;- rnorm(n)\nX3 &lt;- rbinom(n, 1, 0.4)\n\nsigmoid &lt;- function(x) 1 / (1 + exp(-x))\nmu &lt;- sqrt(5) * sigmoid(X1 + X3) + sqrt(5) * sigmoid(X2) * X3\nY  &lt;- mu + rnorm(n, 0, 1)\n\ndat &lt;- tibble(X1, X2, X3 = factor(X3), Y)\nhead(dat)\n\n# A tibble: 6 × 4\n      X1     X2 X3        Y\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n1  1.37   0.251 0      1.85\n2 -0.565 -0.278 0      1.78\n3  0.363 -1.72  0      1.63\n4  0.633 -2.01  0      1.32\n5  0.404 -1.29  0      1.01\n6 -0.106  0.366 1      2.79",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Visualization</span>"
    ]
  },
  {
    "objectID": "02-partial-effect-plots.html#model-linear-regression-with-natural-splines",
    "href": "02-partial-effect-plots.html#model-linear-regression-with-natural-splines",
    "title": "3  Model Visualization",
    "section": "3.2 Model: Linear Regression with Natural Splines",
    "text": "3.2 Model: Linear Regression with Natural Splines\nWe model a flexible, nonlinear relationship by replacing raw predictors with natural cubic spline bases.\nLet \\(f_1(\\cdot)\\) and \\(f_2(\\cdot)\\) be smooth functions represented by spline bases. We fit\n\\[\nY \\;=\\; \\beta_0 \\;+\\; f_1(X_1)\\;+\\; f_2(X_2)\\\\;+\\; \\gamma\\,\\mathbb{1}(X_3)\\;+\\; \\varepsilon,\n\\quad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2).\n\\]\n\nns(X1, df=5) and ns(X2, df=5) are natural spline bases with 5 degrees of freedom.\n\nns(X3, df=3) effectively lets the model represent it smoothly if treated as numeric, but it’s basically equivalent to a categorical main effect.\n\n\nlibrary(splines)\nlm_spline &lt;- lm(\n  Y ~ ns(X1, df = 5) + ns(X2, df = 5) + ns(as.numeric(X3), df = 3),\n  data = dat\n)\n\nWarning in ns(as.numeric(X3), df = 3): shoving 'interior' knots matching\nboundary knots to inside",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Visualization</span>"
    ]
  },
  {
    "objectID": "02-partial-effect-plots.html#partial-effect-plots",
    "href": "02-partial-effect-plots.html#partial-effect-plots",
    "title": "3  Model Visualization",
    "section": "3.3 Partial Effect Plots",
    "text": "3.3 Partial Effect Plots\nWhat is a partial effect plot?\nFor an lm with splines, a partial effect (component-plus-residual) plot shows how a predictor contributes to the fitted values after adjusting for all other terms. We compute the term contribution for a variable (from predict(mod, type = \"terms\")), add the model residuals to form partial residuals, and plot those against the raw predictor. A smooth (LOESS) curve with a confidence band visualizes the nonlinear effect learned by the spline.\n\n3.3.1 Helper functions\nThe two helpers below produce ggplot figures: - simple_peplot() — single numeric predictor - simple_peplot_by() — numeric predictor split into separate curves by a factor (e.g., X3)\n\nsimple_peplot &lt;- function(mod, var, data, span = 0.8) {\n  tt &lt;- predict(mod, type = \"terms\")\n  cn &lt;- colnames(tt)\n  pick &lt;- grep(paste0(\"\\\\b\", var, \"\\\\b\"), cn)\n  if (length(pick) == 0L) stop(\"No model terms matched '\", var, \"'.\")\n  term_contrib &lt;- rowSums(tt[, pick, drop = FALSE])\n  pr &lt;- term_contrib + residuals(mod)\n  x  &lt;- data[[var]]\n\n  ggplot(tibble(x = x, pr = pr), aes(x, pr)) +\n    geom_point(alpha = 0.35, size = 1, color = \"steelblue\") +\n    geom_smooth(method = \"loess\", span = span, se = TRUE, linewidth = 1.1) +\n    labs(x = var, y = \"Partial residual\", title = paste(\"Partial effect of\", var)) +\n    theme_bw(base_size = 12)\n}\n\nsimple_peplot_by &lt;- function(mod, var, by, data, span = 0.8) {\n  tt &lt;- predict(mod, type = \"terms\")\n  pick &lt;- grep(paste0(\"\\\\b\", var, \"\\\\b\"), colnames(tt))\n  stopifnot(length(pick) &gt; 0)\n  term_contrib &lt;- rowSums(tt[, pick, drop = FALSE])\n  pr &lt;- term_contrib + residuals(mod)\n\n  df &lt;- tibble(x = data[[var]], pr = pr, grp = data[[by]])\n\n  ggplot(df, aes(x, pr, color = grp)) +\n    geom_point(alpha = 0.3, size = 1) +\n    geom_smooth(method = \"loess\", span = span, se = TRUE, linewidth = 1.1) +\n    labs(x = var, y = \"Partial residual\",\n         title = paste(\"Partial effect of\", var, \"by\", by), color = by) +\n    theme_bw(base_size = 12)\n}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Visualization</span>"
    ]
  },
  {
    "objectID": "02-partial-effect-plots.html#final-plots",
    "href": "02-partial-effect-plots.html#final-plots",
    "title": "3  Model Visualization",
    "section": "3.4 Final Plots",
    "text": "3.4 Final Plots\nNow we put everything together: plotting the partial effects of our spline-based linear regression.\n\nsimple_peplot(lm_spline, \"X1\", data = dat)\n\n\n\n\n\n\n\nsimple_peplot(lm_spline, \"X2\", data = dat)\n\n\n\n\n\n\n\nsimple_peplot_by(lm_spline, var = \"X1\", by = \"X3\", data = dat)\n\n\n\n\n\n\n\nsimple_peplot_by(lm_spline, var = \"X2\", by = \"X3\", data = dat)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Visualization</span>"
    ]
  },
  {
    "objectID": "02-partial-effect-plots.html#random-forest-with-partial-dependence-plots",
    "href": "02-partial-effect-plots.html#random-forest-with-partial-dependence-plots",
    "title": "3  Model Visualization",
    "section": "3.5 Random Forest with Partial Dependence Plots",
    "text": "3.5 Random Forest with Partial Dependence Plots\n\nlibrary(pdp)            \n\n\nAttaching package: 'pdp'\n\n\nThe following object is masked from 'package:purrr':\n\n    partial\n\nlibrary(patchwork) \nlibrary(randomForest) \n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nrf &lt;- randomForest(Y ~ X1 + X2 + X3, data = dat, ntree = 500, mtry = 2, importance = TRUE)\n\n\npstyle &lt;- function(p, title) p + labs(title = title, x = NULL, y = \"Partial dependence\") +\n  theme_bw(base_size = 12)\n\np_rf_x1 &lt;- partial(rf, pred.var = \"X1\", grid.resolution = 60, train = dat)     |&gt; autoplot() |&gt; pstyle(\"RF: effect of X1\")\np_rf_x2 &lt;- partial(rf, pred.var = \"X2\", grid.resolution = 60, train = dat)     |&gt; autoplot() |&gt; pstyle(\"RF: effect of X2\")\np_rf_x3 &lt;- partial(rf, pred.var = \"X3\", grid.resolution = 2,  train = dat)     |&gt; autoplot() |&gt; pstyle(\"RF: effect of X3\")\n\n# Spline-Linear PDPs\np_lm_x1 &lt;- partial(lm_spline, pred.var = \"X1\", grid.resolution = 60, train = dat) |&gt; autoplot() |&gt; pstyle(\"Spline-LM: effect of X1\")\np_lm_x2 &lt;- partial(lm_spline, pred.var = \"X2\", grid.resolution = 60, train = dat) |&gt; autoplot() |&gt; pstyle(\"Spline-LM: effect of X2\")\np_lm_x3 &lt;- partial(lm_spline, pred.var = \"X3\", grid.resolution = 2,  train = dat) |&gt; autoplot() |&gt; pstyle(\"Spline-LM: effect of X3\")\n\n(p_rf_x1 | p_rf_x2 | p_rf_x3) /\n(p_lm_x1 | p_lm_x2 | p_lm_x3)\n\n\n\n\n\n\n\n\n\npstyle &lt;- function(p, title) p +\n  labs(title = title, x = NULL, y = \"Partial dependence\") +\n  theme_bw(base_size = 12)\n\np_rf_x1_x3 &lt;- partial(rf, pred.var = c(\"X1\", \"X3\"), grid.resolution = 60, train = dat) |&gt;\n  autoplot(rug = FALSE, contour = FALSE) |&gt; pstyle(\"RF: effect of X1 by X3\")\n\np_rf_x2_x3 &lt;- partial(rf, pred.var = c(\"X2\", \"X3\"), grid.resolution = 60, train = dat) |&gt;\n  autoplot(rug = FALSE, contour = FALSE) |&gt; pstyle(\"RF: effect of X2 by X3\")\n\np_rf_x1_x3 / p_rf_x2_x3",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Visualization</span>"
    ]
  },
  {
    "objectID": "02-partial-effect-plots.html#individual-conditional-expectation-ice-plots",
    "href": "02-partial-effect-plots.html#individual-conditional-expectation-ice-plots",
    "title": "3  Model Visualization",
    "section": "3.6 Individual Conditional Expectation (ICE) plots",
    "text": "3.6 Individual Conditional Expectation (ICE) plots\nICE shows one curve per observation, revealing heterogeneity that PDP/PE can hide.\n\n# ---- helpers styled like pstyle() \npstyle_fx &lt;- function(p, title, xlab, ylab) {\n  p + labs(title = title, x = xlab, y = ylab) + theme_bw(base_size = 12)\n}\n\n# ---- ICE\nplot_ice &lt;- function(model, data, var, grid.size = 60, title = NULL) {\n  grid_vals &lt;- seq(min(data[[var]]), max(data[[var]]), length.out = grid.size)\n\n  ice_data &lt;- purrr::map_dfr(seq_len(nrow(data)), function(i) {\n    new_dat &lt;- data[rep(i, grid.size), ]\n    new_dat[[var]] &lt;- grid_vals\n    tibble(\n      ID   = i,\n      x    = grid_vals,\n      yhat = as.numeric(predict(model, newdata = new_dat))\n    )\n  })\n\n  pdp_data &lt;- ice_data |&gt;\n    dplyr::group_by(x) |&gt;\n    dplyr::summarise(mean_y = mean(yhat), .groups = \"drop\")\n\n  g &lt;- ggplot(ice_data, aes(x = x, y = yhat, group = ID)) +\n    geom_line(alpha = 0.18, linewidth = 0.4, color = \"steelblue\") +\n    geom_line(data = pdp_data, aes(x = x, y = mean_y),\n              inherit.aes = FALSE, color = \"red\", linewidth = 1.1)\n\n  pstyle_fx(g, title %||% paste0(\"ICE: \", var), xlab = var, ylab = \"Predicted Y\")\n}\n\n# ---- ICE by a factor (e.g., X3) ----\nplot_ice_by &lt;- function(model, data, var, by, grid.size = 60, title = NULL) {\n  grid_vals &lt;- seq(min(data[[var]]), max(data[[var]]), length.out = grid.size)\n\n  ice_data &lt;- purrr::map_dfr(seq_len(nrow(data)), function(i) {\n    new_dat &lt;- data[rep(i, grid.size), ]\n    new_dat[[var]] &lt;- grid_vals\n    tibble(\n      ID   = i,\n      grp  = new_dat[[by]][1],\n      x    = grid_vals,\n      yhat = as.numeric(predict(model, newdata = new_dat))\n    )\n  })\n\n  pdp_data &lt;- ice_data |&gt;\n    dplyr::group_by(grp, x) |&gt;\n    dplyr::summarise(mean_y = mean(yhat), .groups = \"drop\")\n\n  g &lt;- ggplot(ice_data, aes(x = x, y = yhat, group = interaction(ID, grp), color = grp)) +\n    geom_line(alpha = 0.18, linewidth = 0.35) +\n    geom_line(data = pdp_data, aes(x = x, y = mean_y, color = grp),\n              inherit.aes = FALSE, linewidth = 1.05)\n\n  pstyle_fx(g, title %||% paste0(\"ICE: \", var, \" by \", by), xlab = var, ylab = \"Predicted Y\")\n}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Visualization</span>"
    ]
  },
  {
    "objectID": "02-partial-effect-plots.html#accumulated-local-effects-ale-plots",
    "href": "02-partial-effect-plots.html#accumulated-local-effects-ale-plots",
    "title": "3  Model Visualization",
    "section": "3.7 Accumulated Local Effects (ALE) plots",
    "text": "3.7 Accumulated Local Effects (ALE) plots\nALE curves summarize local effects and are often more reliable than PDP when features are correlated.\n\n# ---- ALE (no extra packages) ----\nplot_ale &lt;- function(model, data, var, grid.size = 40, title = NULL) {\n  qs &lt;- stats::quantile(data[[var]], probs = seq(0, 1, length.out = grid.size), names = FALSE)\n  mids &lt;- (qs[-1] + qs[-length(qs)]) / 2\n\n  delta &lt;- numeric(length(qs) - 1)\n  for (k in seq_len(length(delta))) {\n    low  &lt;- qs[k]; high &lt;- qs[k + 1]\n    idx &lt;- which(data[[var]] &gt;= low & data[[var]] &lt; high)\n    if (length(idx)) {\n      dat_low  &lt;- data[idx, ];  dat_low[[var]]  &lt;- low\n      dat_high &lt;- data[idx, ]; dat_high[[var]] &lt;- high\n      delta[k] &lt;- mean(predict(model, dat_high) - predict(model, dat_low))\n    } else {\n      delta[k] &lt;- 0\n    }\n  }\n\n  ale &lt;- cumsum(delta)\n  ale &lt;- ale - mean(ale)\n\n  g &lt;- ggplot(tibble(x = mids, y = ale), aes(x, y)) +\n    geom_line(linewidth = 1.1)\n\n  pstyle_fx(g, title %||% paste0(\"ALE: \", var), xlab = var, ylab = \"Accumulated local effect\")\n}\n\n\n# ICE / ALE for the spline model\nice_x1 &lt;- plot_ice(lm_spline, dat, \"X1\", grid.size = 60, title = \"ICE: X1 (Spline-LM)\")\nice_x2 &lt;- plot_ice(lm_spline, dat, \"X2\", grid.size = 60, title = \"ICE: X2 (Spline-LM)\")\n\nale_x1 &lt;- plot_ale(lm_spline, dat, \"X1\", grid.size = 40, title = \"ALE: X1 (Spline-LM)\")\nale_x2 &lt;- plot_ale(lm_spline, dat, \"X2\", grid.size = 40, title = \"ALE: X2 (Spline-LM)\")\n\nice_x2_byx3 &lt;- plot_ice_by(lm_spline, dat, var = \"X2\", by = \"X3\", grid.size = 60,\n                           title = \"ICE: X2 by X3 (Spline-LM)\")                        \nice_x1_byx3 &lt;- plot_ice_by(lm_spline, dat, var = \"X1\", by = \"X3\", grid.size = 60,\n                           title = \"ICE: X1 by X3 (Spline-LM)\")                           \n\n# Arrange like your PDP grid\n(ice_x1 | ice_x2) /\n(ale_x1 | ale_x2)\n\n\n\n\n\n\n\n# Show the grouped ICE as well\nice_x1_byx3\n\n\n\n\n\n\n\nice_x2_byx3\n\n\n\n\n\n\n\n\n\n# ICE for RF\nice_rf_x1 &lt;- plot_ice(rf,  dat, \"X1\", grid.size = 60, title = \"ICE: X1 (Random Forest)\")\nice_rf_x2 &lt;- plot_ice(rf,  dat, \"X2\", grid.size = 60, title = \"ICE: X2 (Random Forest)\")\n\n# ICE by X3 for RF\nice_rf_x1_byx3 &lt;- plot_ice_by(rf, dat, var = \"X1\", by = \"X3\", grid.size = 60,\n                              title = \"ICE: X1 by X3 (Random Forest)\")\nice_rf_x2_byx3 &lt;- plot_ice_by(rf, dat, var = \"X2\", by = \"X3\", grid.size = 60,\n                              title = \"ICE: X2 by X3 (Random Forest)\")\n\n# ALE for RF\nale_rf_x1 &lt;- plot_ale(rf,  dat, \"X1\", grid.size = 40, title = \"ALE: X1 (Random Forest)\")\nale_rf_x2 &lt;- plot_ale(rf,  dat, \"X2\", grid.size = 40, title = \"ALE: X2 (Random Forest)\")\n\n# --- Arrange with patchwork (optional) ---\nlibrary(patchwork)\n\n# Side-by-side ICE and ALE for RF\n(ice_rf_x1 | ice_rf_x2) /\n(ale_rf_x1 | ale_rf_x2)\n\n\n\n\n\n\n\n# Grouped ICE by X3\nice_rf_x1_byx3\n\n\n\n\n\n\n\nice_rf_x2_byx3",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Visualization</span>"
    ]
  },
  {
    "objectID": "03-4-models.html",
    "href": "03-4-models.html",
    "title": "4  4 Models",
    "section": "",
    "text": "4.1 Generalized Linear Model (GLM)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>4 Models</span>"
    ]
  },
  {
    "objectID": "03-4-models.html#generalized-linear-model-glm",
    "href": "03-4-models.html#generalized-linear-model-glm",
    "title": "4  4 Models",
    "section": "",
    "text": "4.1.1 Motivating Ideas\nThe Generalized Linear Model (GLM) represents a major unifying step in the history of statistics.\nBefore 1972, researchers used a collection of specialized methods for different kinds of data:\n\nContinuous data: Multiple linear regression (Normal distribution, identity link)\n\nGroup mean comparisons: ANOVA (Normal distribution, identity link)\n\nBinary data: Logistic or probit regression (Binomial distribution, logit/probit link)\n\nCount data: Poisson regression (Poisson distribution, log link)\n\nEach model had its own estimation rules and assumptions.\nNelder and Wedderburn (1972) proposed GLMs as a single framework that could describe all of these models through three shared components:\n\nA random component:\nThe response variable \\(Y_i\\) follows a distribution from the exponential family (e.g., Normal, Binomial, Poisson, Gamma).\nA systematic component:\nPredictors enter linearly through\n\\[\n\\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}.\n\\]\nA link function:\nConnects the expected value \\(\\mu_i = E[Y_i]\\) to the linear predictor:\n\\[\ng(\\mu_i) = \\eta_i.\n\\]\n\nWith this formulation, the same estimation algorithm — Iteratively Reweighted Least Squares (IRLS) — can be used across models.\nGLMs thus generalized linear regression to non-normal data while keeping the interpretability of regression coefficients.\n\n\n4.1.2 Chronology of Key Ideas\nAdapted from Lindsey’s summary of McCullagh & Nelder (who themselves drew from Stiegler), the historical path toward GLMs developed gradually:\n\n\n\n\n\n\n\n\n\nPeriod\nDevelopment\nDistribution & Link\nKey Contributors\n\n\n\n\nEarly 19th century\nMultiple linear regression — foundation of least squares.\nNormal, identity\nLegendre, Gauss\n\n\n1920s–1935\nANOVA formalized — partitioning of variance.\nNormal, identity\nFisher\n\n\n1922\nLikelihood function introduced — general approach to inference.\nAny\nFisher\n\n\n1922\nDilution assays for dose–response data.\nBinomial, complementary log–log\nFisher\n\n\n1934\nExponential family identified — distributions with sufficient statistics.\n—\nFisher\n\n\n1935\nProbit analysis for quantal response data.\nBinomial, probit\nBliss\n\n\n1944–1952\nLogit model for proportions.\nBinomial, logit\nBerkson; Dyke & Patterson\n\n\n1960\nItem response theory (Rasch model).\nBernoulli, logit\nRasch\n\n\n1963\nLog-linear models for count data.\nPoisson, log\nBirch\n\n\n1965–1967\nRegression models for survival data.\nExponential, log or reciprocal\nFeigl & Zelen; Zippin & Armitage; Glasser\n\n\n1966\nInverse polynomials extended regression to Gamma data.\nGamma, reciprocal\nNelder\n\n\n1972\nGeneralized Linear Models unified all the above under one theory and algorithm.\nExponential family, general link\nNelder & Wedderburn",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>4 Models</span>"
    ]
  },
  {
    "objectID": "03-4-models.html#conceptual-unification",
    "href": "03-4-models.html#conceptual-unification",
    "title": "4  4 Models",
    "section": "4.2 Conceptual Unification",
    "text": "4.2 Conceptual Unification\nBy the early 1970s, it became clear that many well-known models were specific cases of a broader principle.\nNelder and Wedderburn (1972) formally expressed this as:\n\\[\ng(\\mu_i) = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}, \\quad\nY_i \\sim \\text{Exponential Family}(\\mu_i, \\phi)\n\\]\nTheir key insight:\n\nAll these models (linear, logistic, Poisson, Gamma, etc.) share the same likelihood structure.\n\nThey can all be estimated through the same maximum-likelihood algorithm.\n\nDiagnostics, residuals, and hypothesis tests could be made consistent across model types.\n\nIn short, GLMs provided the long-sought unification of statistical modeling, bringing together 150 years of development—from Gauss’s least squares and Fisher’s likelihood theory to modern regression frameworks.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>4 Models</span>"
    ]
  },
  {
    "objectID": "03-4-models.html#references",
    "href": "03-4-models.html#references",
    "title": "4  4 Models",
    "section": "4.3 References",
    "text": "4.3 References\n\nLegendre, A. M. (1805). Nouvelles méthodes pour la détermination des orbites des comètes.\n\nGauss, C. F. (1809). Theoria motus corporum coelestium in sectionibus conicis solem ambientium.\n\nFisher, R. A. (1922). On the mathematical foundations of theoretical statistics. Philosophical Transactions of the Royal Society A.\n\nBliss, C. I. (1935). The method of probits. Science, 79(2037), 38–39.\n\nBerkson, J. (1944). Application of the logistic function to bio-assay. JASA, 39(227), 357–365.\n\nRasch, G. (1960). Probabilistic models for some intelligence and attainment tests.\n\nBirch, M. W. (1963). Maximum likelihood in three-way contingency tables. JASA, 58, 1071–1081.\n\nFeigl, P., & Zelen, M. (1965). Estimation of exponential survival probabilities with concomitant information. Biometrics, 21, 826–838.\n\nNelder, J. A., & Wedderburn, R. W. M. (1972). Generalized Linear Models. JRSS A, 135(3), 370–384.\n\nMcCullagh, P., & Nelder, J. A. (1989). Generalized Linear Models (2nd ed.). Chapman & Hall.\n\nLindsey, J. K. (1997). Applying Generalized Linear Models. Springer.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>4 Models</span>"
    ]
  },
  {
    "objectID": "03-4-models.html#expressing-the-glm-mathematically",
    "href": "03-4-models.html#expressing-the-glm-mathematically",
    "title": "4  4 Models",
    "section": "4.4 Expressing the GLM Mathematically",
    "text": "4.4 Expressing the GLM Mathematically\nA single-parameter exponential family can be expressed as:\n\\[\nf_X(x \\mid \\theta)\n= h(x)\\,\\exp\\!\\left[\\eta(\\theta)\\,T(x) - A(\\theta)\\right],\n\\]\nor equivalently,\n\\[\n\\log f_X(x \\mid \\theta)\n= \\eta(\\theta)\\,T(x) - A(\\theta) + B(x).\n\\]\n\n4.4.1 Components\n\n\n\n\n\n\n\n\nSymbol\nRole\nGLM Interpretation\n\n\n\n\n\\[T(x)\\]\nSufficient statistic\nResponse variable \\[Y_i\\]\n\n\n\\[h(x)\\] or \\[B(x)\\]\nBase measure\nConstant term \\[c(y_i, \\phi)\\]\n\n\n\\[\\eta(\\theta)\\]\nNatural (canonical) parameter\nLinear predictor \\[\\eta_i = X_i^\\top \\beta\\] under canonical link\n\n\n\\[A(\\theta)\\]\nCumulant (log-partition) function\nDetermines mean and variance\n\n\n\\[\\theta\\]\nModel parameter\nCanonical parameter of the distribution\n\n\n\nFor any exponential-family member:\n\\[\n\\mathbb{E}[T(X)] = A'(\\theta),\n\\]\n\\[\n\\mathrm{Var}[T(X)] = A''(\\theta).\n\\]\nThus, for GLMs:\n\\[\n\\mu_i = \\mathbb{E}[Y_i] = A'(\\theta_i),\n\\]\n\\[\n\\mathrm{Var}(Y_i) = a(\\phi) A''(\\theta_i),\n\\]\nand the link function connects the mean to the predictors:\n\\[\ng(\\mu_i) = \\eta_i = X_i^\\top \\beta.\n\\]\nIf the link is canonical, then:\n\\[\ng(\\mu_i) = \\theta_i.\n\\]\n\n\n\n4.4.2 Binomial Distribution (Logistic GLM)\n\\[\nY_i \\sim \\text{Binomial}(n_i, p_i)\n\\]\nFor simplicity, we often take \\[n_i = 1\\] so that:\n\\[\nY_i \\sim \\text{Binomial}(1, p_i).\n\\]\nThe probability mass function is:\n\\[\nf_Y(y_i \\mid p_i) = \\binom{n_i}{y_i} p_i^{y_i} (1 - p_i)^{n_i - y_i}.\n\\]\nThis can be written in exponential-family form:\n\\[\nf_Y(y_i \\mid \\theta_i)\n= h(y_i)\\,\\exp\\!\\left[y_i \\theta_i - A(\\theta_i)\\right],\n\\]\nwhere:\n\\[\n\\theta_i = \\log\\!\\frac{p_i}{1 - p_i},\n\\]\n\\[\nA(\\theta_i) = n_i \\log(1 + e^{\\theta_i}),\n\\]\n\\[\nh(y_i) = \\binom{n_i}{y_i}.\n\\]\nThen the mean and variance follow from derivatives of \\[A(\\theta_i)\\]:\n\\[\n\\mu_i = A'(\\theta_i) = n_i \\frac{e^{\\theta_i}}{1 + e^{\\theta_i}} = n_i p_i,\n\\]\n\\[\n\\mathrm{Var}(Y_i) = A''(\\theta_i) = n_i p_i (1 - p_i).\n\\]\nThe canonical link function is the logit:\n\\[\ng(p_i) = \\log\\!\\frac{p_i}{1 - p_i} = \\eta_i = X_i^\\top \\beta,\n\\]\nand the inverse link (mean function) is:\n\\[\np_i = \\frac{e^{\\eta_i}}{1 + e^{\\eta_i}}.\n\\]\n\n\n\n4.4.3 Poisson Distribution (Log-linear GLM)\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i)\n\\]\nThe probability mass function is:\n\\[\nf_Y(y_i \\mid \\lambda_i) = \\frac{\\lambda_i^{y_i} e^{-\\lambda_i}}{y_i!}.\n\\]\nThis can be expressed in exponential-family form:\n\\[\nf_Y(y_i \\mid \\theta_i)\n= h(y_i)\\,\\exp\\!\\left[y_i \\theta_i - A(\\theta_i)\\right],\n\\]\nwhere:\n\\[\n\\theta_i = \\log \\lambda_i,\n\\]\n\\[\nA(\\theta_i) = e^{\\theta_i},\n\\]\n\\[\nh(y_i) = \\frac{1}{y_i!}.\n\\]\nFrom these we derive:\n\\[\n\\mu_i = A'(\\theta_i) = e^{\\theta_i} = \\lambda_i,\n\\]\n\\[\n\\mathrm{Var}(Y_i) = A''(\\theta_i) = e^{\\theta_i} = \\lambda_i.\n\\]\nThe canonical link is the log link:\n\\[\ng(\\mu_i) = \\log \\mu_i = \\eta_i = X_i^\\top \\beta,\n\\]\nand the inverse link is:\n\\[\n\\mu_i = e^{\\eta_i}.\n\\]\nIf we model rates with exposure \\[E_i\\]:\n\\[\n\\log \\lambda_i = \\log E_i + X_i^\\top \\beta,\n\\]\nor equivalently:\n\\[\n\\lambda_i = E_i\\,e^{X_i^\\top \\beta}.\n\\]\n\n\n\n4.4.4 Summary Table\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nCanonical Parameter \\[\\theta\\]\nCumulant \\[A(\\theta)\\]\nMean \\[A'(\\theta)\\]\nVariance \\[A''(\\theta)\\]\nCanonical Link\n\n\n\n\n\\[Y_i \\sim \\text{Binomial}(1, p_i)\\]\n\\[\\log\\frac{p_i}{1-p_i}\\]\n\\[\\log(1 + e^{\\theta})\\]\n\\[p_i\\]\n\\[p_i(1 - p_i)\\]\nLogit\n\n\n\\[Y_i \\sim \\text{Poisson}(\\lambda_i)\\]\n\\[\\log \\lambda_i\\]\n\\[e^{\\theta}\\]\n\\[\\lambda_i\\]\n\\[\\lambda_i\\]\nLog\n\n\n\n\n\n\n4.4.5 What We Will Demonstrate in Code\n\nSimulate Data\n\nBinomial: \\[Y_i \\sim \\text{Binomial}(1, p_i)\\]\n\nPoisson: \\[Y_i \\sim \\text{Poisson}(\\lambda_i)\\]\n\nFit GLMs with Canonical Links\n\nLogistic GLM: \\[g(p_i) = \\log\\frac{p_i}{1-p_i}\\]\n\nPoisson GLM: \\[g(\\lambda_i) = \\log \\lambda_i\\]\n\nVerify Theoretical Relationships\n\nMean: \\[\\mu_i = A'(\\theta_i)\\]\n\nVariance: \\[\\mathrm{Var}(Y_i) = A''(\\theta_i)\\]\n\nCanonical link linearity: \\[g(\\mu_i) = X_i^\\top \\beta\\]\n\nVisualize Results\n\nPredicted vs observed \\[Y_i\\]\n\nInverse-link response curves \\[g^{-1}(\\eta_i)\\]\n\nDeviance and residual diagnostics",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>4 Models</span>"
    ]
  },
  {
    "objectID": "03-4-models.html#code",
    "href": "03-4-models.html#code",
    "title": "4  4 Models",
    "section": "4.5 Code",
    "text": "4.5 Code\n\nset.seed(1)\nn  &lt;- 500\nx1 &lt;- rnorm(n)\nx2 &lt;- rnorm(n)\neta &lt;- -0.5 + 1.1*x1 - 0.8*x2\np   &lt;- plogis(eta)\ny   &lt;- rbinom(n, size = 1, prob = p)\n\ndf &lt;- data.frame(y, x1, x2)\n\nm_logit &lt;- glm(y ~ x1 + x2, data = df, family = binomial())\nsummary(m_logit)\n\n\nCall:\nglm(formula = y ~ x1 + x2, family = binomial(), data = df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -0.3858     0.1078  -3.579 0.000344 ***\nx1            1.1572     0.1318   8.780  &lt; 2e-16 ***\nx2           -0.7521     0.1107  -6.794 1.09e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 685.93  on 499  degrees of freedom\nResidual deviance: 530.03  on 497  degrees of freedom\nAIC: 536.03\n\nNumber of Fisher Scoring iterations: 4\n\n# Effect curve for x1 (x2 fixed at 0), with 95% CI on response scale\nlibrary(ggplot2)\nxgrid &lt;- data.frame(x1 = seq(-3, 3, length.out = 200), x2 = 0)\npred  &lt;- predict(m_logit, newdata = xgrid, type = \"link\", se.fit = TRUE)\neta_hat &lt;- pred$fit\nse_eta  &lt;- pred$se.fit\np_hat   &lt;- plogis(eta_hat)\nlo      &lt;- plogis(eta_hat - 1.96*se_eta)\nhi      &lt;- plogis(eta_hat + 1.96*se_eta)\n\nggplot() +\n  geom_point(aes(x = x1, y = y), data = df, alpha = 0.25) +\n  geom_line(aes(x = xgrid$x1, y = p_hat), linewidth = 1) +\n  geom_ribbon(aes(x = xgrid$x1, ymin = lo, ymax = hi), alpha = 0.2) +\n  labs(title = \"Binomial GLM (logit): effect of x1 | x2=0\",\n       x = \"x1\", y = \"P(Y=1)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Quick residual check\nplot(fitted(m_logit), residuals(m_logit, type = \"deviance\"),\n     xlab = \"Fitted probability\", ylab = \"Deviance residual\",\n     main = \"Logistic GLM: residuals vs fitted\")\nabline(h = 0, lty = 2)\n\n\n\n\n\n\n\n\n\nset.seed(2)\nn  &lt;- 500\nx1 &lt;- rnorm(n)\nx2 &lt;- rnorm(n)\nE  &lt;- runif(n, 0.5, 2.5)                # exposure\neta &lt;- 0.2 + 0.5*x1 + 0.6*x2 + log(E)   # include offset in DGP\nlambda &lt;- exp(eta)\ny  &lt;- rpois(n, lambda)\n\ndfp &lt;- data.frame(y, x1, x2, E)\n\nm_pois &lt;- glm(y ~ x1 + x2 + offset(log(E)), data = dfp, family = poisson())\nsummary(m_pois)\n\n\nCall:\nglm(formula = y ~ x1 + x2 + offset(log(E)), family = poisson(), \n    data = dfp)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.24655    0.03577   6.892  5.5e-12 ***\nx1           0.50714    0.02559  19.814  &lt; 2e-16 ***\nx2           0.59661    0.02793  21.359  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1337.16  on 499  degrees of freedom\nResidual deviance:  498.35  on 497  degrees of freedom\nAIC: 1634.3\n\nNumber of Fisher Scoring iterations: 5\n\n# Effect curve for x1 (x2 = 0, E = 1), with 95% CI on mean scale\nlibrary(ggplot2)\nxgrid &lt;- data.frame(x1 = seq(-3, 3, length.out = 200), x2 = 0, E = 1)\npred  &lt;- predict(m_pois, newdata = xgrid, type = \"link\", se.fit = TRUE)\neta_hat &lt;- pred$fit\nse_eta  &lt;- pred$se.fit\nmu_hat  &lt;- exp(eta_hat)\nlo      &lt;- exp(eta_hat - 1.96*se_eta)\nhi      &lt;- exp(eta_hat + 1.96*se_eta)\n\nggplot() +\n  geom_point(aes(x = x1, y = y/E), data = dfp, alpha = 0.25) +\n  geom_line(aes(x = xgrid$x1, y = mu_hat), linewidth = 1) +\n  geom_ribbon(aes(x = xgrid$x1, ymin = lo, ymax = hi), alpha = 0.2) +\n  labs(title = \"Poisson GLM (log): effect of x1 | x2=0, E=1\",\n       x = \"x1\", y = \"Rate (λ)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Quick residual check + overdispersion diagnostic\nplot(fitted(m_pois), residuals(m_pois, type = \"deviance\"),\n     xlab = \"Fitted mean λ\", ylab = \"Deviance residual\",\n     main = \"Poisson GLM: residuals vs fitted\")\nabline(h = 0, lty = 2)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>4 Models</span>"
    ]
  },
  {
    "objectID": "03-4-models.html#ordinal-regression-proportional-odds",
    "href": "03-4-models.html#ordinal-regression-proportional-odds",
    "title": "4  4 Models",
    "section": "4.6 Ordinal Regression (Proportional Odds)",
    "text": "4.6 Ordinal Regression (Proportional Odds)\nWe observe a response $ Y $ ordinal, or a finely discretized/continuous outcome) and predictors $ x ^p$ .\nWe want to model the conditional cumulative distribution function (CDF)\n\\[\nF(y \\mid x) = P(Y \\le y \\mid x).\n\\]\nFollowing Liu et al. (2017), we use a cumulative probability model (CPM), which is the same mathematical structure as an ordinal / proportional odds regression. \\[\ng\\big( P(Y \\le y \\mid x) \\big)\n= \\alpha(y) - x^\\top \\beta,\n\\tag{1}\n\\] where\n\\[ g(\\cdot) \\] is a link function (logit, probit, loglog, cloglog), \\[ \\alpha(y) \\] is a nondecreasing function of the cutpoint \\[ y \\] (it plays the role of ordered thresholds), \\[ \\beta \\] is a single vector of regression coefficients shared across all cutpoints (this is the proportional / parallel slopes assumption).\nThis is the form used in Liu et al. (2017), Modeling continuous response variables using ordinal regression.\nSuppose \\[Y \\in \\{1,2,\\dots,K\\}.\\]\nDefine \\[\\theta_k = \\alpha(k)\\] for \\[k=1,\\dots,K-1.\\]\nThen (1) becomes the standard cumulative link / proportional odds model\n\\[\ng\\big( P(Y \\le k \\mid x) \\big)\n= \\theta_k - x^\\top \\beta, \\quad k = 1, \\dots, K-1.\n\\]\n\n4.6.1 Choice of link\n\nlogit:\n\\[\ng(p) = \\log\\frac{p}{1-p}, \\qquad\ng^{-1}(y) = \\frac{e^{y}}{1+e^{y}}\n\\]\nprobit:\n\\[\ng(p) = \\Phi^{-1}(p), \\qquad\ng^{-1}(y) = \\Phi(y)\n\\]\nloglog:\n\\[\ng(p) = -\\log\\big(-\\log(p)\\big), \\qquad\ng^{-1}(y) = \\exp\\big[-\\exp(-y)\\big]\n\\]\ncloglog:\n\\[\ng(p) = \\log\\big[-\\log(1-p)\\big], \\qquad\ng^{-1}(y) = 1 - \\exp\\big[-\\exp(y)\\big]\n\\]\n\nHere, \\[\\Phi(\\cdot)\\] is the CDF of the standard normal distribution.\nLet \\[\n\\pi_k(x) = P(Y = k \\mid x), \\quad k=1,\\dots,K.\n\\]\nFrom the cumulative probabilities: \\[\n\\begin{aligned}\nP(Y = 1 \\mid x) &= P(Y \\le 1 \\mid x), \\\\\nP(Y = k \\mid x) &= P(Y \\le k \\mid x) - P(Y \\le k-1 \\mid x), \\quad k=2,\\dots,K-1, \\\\\nP(Y = K \\mid x) &= 1 - P(Y \\le K-1 \\mid x).\n\\end{aligned}\n\\]\nSo with the logit link, the full model is\n\\[\n\\log \\frac{P(Y \\le k \\mid x)}{1 - P(Y \\le k \\mid x)}\n= \\theta_k - x^\\top \\beta, \\quad k=1,\\dots,K-1,\n\\] and the \\[\\pi_k(x)\\] are obtained by the above expressions.\n\n\n4.6.2 Interpretation\n\\(\\beta\\) describes how a one-unit change in a predictor shifts the entire conditional distribution of \\(Y\\). \\(\\theta_k\\) locate the cutpoints between ordered categories. Using different links corresponds to assuming different latent error distributions, as listed in the paper.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>4 Models</span>"
    ]
  },
  {
    "objectID": "03-4-models.html#neural-net",
    "href": "03-4-models.html#neural-net",
    "title": "4  4 Models",
    "section": "4.7 Neural Net",
    "text": "4.7 Neural Net\n\n4.7.1 McCulloch–Pitts Neuron (1943)\n\n4.7.1.1 Model Description\nA neuron fires if it receives enough excitatory input and no inhibitory input.\nLet\n- \\(v_i(t) \\in \\{0,1\\}\\)$ activation (firing) state of neuron \\(i\\) at time \\(t\\),\n- \\(E_i\\): set of excitatory inputs,\n- \\(I_i\\): set of inhibitory inputs,\n- \\(T_i\\): excitatory threshold.\n\n\n4.7.1.2 Model Equation\n\\[\nv_i(t+1) =\n\\begin{cases}\n1, & \\text{if } \\displaystyle \\sum_{j \\in E_i} v_j(t) \\ge T_i \\text{ and } \\sum_{k \\in I_i} v_k(t) = 0, \\\\[6pt]\n0, & \\text{otherwise.}\n\\end{cases}\n\\]\nThis is a logical threshold model — the first formal mathematical neuron, but without a learning rule.\n\n\n\n4.7.2 Rosenblatt Perceptron (1958)\n\n4.7.2.1 Model Description\nA single-layer learnable threshold unit for classification.\nLet\n- Input vector \\(x \\in \\mathbb{R}^d\\),\n- Weight vector \\(w \\in \\mathbb{R}^d\\),\n- Bias \\(b\\),\n- Target label \\(t \\in \\{0,1\\}\\).\n\n\n4.7.2.2 Forward Function\n\\[\ny =\n\\begin{cases}\n1, & \\text{if } w^\\top x + b \\ge 0, \\\\[4pt]\n0, & \\text{if } w^\\top x + b &lt; 0.\n\\end{cases}\n\\]\nor equivalently\n\\[\ny = \\mathbf{1}\\{w^\\top x + b \\ge 0\\}.\n\\]\n\n\n4.7.2.3 Learning Rule\nGiven a learning rate (&gt; 0):\n\\[\nw \\leftarrow w + \\eta (t - y) x, \\qquad\nb \\leftarrow b + \\eta (t - y).\n\\]\nIf the perceptron predicts incorrectly, the weights shift toward or away from the input vector to correct the error.\n\n\n\n4.7.3 Rumelhart–Hinton–Williams Backpropagation Network (1986)\nA multilayer feedforward network trained by gradient descent.\nLet\n- Layer index \\(\\ell = 1, 2, \\dots, L\\),\n- Activations \\(a_j^{(\\ell)}\\),\n- Net inputs \\(z_j^{(\\ell)}\\),\n- Weights \\(w_{ij}^{(\\ell)}\\),\n- Biases \\(b_j^{(\\ell)}\\).\n\n4.7.3.1 Forward Pass\nFor the input layer: \\[\na_j^{(1)} = x_j.\n\\]\nFor each hidden or output layer: \\[\nz_j^{(\\ell)} = \\sum_i w_{ij}^{(\\ell)} a_i^{(\\ell-1)} + b_j^{(\\ell)}, \\qquad\na_j^{(\\ell)} = f(z_j^{(\\ell)}),\n\\] where (f) is a differentiable activation function (e.g. sigmoid).\nThe total loss (mean-squared error): \\[\nE = \\frac{1}{2} \\sum_k (t_k - a_k^{(L)})^2.\n\\]\n\n\n4.7.3.2 Backward Pass (Error Terms)\nFor output layer units: \\[\n\\delta_k^{(L)} = (t_k - a_k^{(L)}) f'(z_k^{(L)}).\n\\]\nFor hidden layer units: \\[\n\\delta_j^{(\\ell)} = f'(z_j^{(\\ell)}) \\sum_k w_{jk}^{(\\ell+1)} \\delta_k^{(\\ell+1)}.\n\\]\n\n\n4.7.3.3 Weight and Bias Updates\n\\[\n\\Delta w_{ij}^{(\\ell)} = \\eta \\, a_i^{(\\ell-1)} \\, \\delta_j^{(\\ell)}, \\qquad\nw_{ij}^{(\\ell)} \\leftarrow w_{ij}^{(\\ell)} + \\Delta w_{ij}^{(\\ell)}.\n\\]\n\\[\n\\Delta b_j^{(\\ell)} = \\eta \\, \\delta_j^{(\\ell)}, \\qquad\nb_j^{(\\ell)} \\leftarrow b_j^{(\\ell)} + \\Delta b_j^{(\\ell)}.\n\\]\n\n\n\n4.7.4 Summary Table\n\n\n\n\n\n\n\n\nModel\nOutput Function\nLearning Rule\n\n\n\n\nMcCulloch–Pitts (1943)\n\\[v_i(t+1)=1\\] if enough excitation and no inhibition\nNone (fixed logic)\n\n\nRosenblatt (1958)\n\\[y=\\mathbf{1}\\{w^\\top x + b \\ge 0\\}\\]\n\\[w \\leftarrow w + \\eta (t-y)x\\]\n\n\nRumelhart et al. (1986)\n\\[a_j=f\\!\\left(\\sum_i w_{ij}a_i+b_j\\right)\\]\n\\[\\Delta w_{ij}=\\eta a_i \\delta_j\\] with backpropagation\n\n\n\nThese three equations trace the mathematical evolution of neural networks:\nfrom logical firing (McCulloch–Pitts) → learnable thresholds (Rosenblatt) → differentiable, multilayer learning (Rumelhart–Hinton–Williams).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>4 Models</span>"
    ]
  },
  {
    "objectID": "03-4-models.html#create-visualizations-of-the-prediction-relationship",
    "href": "03-4-models.html#create-visualizations-of-the-prediction-relationship",
    "title": "4  4 Models",
    "section": "4.8 Create visualizations of the prediction relationship",
    "text": "4.8 Create visualizations of the prediction relationship\n\nlibrary(tidyverse)\nlibrary(nnet)           # for neural net\nlibrary(randomForest)   # for random forest\nlibrary(MASS)           # for polr (ordinal regression)\nset.seed(42)\nn  &lt;- 2000\nX1 &lt;- rnorm(n)\nX2 &lt;- rnorm(n)\nX3 &lt;- rbinom(n, 1, 0.4)\n\nsigmoid &lt;- function(x) 1 / (1 + exp(-x))\nmu &lt;- sqrt(5) * sigmoid(X1 + X3) + sqrt(5) * sigmoid(X2) * X3\nY  &lt;- mu + rnorm(n, 0, 1)\n\ndat &lt;- tibble(X1, X2, X3 = factor(X3), Y)\n\n\nlibrary(tidyverse)\nlibrary(nnet)\nlibrary(randomForest)\nlibrary(MASS)\n\nset.seed(42)\n\n# --- models -------------------------------------------------\nglm_fit &lt;- glm(Y ~ X1 + X2 + X3, data = dat, family = gaussian())\n\ndat &lt;- dat %&gt;%\n  mutate(pred_glm = predict(glm_fit))\n\nnn_fit &lt;- nnet(\n  Y ~ X1 + X2 + X3,\n  data   = dat,\n  size   = 5,\n  linout = TRUE,\n  maxit  = 1000,\n  decay  = 1e-4,\n  trace  = FALSE\n)\n\ndat &lt;- dat %&gt;%\n  mutate(pred_nn = as.numeric(predict(nn_fit, dat)))\n\nrf_fit &lt;- randomForest(\n  Y ~ X1 + X2 + X3,\n  data = dat,\n  ntree = 300,\n  importance = TRUE\n)\n\ndat &lt;- dat %&gt;%\n  mutate(pred_rf = predict(rf_fit, dat))\n\n# --- ordinal target -----------------------------------------\ndat &lt;- dat %&gt;%\n  mutate(\n    Y_ord = cut(\n      Y,\n      breaks = quantile(Y, probs = c(0, 0.33, 0.66, 1)),\n      labels = c(\"Low\", \"Medium\", \"High\"),\n      include.lowest = TRUE,\n      ordered_result = TRUE\n    )\n  )\n\nord_fit &lt;- polr(Y_ord ~ X1 + X2 + X3, data = dat, method = \"logistic\")\n\ndat &lt;- dat %&gt;%\n  mutate(pred_ord = predict(ord_fit, type = \"class\"))\n\n# --- long format for compare plot ---------------------------\ndat_long &lt;- dat %&gt;%\n  dplyr::select(Y, pred_glm, pred_nn, pred_rf) %&gt;%\n  tidyr::pivot_longer(\n    cols = c(pred_glm, pred_nn, pred_rf),\n    names_to = \"model\",\n    values_to = \"prediction\"\n  ) %&gt;%\n  mutate(\n    model = dplyr::recode(\n      model,\n      pred_glm = \"GLM\",\n      pred_nn  = \"Neural Net\",\n      pred_rf  = \"Random Forest\"\n    )\n  )\n\n# --- compare scatter ----------------------------------------\nggplot(dat_long, aes(x = Y, y = prediction)) +\n  geom_point(alpha = 0.25) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  facet_wrap(~ model, nrow = 1) +\n  labs(\n    title = \"Observed vs Predicted: Model Comparison\",\n    x = \"Observed Y\",\n    y = \"Predicted Y\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n# --- residual density ---------------------------------------\ndat_long %&gt;%\n  mutate(resid = prediction - Y) %&gt;%\n  ggplot(aes(x = resid, fill = model)) +\n  geom_density(alpha = 0.4) +\n  labs(\n    title = \"Residual distributions by model\",\n    x = \"Prediction error (pred - Y)\",\n    y = \"Density\"\n  ) +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>4 Models</span>"
    ]
  },
  {
    "objectID": "04-nonlinearity.html",
    "href": "04-nonlinearity.html",
    "title": "5  How Random Forests Capture Nonlinearity",
    "section": "",
    "text": "5.1 A decision tree is a nonlinear, piecewise-constant function\nA regression tree partitions the feature space into disjoint regions:\n\\[\n\\mathcal{X} = \\bigcup_{m=1}^{M} R_m,\\qquad R_m \\cap R_{m'} = \\emptyset.\n\\]\nThe tree predicts using:\n\\[\n\\hat{f}_{\\text{tree}}(x)\n= \\sum_{m=1}^{M} c_m \\, \\mathbf{1}(x \\in R_m),\n\\]\na step function that jumps across regions → inherently nonlinear.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>How Random Forests Capture Nonlinearity</span>"
    ]
  },
  {
    "objectID": "04-nonlinearity.html#trees-express-interactions-implicitly",
    "href": "04-nonlinearity.html#trees-express-interactions-implicitly",
    "title": "5  How Random Forests Capture Nonlinearity",
    "section": "5.2 Trees express interactions implicitly",
    "text": "5.2 Trees express interactions implicitly\nEach region is defined by multiple conditions:\n\\[\nR_m = \\bigcap_{s=1}^{k_m}\n\\{ x_{j_s} &lt; t_s \\} \\quad \\text{or} \\quad \\{ x_{j_s} \\ge t_s \\}.\n\\]\nSo a prediction like:\n\\[\n\\hat{f}(x) = c_m  \n\\text{ if } (x_1 &lt; t_1) \\land (x_3 &gt; t_2) \\land (x_2 &lt; t_3)\n\\]\nautomatically encodes interactions between variables.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>How Random Forests Capture Nonlinearity</span>"
    ]
  },
  {
    "objectID": "04-nonlinearity.html#random-forests-average-many-nonlinear-trees",
    "href": "04-nonlinearity.html#random-forests-average-many-nonlinear-trees",
    "title": "5  How Random Forests Capture Nonlinearity",
    "section": "5.3 Random forests average many nonlinear trees",
    "text": "5.3 Random forests average many nonlinear trees\nA random forest prediction is:\n\\[\n\\hat{f}_{\\text{RF}}(x)\n= \\frac{1}{B} \\sum_{b=1}^{B} \\hat{f}_b(x).\n\\]\nExpanding:\n\\[\n\\hat{f}_{\\text{RF}}(x)\n= \\frac{1}{B}\n\\sum_{b=1}^{B}\n\\left( \\sum_{m=1}^{M_b}\nc_{bm} \\, \\mathbf{1}(x \\in R_{bm}) \\right).\n\\]\nThis is a sum of many nonlinear indicator functions, giving a highly flexible model.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>How Random Forests Capture Nonlinearity</span>"
    ]
  },
  {
    "objectID": "04-nonlinearity.html#randomization-increases-functional-flexibility",
    "href": "04-nonlinearity.html#randomization-increases-functional-flexibility",
    "title": "5  How Random Forests Capture Nonlinearity",
    "section": "5.4 Randomization increases functional flexibility",
    "text": "5.4 Randomization increases functional flexibility\nEach tree uses different:\n\nbootstrap samples\n\nsubsets of features\n\nsplit thresholds\n\nThus its regions differ: \\[ R_{b1}, R_{b2}, …\\]\nAveraging these diverse nonlinear structures approximates:\n\\[\n\\hat{f}_{\\text{RF}}(x)\n\\approx \\mathbb{E}_{\\text{boot},\\ \\text{feat}}\n\\left[ \\hat{f}_{\\text{tree}}(x) \\right].\n\\]\n##*Random forests can be viewed as kernel methods\nDefine the forest kernel:\n\\[\nK(x, x')\n= \\mathbb{P}_{\\theta}\n( x \\text{ and } x' \\text{ fall in the same leaf} ).\n\\]\nThen the forest prediction can be written:\n\\[\n\\hat{f}_{\\text{RF}}(x)\n= \\sum_{i=1}^{n} \\alpha_i \\, K(x, x_i).\n\\]\nThis is a nonlinear kernel smoother.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>How Random Forests Capture Nonlinearity</span>"
    ]
  },
  {
    "objectID": "04-nonlinearity.html#summary",
    "href": "04-nonlinearity.html#summary",
    "title": "5  How Random Forests Capture Nonlinearity",
    "section": "5.5 Summary",
    "text": "5.5 Summary\nRandom forests capture nonlinearity because they:\n\ncreate piecewise-constant nonlinear maps,\n\nembed interactions through sequential splits,\n\naverage many different nonlinear partitions,\n\nbehave like a data-driven nonlinear kernel estimator.\n\nOverall:\n\\[\n\\hat{f}_{\\text{RF}}(x)\n= \\frac{1}{B} \\sum_{b=1}^{B}\n\\sum_{m=1}^{M_b}\nc_{bm}\\, \\mathbf{1}(x \\in R_{bm}),\n\\]\nwhich is fundamentally nonlinear and highly flexible.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>How Random Forests Capture Nonlinearity</span>"
    ]
  },
  {
    "objectID": "05-final.html",
    "href": "05-final.html",
    "title": "6  Final Exam",
    "section": "",
    "text": "6.1 Question 1",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Final Exam</span>"
    ]
  },
  {
    "objectID": "05-final.html#question-1",
    "href": "05-final.html#question-1",
    "title": "6  Final Exam",
    "section": "",
    "text": "6.1.1 Data Generating Process\nWe generate synthetic data from a linear regression model with a finite sample size of N = 200 observations and p = 50 predictor variables. The response variable is generated according to\n\\[\nY = X\\beta + \\varepsilon, \\qquad \\varepsilon \\sim \\mathcal{N}(0_N, I_N),\n\\]\nwhere \\(X \\in \\mathbb{R}^{N \\times p}\\) is the design matrix and \\(\\beta \\in \\mathbb{R}^p\\) is the vector of true regression coefficients.\n\n6.1.1.1 Block structure\nThe predictor matrix is constructed by concatenating four blocks with heterogeneous correlation structures:\n\\[\nX = [\\, U \\;\\; V \\;\\; W \\;\\; Z \\,],\n\\]\nwith dimensions\n\n\\(U \\in \\mathbb{R}^{N \\times 10}\\)\n\\(V \\in \\mathbb{R}^{N \\times 10}\\)\n\\(W \\in \\mathbb{R}^{N \\times 10}\\)\n\\(Z \\in \\mathbb{R}^{N \\times 20}\\)\n\nThe coefficient vector is partitioned accordingly as\n\\[\n\\beta = (\\alpha^\\top, \\gamma^\\top, \\delta^\\top, \\zeta^\\top)^\\top ,\n\\]\nso that the data generating model can be written in block form as\n\\[\nY = U\\alpha + V\\gamma + W\\delta + Z\\zeta + \\varepsilon .\n\\]\nEach block is designed to represent a distinct regime in terms of signal strength, sparsity, and predictor correlation.\n\n\n\n\n\n\n\n\n\n\n\nBlock\nDimension\nNonzero Coefficients\nSignal Strength\nCorrelation Structure\nPurpose\n\n\n\n\nU\n10\n1 (α₁ = 3)\nStrong, sparse\nEquicorrelated (ρ = 0.95)\nAssess behavior under severe multicollinearity\n\n\nV\n10\n3 (γ₁–γ₃ = 1.5)\nModerate, sparse\nEquicorrelated (ρ = 0.50)\nCompare methods with multiple correlated signals\n\n\nW\n10\n5 (δ₁–δ₅ = 0.3)\nWeak, dense\nEquicorrelated (ρ = 0.20)\nExamine bias–variance trade-offs under weak signals\n\n\nZ\n20\n0\nNone\nIndependent (ρ = 0)\nEvaluate false positives and shrinkage of noise\n\n\n\n\n\nCode\nmake_equicorr &lt;- function(p, rho){\n  S &lt;- matrix(rho, p, p); diag(S) &lt;- 1\n  S\n}\n\ngen_data &lt;- function(n=200){\n  # block sizes\n  pU &lt;- 10; pV &lt;- 10; pW &lt;- 10; pZ &lt;- 20\n  p  &lt;- pU+pV+pW+pZ\n  \n  SU &lt;- make_equicorr(pU, 0.95)\n  SV &lt;- make_equicorr(pV, 0.50)\n  SW &lt;- make_equicorr(pW, 0.20)\n  SZ &lt;- diag(pZ)\n  \n  U &lt;- mvtnorm::rmvnorm(n, sigma=SU)\n  V &lt;- mvtnorm::rmvnorm(n, sigma=SV)\n  W &lt;- mvtnorm::rmvnorm(n, sigma=SW)\n  Z &lt;- mvtnorm::rmvnorm(n, sigma=SZ)\n  \n  X &lt;- cbind(U,V,W,Z)\n  colnames(X) &lt;- paste0(\"X\", seq_len(ncol(X)))\n\n  beta &lt;- rep(0, p)\n  # alpha\n  beta[1] &lt;- 3\n  # gamma\n  beta[(pU+1):(pU+3)] &lt;- 1.5\n  # delta\n  beta[(pU+pV+1):(pU+pV+5)] &lt;- 0.3\n  # zeta already 0\n  \n  eps &lt;- rnorm(n, 0, 1)\n  y &lt;- as.numeric(X %*% beta + eps)\n  \n  block &lt;- c(rep(\"U\",pU), rep(\"V\",pV), rep(\"W\",pW), rep(\"Z\",pZ))\n  list(X=X, y=y, beta=beta, block=block)\n}\n\n\n\n\n\n6.1.2 Models and Tuning Procedures\nAll models are fit within a single train–test split using a unified workflow. All tuning and model fitting are performed using training data only, while final performance is evaluated on held-out test data.\nPrior to model fitting, predictors are standardized using statistics computed from the training data and applied to both training and test sets to prevent data leakage.\n\n6.1.2.1 Ordinary Least Squares (OLS)\nOrdinary least squares is fit on the standardized training data without regularization:\n\\[\n\\hat{\\beta}^{\\text{OLS}} = \\arg\\min_{\\beta}\\|Y - X\\beta\\|_2^2.\n\\]\nOLS provides an unregularized reference point with minimal bias but potentially high variance, particularly in the presence of correlated predictors. Comparisons with OLS help isolate the impact of regularization.\n\n\n6.1.2.2 Ridge, Lasso, and Elastic Net\nRegularized regression models are fit using the glmnet framework. For ridge, lasso, and elastic net, the tuning parameter \\(\\lambda\\) is selected by cross-validation on the training data to minimize mean squared prediction error.\nThe estimators take the form\n\\[\n\\hat{\\beta}\n= \\arg\\min_{\\beta}\\|Y - X\\beta\\|_2^2\n+ \\lambda\\left(\n\\alpha\\|\\beta\\|_1 + (1-\\alpha)\\|\\beta\\|_2^2\n\\right),\n\\]\nwith the following special cases:\n\nRidge regression: \\(\\alpha = 0\\)\nLasso regression: \\(\\alpha = 1\\)\nElastic net: \\(\\alpha \\in (0,1)\\), fixed at a predefined value\n\nFor each method, the value of \\(\\lambda\\) corresponding to the minimum cross-validated error (\\(\\lambda_{\\text{min}}\\)) is selected, and the model is refit on the full training set at this value.\n\n\n6.1.2.3 Principal Components Regression (PCR)\nPrincipal components regression is fit by first computing the principal components of the standardized training predictors. A linear regression model is then fit using the first \\(k\\) components.\nThe number of retained components \\(k\\) is selected via cross-validation by minimizing the cross-validated root mean squared error. To reduce variance, the smallest \\(k\\) within one standard error of the minimum is chosen.\n\n\n6.1.2.4 Implementation details\nFor each fitted model, coefficient estimates and test-set predictions are stored for downstream evaluation. Tuning parameters (selected \\(\\lambda\\) or number of components \\(k\\)) are recorded to facilitate comparison across methods.\nAll subsequent analyses—including prediction accuracy, coefficient estimation error, variable selection stability, and block-wise diagnostics—are based on these fitted models.\n\n\n\n6.1.3 Evaluation Metrics\nModel performance is evaluated using repeated random train–test splits. Specifically, for each repetition \\(r = 1,\\dots,R\\) with \\(R=60\\), a new dataset is generated, randomly split into a training set (70%) and a test set (30%), and all models are fit and evaluated independently. This yields empirical distributions for all metrics, allowing uncertainty and variability to be assessed.\n\n\n6.1.3.1 Out-of-sample predictive accuracy\nFor each method and each repetition, predictive accuracy is measured using the test-set mean squared error (MSE):\n\\[\n\\text{MSE}^{(r)} = \\frac{1}{n_{\\text{test}}}\n\\sum_{i \\in \\mathcal{T}_r} (y_i - \\hat{y}_i)^2,\n\\]\nwhere \\(\\mathcal{T}_r\\) denotes the test set for repetition \\(r\\).\nTo assess block-wise predictive contributions, we compute a drop-one-block increase in test MSE. For block \\(b \\in \\{U,V,W,Z\\}\\), let \\(\\hat{y}_i^{(-b)}\\) denote predictions obtained after setting coefficients in block \\(b\\) to zero. The marginal contribution of block \\(b\\) is defined as\n\\[\n\\Delta\\text{MSE}_b^{(r)}\n= \\text{MSE}_b^{(r)} - \\text{MSE}^{(r)},\n\\]\nwhere \\(\\text{MSE}_b^{(r)}\\) is the test MSE using \\(\\hat{y}^{(-b)}\\). Positive values indicate that the block contributes to predictive accuracy.\n\n\n6.1.3.2 Coefficient estimation error\nAccuracy of coefficient estimation is evaluated using the mean squared error between estimated and true coefficients:\n\\[\n\\text{Coef-MSE}^{(r)} = \\frac{1}{p}\n\\sum_{j=1}^{p} \\bigl(\\hat{\\beta}_j^{(r)} - \\beta_j\\bigr)^2.\n\\]\nBlock-specific coefficient error is computed analogously by restricting the summation to coefficients belonging to a given block \\(b\\):\n\\[\n\\text{Coef-MSE}_b^{(r)} =\n\\frac{1}{|b|}\n\\sum_{j \\in b} \\bigl(\\hat{\\beta}_j^{(r)} - \\beta_j\\bigr)^2.\n\\]\n\n\n6.1.3.3 Variable selection stability\nFor methods with an explicit variable selection component (lasso and elastic net), selection is defined by nonzero estimated coefficients. Let \\(S^{(r)} \\subset \\{1,\\dots,p\\}\\) denote the selected set in repetition \\(r\\).\nStability of variable selection is quantified using the Jaccard similarity between selected sets from two repetitions \\(r\\) and \\(r'\\):\n\\[\nJ\\bigl(S^{(r)}, S^{(r')}\\bigr)\n= \\frac{|S^{(r)} \\cap S^{(r')}|}\n{|S^{(r)} \\cup S^{(r')}|}.\n\\]\nWe compute Jaccard similarities over many randomly chosen pairs of repetitions to obtain an empirical distribution. Block-wise selection stability is computed by restricting the selected sets to indices belonging to each block.\n\n\n6.1.3.4 Shrinkage magnitude and coefficient patterns\nTo summarize the degree of coefficient shrinkage, we compute the \\(\\ell_1\\) and \\(\\ell_2\\) norms of the estimated coefficient vector:\n\\[\n\\|\\hat{\\beta}^{(r)}\\|_1 = \\sum_{j=1}^{p} |\\hat{\\beta}_j^{(r)}|,\n\\qquad\n\\|\\hat{\\beta}^{(r)}\\|_2 =\n\\left(\\sum_{j=1}^{p} (\\hat{\\beta}_j^{(r)})^2\\right)^{1/2}.\n\\]\nIn addition, the full set of estimated coefficients \\(\\hat{\\beta}^{(r)}\\) is retained across repetitions, enabling visualization of shrinkage patterns and comparisons across blocks with differing correlation structures.\n\nAcross all metrics, comparisons are based on the empirical distributions obtained from repeated resampling rather than single-point estimates, allowing uncertainty and variability to be explicitly assessed.\n\n\nCode\nscale_train_test &lt;- function(Xtr, Xte){\n  mu &lt;- colMeans(Xtr)\n  sd &lt;- apply(Xtr, 2, sd)\n  sd[sd == 0] &lt;- 1\n\n  Xtr_s &lt;- sweep(sweep(Xtr, 2, mu, \"-\"), 2, sd, \"/\")\n  Xte_s &lt;- sweep(sweep(Xte, 2, mu, \"-\"), 2, sd, \"/\")\n\n  colnames(Xtr_s) &lt;- colnames(Xtr)\n  colnames(Xte_s) &lt;- colnames(Xtr)\n\n  list(Xtr = Xtr_s, Xte = Xte_s, mu = mu, sd = sd)\n}\n\n\nblock_mse &lt;- function(beta_hat, beta_true, block){\n  tibble(\n    block = c(\"U\",\"V\",\"W\",\"Z\"),\n    coef_mse = c(\n      mean((beta_hat[block==\"U\"]-beta_true[block==\"U\"])^2),\n      mean((beta_hat[block==\"V\"]-beta_true[block==\"V\"])^2),\n      mean((beta_hat[block==\"W\"]-beta_true[block==\"W\"])^2),\n      mean((beta_hat[block==\"Z\"]-beta_true[block==\"Z\"])^2)\n    )\n  )\n}\n\ndrop_block_delta_mse &lt;- function(Xte, yte, beta_hat, block){\n  yhat_full &lt;- as.numeric(Xte %*% beta_hat)\n  mse_full  &lt;- mean((yte - yhat_full)^2)\n  deltas &lt;- map_dbl(c(\"U\",\"V\",\"W\",\"Z\"), function(b){\n    beta2 &lt;- beta_hat\n    beta2[block==b] &lt;- 0\n    mse2 &lt;- mean((yte - as.numeric(Xte %*% beta2))^2)\n    mse2 - mse_full\n  })\n  tibble(block=c(\"U\",\"V\",\"W\",\"Z\"), delta_mse=deltas)\n}\n\nfit_one_split &lt;- function(dat, test_frac=0.3, alpha_en=0.5){\n  X &lt;- dat$X; y &lt;- dat$y; beta_true &lt;- dat$beta; block &lt;- dat$block\n  n &lt;- nrow(X)\n  id_te &lt;- sample.int(n, size=floor(test_frac*n))\n  id_tr &lt;- setdiff(seq_len(n), id_te)\n  \n  Xtr0 &lt;- X[id_tr,]; ytr &lt;- y[id_tr]\n  Xte0 &lt;- X[id_te,]; yte &lt;- y[id_te]\n  \n  sc &lt;- scale_train_test(Xtr0, Xte0)\n  Xtr &lt;- sc$Xtr; Xte &lt;- sc$Xte\n  \n  out &lt;- list()\n  \n  # ----- OLS -----\n  ols_fit &lt;- lm(ytr ~ Xtr)\n  beta_ols &lt;- coef(ols_fit)[-1]\n  yhat &lt;- as.numeric(cbind(1,Xte) %*% coef(ols_fit))\n  out$ols &lt;- list(beta=beta_ols, yhat=yhat, tune=NA)\n  \n  # ----- Ridge -----\n  cv_r &lt;- cv.glmnet(Xtr, ytr, alpha=0, standardize=FALSE)\n  b_r &lt;- as.numeric(coef(cv_r, s=\"lambda.min\"))[-1]\n  yhat_r &lt;- as.numeric(predict(cv_r, newx=Xte, s=\"lambda.min\"))\n  out$ridge &lt;- list(beta=b_r, yhat=yhat_r, tune=cv_r$lambda.min)\n  \n  # ----- Lasso -----\n  cv_l &lt;- cv.glmnet(Xtr, ytr, alpha=1, standardize=FALSE)\n  b_l &lt;- as.numeric(coef(cv_l, s=\"lambda.min\"))[-1]\n  yhat_l &lt;- as.numeric(predict(cv_l, newx=Xte, s=\"lambda.min\"))\n  out$lasso &lt;- list(beta=b_l, yhat=yhat_l, tune=cv_l$lambda.min)\n  \n  # ----- Elastic Net (alpha fixed) -----\n  cv_e &lt;- cv.glmnet(Xtr, ytr, alpha=alpha_en, standardize=FALSE)\n  b_e &lt;- as.numeric(coef(cv_e, s=\"lambda.min\"))[-1]\n  yhat_e &lt;- as.numeric(predict(cv_e, newx=Xte, s=\"lambda.min\"))\n  out$enet &lt;- list(beta=b_e, yhat=yhat_e, tune=cv_e$lambda.min)\n  \n  # ----- PCR -----\n  df_tr &lt;- data.frame(y=ytr, Xtr)\n\n  pcr_fit &lt;- pcr(y ~ ., data=df_tr, scale=FALSE, validation=\"CV\", segments=10)\n  # choose k by CV RMSEP\n  rmsep &lt;- RMSEP(pcr_fit, estimate=\"CV\")\n\n  rm &lt;- as.numeric(rmsep$val[1,1,-1, drop=TRUE])\n  se &lt;- as.numeric(rmsep$se[1,1,-1, drop=TRUE])\n  \n  k_min &lt;- which.min(rm)\n  thr &lt;- rm[k_min] + se[k_min]\n  idx &lt;- which(rm &lt;= thr)\n  \n  k_best &lt;- if(length(idx)==0) k_min else idx[1]\n  print(c(k_min=k_min, k_best=k_best))\n\n  df_te &lt;- as.data.frame(Xte)  # must have same column names as training predictors\n  yhat_p &lt;- as.numeric(predict(pcr_fit, newdata = df_te, ncomp = k_best))\n  \n  # coefficients in original X space (no intercept)\n  b_p &lt;- as.numeric(coef(pcr_fit, ncomp = k_best, intercept = FALSE))\n  out$pcr &lt;- list(beta = b_p, yhat = yhat_p, tune = k_best)\n\n  \n  # ---- assemble metrics tidy ----\n  methods &lt;- names(out)\n  res_main &lt;- map_dfr(methods, function(m){\n    bh &lt;- out[[m]]$beta\n    mse &lt;- mean((yte - out[[m]]$yhat)^2)\n    coefmse &lt;- mean((bh - beta_true)^2)\n    tibble(method=m, test_mse=mse, coef_mse=coefmse,\n           tune=out[[m]]$tune,\n           l1=sum(abs(bh)), l2=sqrt(sum(bh^2)))\n  })\n  \n  res_coef_block &lt;- map_dfr(methods, function(m){\n    block_mse(out[[m]]$beta, beta_true, block) %&gt;%\n      mutate(method=m)\n  })\n  \n  res_drop &lt;- map_dfr(methods, function(m){\n    drop_block_delta_mse(Xte, yte, out[[m]]$beta, block) %&gt;%\n      mutate(method=m)\n  })\n  \n  # selection info (lasso + enet)\n  sel &lt;- map_dfr(c(\"lasso\",\"enet\"), function(m){\n    bh &lt;- out[[m]]$beta\n    tibble(method=m,\n           n_selected=sum(bh!=0),\n           nU=sum(bh[block==\"U\"]!=0),\n           nV=sum(bh[block==\"V\"]!=0),\n           nW=sum(bh[block==\"W\"]!=0),\n           nZ=sum(bh[block==\"Z\"]!=0),\n           selected=list(as.integer(bh!=0)))\n  })\n  \n    # ---- store beta_hat for all methods (for shrinkage + correlation behavior) ----\n  beta_hat_long &lt;- map_dfr(methods, function(m){\n    tibble(\n      method = m,\n      j = seq_along(out[[m]]$beta),\n      beta_hat = out[[m]]$beta\n    )\n  })\n\n  list(main=res_main, coef_block=res_coef_block, drop=res_drop, sel=sel, beta_hat_long=beta_hat_long)\n\n}\n\njaccard &lt;- function(a,b){\n  inter &lt;- sum(a==1 & b==1)\n  uni   &lt;- sum(a==1 | b==1)\n  if(uni==0) return(1)\n  inter/uni\n}\n\njaccard_restrict &lt;- function(a, b, idx){\n  a &lt;- a[idx]; b &lt;- b[idx]\n  inter &lt;- sum(a==1 & b==1)\n  uni   &lt;- sum(a==1 | b==1)\n  if(uni==0) return(1)\n  inter/uni\n}\n\nget_jaccard_df &lt;- function(sel_df_method, n_pairs=2000){\n  mats &lt;- do.call(rbind, sel_df_method$selected)  # R x p\n  Rn &lt;- nrow(mats)\n  pairs &lt;- replicate(n_pairs, sample.int(Rn, 2))\n  js &lt;- apply(pairs, 2, \\(ij) jaccard(mats[ij[1],], mats[ij[2],]))\n  tibble::tibble(jaccard = js)\n}\n\nget_jaccard_block_df &lt;- function(sel_df_method, block_vec, n_pairs=2000){\n  mats &lt;- do.call(rbind, sel_df_method$selected)  # R x p\n  Rn &lt;- nrow(mats)\n  pairs &lt;- replicate(n_pairs, sample.int(Rn, 2))\n  \n  blocks &lt;- unique(block_vec)\n  purrr::map_dfr(blocks, function(bk){\n    idx &lt;- which(block_vec == bk)\n    js &lt;- apply(pairs, 2, \\(ij) jaccard_restrict(mats[ij[1],], mats[ij[2],], idx))\n    tibble::tibble(block=bk, jaccard=js)\n  })\n}\n\n\n\n\n\n6.1.4 Results\n\n6.1.4.1 Predictive performance and bias–variance trade-off\n\n\nCode\np_pred_overall &lt;- ggplot(main_df, aes(x=method, y=test_mse, fill=method, color=method)) +\n  geom_violin(trim=FALSE, alpha=0.25, linewidth=0) +\n  geom_boxplot(width=0.15, outlier.shape=NA, alpha=0.55, color=\"black\") +\n  geom_point(alpha=0.25, position=position_jitter(width=0.10, height=0), size=1) +\n  scale_fill_manual(values=pal, labels=method_labels) +\n  scale_color_manual(values=pal, labels=method_labels) +\n  scale_x_discrete(labels=method_labels) +\n  labs(x=NULL, y=\"Test MSE\", title=\"Out-of-sample predictive accuracy (overall)\") +\n  theme_pub\n\np_pred_overall\n\n\n\n\n\n\n\n\n\nCode\ndrop_df$block &lt;- factor(drop_df$block, levels=c(\"U\",\"V\",\"W\",\"Z\"))\n\np_pred_block &lt;- ggplot(drop_df, aes(x=method, y=delta_mse, fill=method, color=method)) +\n  geom_violin(trim=FALSE, alpha=0.25, linewidth=0) +\n  geom_boxplot(width=0.15, outlier.shape=NA, alpha=0.55, color=\"black\") +\n  facet_wrap(~block, nrow=1) +\n  scale_fill_manual(values=pal, labels=method_labels) +\n  scale_color_manual(values=pal, labels=method_labels) +\n  scale_x_discrete(labels=method_labels) +\n  labs(x=NULL, y=expression(Delta~\"MSE when dropping block\"),\n       title=\"Out-of-sample predictive contribution by block (drop-one-block ΔMSE)\") +\n  theme_pub\n\np_pred_block\n\n\n\n\n\n\n\n\n\nRegularized methods outperform OLS in out-of-sample prediction, illustrating the bias–variance trade-off in the presence of correlated predictors. While OLS is unbiased, its high variance leads to inferior generalization. Ridge reduces variance through uniform shrinkage but offers limited gains when noise variables are present. Lasso and Elastic Net achieve the best predictive performance by reducing effective model complexity through shrinkage and sparsity. PCR provides weaker improvements, indicating that dimension reduction alone is insufficient when signal is sparse at the variable level.\n\n\n6.1.4.2 Coefficient estimation accuracy\n\n\nCode\np_coef_overall &lt;- ggplot(main_df, aes(x=method, y=coef_mse, fill=method, color=method)) +\n  geom_violin(trim=FALSE, alpha=0.25, linewidth=0) +\n  geom_boxplot(width=0.15, outlier.shape=NA, alpha=0.55, color=\"black\") +\n  geom_point(alpha=0.25, position=position_jitter(width=0.10, height=0), size=1) +\n  scale_fill_manual(values=pal, labels=method_labels) +\n  scale_color_manual(values=pal, labels=method_labels) +\n  scale_x_discrete(labels=method_labels) +\n  labs(x=NULL, y=\"Coefficient MSE\", title=\"Coefficient estimation error (overall)\") +\n  theme_pub\n\np_coef_overall\n\n\n\n\n\n\n\n\n\nCode\ncoef_block_df$block &lt;- factor(coef_block_df$block, levels=c(\"U\",\"V\",\"W\",\"Z\"))\n\np_coef_block &lt;- ggplot(coef_block_df, aes(x=method, y=coef_mse, fill=method, color=method)) +\n  geom_violin(trim=FALSE, alpha=0.25, linewidth=0) +\n  geom_boxplot(width=0.15, outlier.shape=NA, alpha=0.55, color=\"black\") +\n  facet_wrap(~block, nrow=1) +\n  scale_fill_manual(values=pal, labels=method_labels) +\n  scale_color_manual(values=pal, labels=method_labels) +\n  scale_x_discrete(labels=method_labels) +\n  labs(x=NULL, y=\"Blockwise coefficient MSE\", title=\"Coefficient estimation error by block\") +\n  theme_pub\n\np_coef_block\n\n\n\n\n\n\n\n\n\nCoefficient estimation accuracy improves substantially under regularization. Lasso yields the lowest estimation error by eliminating weak and null coefficients, whereas Ridge introduces bias through uniform shrinkage of all coefficients. Elastic Net balances these effects, achieving moderate estimation error while improving stability. PCR performs poorly in coefficient recovery due to misalignment between principal components and the true sparse structure.\n\n\n6.1.4.3 Variable selection, sparsity, and stability\n\n\nCode\np_sel_stab_overall &lt;- ggplot(jac_df, aes(x=method, y=jaccard, fill=method, color=method)) +\n  geom_violin(trim=FALSE, alpha=0.25, linewidth=0) +\n  geom_boxplot(width=0.15, outlier.shape=NA, alpha=0.55, color=\"black\") +\n  scale_fill_manual(values=pal_sel, labels=c(lasso=\"Lasso\", enet=\"Elastic Net\")) +\n  scale_color_manual(values=pal_sel, labels=c(lasso=\"Lasso\", enet=\"Elastic Net\")) +\n  scale_x_discrete(labels=c(lasso=\"Lasso\", enet=\"Elastic Net\")) +\n  labs(x=NULL, y=\"Jaccard similarity\",\n       title=\"Selection stability across resamples (overall)\") +\n  theme_pub\n\np_sel_stab_overall\n\n\n\n\n\n\n\n\n\nCode\nsel_long &lt;- sel_df %&gt;%\n  select(rep, method, n_selected, nU, nV, nW, nZ) %&gt;%\n  pivot_longer(cols=c(n_selected,nU,nV,nW,nZ), names_to=\"block\", values_to=\"count\") %&gt;%\n  mutate(block = recode(block, n_selected=\"Overall\", nU=\"U\", nV=\"V\", nW=\"W\", nZ=\"Z\"),\n         block = factor(block, levels=c(\"Overall\",\"U\",\"V\",\"W\",\"Z\")))\n\np_sel_stab_block &lt;- ggplot(jac_block_df, aes(x=method, y=jaccard, fill=method, color=method)) +\n  geom_violin(trim=FALSE, alpha=0.25, linewidth=0) +\n  geom_boxplot(width=0.15, outlier.shape=NA, alpha=0.55, color=\"black\") +\n  facet_wrap(~block, nrow=1) +\n  scale_fill_manual(values=pal_sel, labels=c(lasso=\"Lasso\", enet=\"Elastic Net\")) +\n  scale_color_manual(values=pal_sel, labels=c(lasso=\"Lasso\", enet=\"Elastic Net\")) +\n  scale_x_discrete(labels=c(lasso=\"Lasso\", enet=\"Elastic Net\")) +\n  labs(x=NULL, y=\"Jaccard similarity\",\n       title=\"Selection stability across resamples (by block)\") +\n  theme_pub\n\np_sel_stab_block\n\n\n\n\n\n\n\n\n\nCode\np_sel_count &lt;- ggplot(sel_long, aes(x=method, y=count, fill=method, color=method)) +\n  geom_violin(trim=FALSE, alpha=0.25, linewidth=0) +\n  geom_boxplot(width=0.15, outlier.shape=NA, alpha=0.55, color=\"black\") +\n  facet_wrap(~block, nrow=1) +\n  scale_fill_manual(values=pal_sel, labels=c(lasso=\"Lasso\", enet=\"Elastic Net\")) +\n  scale_color_manual(values=pal_sel, labels=c(lasso=\"Lasso\", enet=\"Elastic Net\")) +\n  scale_x_discrete(labels=c(lasso=\"Lasso\", enet=\"Elastic Net\")) +\n  labs(x=NULL, y=\"# selected variables\",\n       title=\"Selection sparsity (counts) overall and by block\") +\n  theme_pub\n\np_sel_count\n\n\n\n\n\n\n\n\n\nCode\njac_block_df$block &lt;- factor(jac_block_df$block, levels=c(\"U\",\"V\",\"W\",\"Z\"))\n\n\nLasso produces sparse but unstable variable selection, particularly in highly correlated predictor blocks. Elastic Net selects larger but more stable sets of variables by encouraging grouped selection among correlated predictors. This highlights a fundamental trade-off between sparsity and stability in penalized regression.\n\n\n6.1.4.4 Shrinkage behavior\n\n\nCode\np_shrink_overall &lt;- ggplot(main_df, aes(x=method, y=l2, fill=method, color=method)) +\n  geom_violin(trim=FALSE, alpha=0.25, linewidth=0) +\n  geom_boxplot(width=0.15, outlier.shape=NA, alpha=0.55, color=\"black\") +\n  scale_fill_manual(values=pal, labels=method_labels) +\n  scale_color_manual(values=pal, labels=method_labels) +\n  scale_x_discrete(labels=method_labels) +\n  labs(x=NULL, y=expression(paste(\"||\", hat(beta), \"||\"[2])),\n       title=\"Shrinkage magnitude (overall L2 norm)\") +\n  theme_pub\n\np_shrink_overall\n\n\n\n\n\n\n\n\n\nCode\nblock_l2_df$block &lt;- factor(block_l2_df$block, levels=c(\"U\",\"V\",\"W\",\"Z\"))\nblock_l2_df$method &lt;- factor(block_l2_df$method, levels=method_levels)\n\np_shrink_block &lt;- ggplot(block_l2_df, aes(x=method, y=block_l2, fill=method, color=method)) +\n  geom_violin(trim=FALSE, alpha=0.25, linewidth=0) +\n  geom_boxplot(width=0.15, outlier.shape=NA, alpha=0.55, color=\"black\") +\n  facet_wrap(~block, nrow=1) +\n  scale_fill_manual(values=pal, labels=method_labels) +\n  scale_color_manual(values=pal, labels=method_labels) +\n  scale_x_discrete(labels=method_labels) +\n  labs(x=NULL, y=expression(paste(\"||\", hat(beta)[block], \"||\"[2])),\n       title=\"Shrinkage magnitude by block (L2 norm)\") +\n  theme_pub\n\np_shrink_block\n\n\n\n\n\n\n\n\n\nCode\n# need beta_hat_df + true beta vector and block vector\nbeta_true_df &lt;- tibble(j=seq_along(dat$beta), beta_true=dat$beta, block=dat$block)\n\nbeta_mean_df &lt;- beta_hat_df %&gt;%\n  group_by(method, j, block) %&gt;%\n  summarise(beta_hat_mean = mean(beta_hat), .groups=\"drop\") %&gt;%\n  left_join(beta_true_df, by=c(\"j\",\"block\"))\n\np_pattern_scatter &lt;- ggplot(beta_mean_df, aes(x=beta_true, y=beta_hat_mean, color=block)) +\n  geom_hline(yintercept=0, linewidth=0.3, alpha=0.5) +\n  geom_vline(xintercept=0, linewidth=0.3, alpha=0.5) +\n  geom_point(alpha=0.7, size=1.4) +\n  facet_wrap(~method, nrow=1, labeller=as_labeller(method_labels)) +\n  labs(x=expression(beta[j]~\"(true)\"),\n       y=expression(bar(hat(beta))[j]~\"(mean over resamples)\"),\n       title=\"Shrinkage pattern: true vs estimated coefficients (colored by block)\") +\n  theme_pub\n\np_pattern_scatter\n\n\n\n\n\n\n\n\n\nShrinkage behavior differs markedly across methods: Ridge applies strong global shrinkage, Lasso induces exact zeros, Elastic Net combines both mechanisms, and PCR lacks direct shrinkage in the original predictor space. In highly correlated blocks, OLS and PCR exhibit substantial coefficient instability, Ridge stabilizes estimates via shrinkage, and Elastic Net mitigates instability through grouped selection, whereas Lasso remains sensitive to correlation structure.\n\n\n6.1.4.5 Effects of correlated predictors\n\n\nCode\nbeta_sd_var$block &lt;- factor(beta_sd_var$block, levels=c(\"U\",\"V\",\"W\",\"Z\"))\nbeta_sd_var$method &lt;- factor(beta_sd_var$method, levels=method_levels)\n\np_corr_behavior &lt;- ggplot(beta_sd_var, aes(x=block, y=sd_beta, fill=block)) +\n  geom_boxplot(outlier.shape=NA, alpha=0.55) +\n  geom_jitter(alpha=0.20, width=0.15, size=1) +\n  facet_wrap(~method, nrow=1, labeller=as_labeller(method_labels)) +\n  labs(x=\"Block\", y=expression(sd(hat(beta)[j])),\n       title=\"Effect of correlation: coefficient instability by block\") +\n  theme_pub +\n  theme(legend.position=\"none\")\n\np_corr_behavior\n\n\n\n\n\n\n\n\n\nCorrelation among predictors has a pronounced impact on model behavior. In highly correlated blocks, OLS and PCR exhibit substantial coefficient instability due to near-nonidentifiability of the regression problem.\n\n\n\n6.1.5 Summary\n\n\nCode\nlibrary(kableExtra)\nsummary_table &lt;- main_df %&gt;%\n  group_by(method) %&gt;%\n  summarise(\n    `Test MSE` = mean(test_mse),\n    `Coef MSE` = mean(coef_mse),\n    `||β̂||₂` = mean(l2),\n    .groups = \"drop\"\n  )\nknitr::kable(\n  summary_table,\n  digits = 3,\n  caption = \"Overall predictive and estimation performance across methods\",\n  col.names = c(\"Method\", \"Test MSE\", \"Coefficient MSE\", \"L2 Norm of β̂\")\n)\n\n\n\nOverall predictive and estimation performance across methods\n\n\nMethod\nTest MSE\nCoefficient MSE\nL2 Norm of β̂\n\n\n\n\nols\n1.570\n0.049\n4.267\n\n\nridge\n1.662\n0.085\n2.878\n\n\nlasso\n1.223\n0.012\n3.660\n\n\nenet\n1.277\n0.024\n3.416\n\n\npcr\n1.581\n0.055\n4.189\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nPrediction\nCoefficient Accuracy\nSparsity\nSelection Stability\nCorrelated Predictors\nKey Trade-off\n\n\n\n\nOLS\nPoor\nModerate\nNone\nN/A\nVery unstable\nUnbiased but high variance\n\n\nRidge\nModerate\nPoor–Moderate\nNone\nN/A\nStable shrinkage\nVariance reduction without sparsity\n\n\nLasso\nBest\nBest\nHigh\nLow\nUnstable selection\nSparsity vs stability\n\n\nElastic Net\nBest\nModerate–Good\nModerate\nHigh\nGrouped selection\nBalanced regularization\n\n\nPCR\nModerate\nPoor\nImplicit\nN/A\nDepends on alignment\nDimension reduction vs sparsity\n\n\n\nThis exercise highlights how different regularization strategies trade off bias, variance, sparsity, and stability in a structured, correlated predictor setting.\nOverall, methods that explicitly impose sparsity—lasso and elastic net—achieve superior out-of-sample predictive accuracy and substantially lower coefficient estimation error compared to unregularized OLS and dimension-reduction-based PCR. In particular, lasso performs best in terms of both prediction and estimation when the true signal is sparse, as it effectively suppresses noise variables while retaining the dominant coefficients.\nRidge regression exhibits the strongest overall shrinkage and the most stable coefficient estimates under high within-block correlation, but this comes at the cost of higher estimation bias and weaker variable discrimination. Elastic net balances these behaviors by combining \\(\\ell_1\\)-induced sparsity with \\(\\ell_2\\)-based stabilization, leading to improved selection stability relative to lasso while maintaining competitive predictive performance.\nThe blockwise analyses further clarify the interaction between correlation structure and regularization. Highly correlated blocks benefit from shrinkage-based methods, while sparsity-driven methods more accurately identify truly active predictors within moderately correlated blocks. Blocks containing no signal are consistently downweighted, especially by lasso and elastic net, demonstrating effective regularization against overfitting.\nFinally, selection stability results show that elastic net produces more consistent variable selection across resamples than lasso, particularly in correlated blocks, underscoring the practical importance of combining sparsity and grouping effects.\nTaken together, these results emphasize that no single regularization method is uniformly optimal. Instead, the appropriate choice depends on the underlying signal structure, correlation patterns, and the relative importance of prediction accuracy, interpretability, and stability.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Final Exam</span>"
    ]
  },
  {
    "objectID": "05-final.html#question-2",
    "href": "05-final.html#question-2",
    "title": "6  Final Exam",
    "section": "6.2 Question 2",
    "text": "6.2 Question 2\n\n6.2.1 Data and setup\nWe consider three data-generating settings to examine regularization and degrees of freedom under different modeling regimes. Two of the settings follow the simulation designs of Mentch and Zhou (2020), while the third corresponds exactly to the block-correlated linear model used in Q1. Model performance is evaluated using out-of-sample mean squared error (MSE).\n\n\n6.2.1.1 Linear Medium setting (Mentch and Zhou, 2020)\nWe consider a sparse linear regression model with \\[\nn = 500, \\quad p = 100, \\quad s = 5.\n\\] The response is generated according to \\[\nY = X\\beta + \\varepsilon,\n\\] where \\[\n\\beta_j =\n\\begin{cases}\n1, & j = 1,\\dots,s, \\\\\n0, & j = s+1,\\dots,p,\n\\end{cases}\n\\quad\n\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2).\n\\] The noise variance \\(\\sigma^2\\) is chosen to achieve a fixed signal-to-noise ratio, \\[\n\\mathrm{SNR} = \\frac{\\mathrm{Var}(X\\beta)}{\\sigma^2}.\n\\] This setting represents a regime in which the linear model is correctly specified.\n\n\n\n6.2.1.2 MARSadd setting (Mentch and Zhou, 2020)\nTo study a nonlinear data-generating mechanism, we consider the MARSadd model with \\[\nn = 500, \\quad p = s = 5,\n\\] where predictors are sampled independently from \\(\\mathrm{Unif}(0,1)\\))\\(. The response is generated as\\)$ Y = 0.1 + 3X_3 + 2X_4 + X_5 + , (0,^2), $$ with \\(\\sigma^2\\) chosen to control the signal-to-noise ratio. This setting introduces nonlinear and interaction effects that cannot be represented by linear models.\n\n\n6.2.1.3 Block-correlated linear setting (Q1)\nThe third setting follows exactly the block-correlated linear data-generating process used in Q1. Predictors are partitioned into blocks with different within-block correlation structures, and the response is generated according to a sparse linear model with additive Gaussian noise. All aspects of this setting are identical to Q1 and are not repeated here.\n\n\n6.2.1.4 Train–test split\nFor each setting, the data are randomly split into training and test sets. Models are fit on the training data, and predictive performance is evaluated on the test data using mean squared error. Results are averaged over repeated fits at each regularization level to reduce Monte Carlo variability.\n\n\nCode\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(glmnet)\nlibrary(ranger)\n\ngen_linear_medium &lt;- function(n = 500, p = 100, s = 5, snr = 1, seed = NULL) {\n  if (!is.null(seed)) set.seed(seed)\n\n  X &lt;- matrix(runif(n * p), n, p)   # same \"Unif(0,1)\" style\n  beta &lt;- c(rep(1, s), rep(0, p - s))\n  f &lt;- as.numeric(X %*% beta)\n\n  # choose sigma^2 to hit target SNR = Var(f)/sigma^2\n  sigma2 &lt;- var(f) / snr\n  y &lt;- f + rnorm(n, sd = sqrt(sigma2))\n\n  list(X = X, y = y, name = sprintf(\"Linear Medium (n=%d,p=%d,s=%d,SNR=%g)\", n, p, s, snr))\n}\n\ngen_marsadd &lt;- function(n = 500, snr = 1, seed = NULL) {\n  if (!is.null(seed)) set.seed(seed)\n\n  p &lt;- 5\n  X &lt;- matrix(runif(n * p), n, p)\n\n  # One common \"MARSadd\" style nonlinear signal (matches the spirit of the paper)\n  f &lt;- 0.1 * exp(4 * X[, 1]) / (1 + exp(-20 * (X[, 2] - 0.5))) +\n       3 * X[, 3] + 2 * X[, 4] + 1 * X[, 5]\n\n  sigma2 &lt;- var(f) / snr\n  y &lt;- f + rnorm(n, sd = sqrt(sigma2))\n\n  list(X = X, y = y, name = sprintf(\"MARSadd (n=%d,p=5,SNR=%g)\", n, snr))\n}\n\n\n\n\n\n6.2.2 Methods\nWe compare ridge regression and random forests under varying levels of regularization. For each method, we examine how regularization affects effective degrees of freedom and out-of-sample predictive accuracy.\n\n\n6.2.2.1 Ridge regression\nRidge regression estimates regression coefficients by solving \\[\n\\hat{\\beta}_\\lambda\n= \\arg\\min_{\\beta}\n\\left\\{\n\\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_2^2\n\\right\\},\n\\] where \\(\\lambda &gt; 0\\) is the regularization parameter. Larger values of \\(\\lambda\\) impose stronger shrinkage and reduce model complexity.\nFor ridge regression, the effective degrees of freedom admit a closed-form expression. Let \\(d_1,\\dots,d_p\\) denote the singular values of the standardized design matrix \\(X\\). The degrees of freedom are given by \\[\n\\mathrm{DoF}_{\\text{ridge}}(\\lambda)\n= \\sum_{j=1}^p \\frac{d_j^2}{d_j^2 + \\lambda}.\n\\] This quantity can be interpreted as the effective number of parameters used by the model.\n\n\n6.2.2.2 Random forests\nRandom forests are ensembles of decision trees constructed using bootstrap samples and randomized splitting rules. Following Mentch and Zhou (2020), we treat the number of candidate predictors considered at each split, denoted mtry, as a regularization parameter. Smaller values of mtry restrict the set of allowable splits and lead to stronger regularization, while larger values increase model flexibility.\nDegrees of freedom for random forests are defined in terms of the sensitivity of fitted values to perturbations in the response: \\[\n\\mathrm{DoF}_{\\text{RF}}\n= \\sum_{i=1}^n \\frac{\\partial \\hat{y}_i}{\\partial y_i}.\n\\] Because this quantity is not available in closed form, it is estimated using a Monte Carlo perturbation approach. Specifically, for a small perturbation vector \\(\\omega\\), \\[\n\\mathrm{DoF}_{\\text{RF}}\n\\approx \\mathbb{E}\\left[\n\\frac{(\\hat{f}(y+\\omega) - \\hat{f}(y))^\\top \\omega}{\\|\\omega\\|_2^2}\n\\right],\n\\] where the expectation is approximated by repeated random perturbations.\n\n\n\n6.2.2.3 Model evaluation\nFor each data-generating setting, models are fit on a training set and evaluated on a held-out test set. Predictive accuracy is measured using test mean squared error (MSE). To reduce Monte Carlo variability arising from random forest fitting and the estimation of degrees of freedom, model fitting is repeated multiple times at each regularization level, and results are summarized using average test MSE and average degrees of freedom.\n\n\nCode\nridge_df &lt;- function(X, lambda) {\n  Xs &lt;- scale(X, center = TRUE, scale = TRUE)\n  d  &lt;- svd(Xs, nu = 0, nv = 0)$d\n  sum(d^2 / (d^2 + lambda))\n}\n\nrf_df_mc &lt;- function(X, y, mtry, num.trees = 50, tau = 0.1 * sd(y), B = 15) {\n  dat0 &lt;- data.frame(y = y, X)\n  fit0 &lt;- ranger(y ~ ., data = dat0, num.trees = num.trees, mtry = mtry)\n  yhat0 &lt;- predict(fit0, data = dat0)$predictions\n\n  vals &lt;- replicate(B, {\n    omega &lt;- rnorm(length(y), sd = tau)\n    datb &lt;- data.frame(y = y + omega, X)\n    fitb &lt;- ranger(y ~ ., data = datb, num.trees = num.trees, mtry = mtry)\n    yhatb &lt;- predict(fitb, data = dat0)$predictions\n    sum((yhatb - yhat0) * omega) / (tau^2)\n  })\n\n  mean(vals)\n}\nrun_setting_simple &lt;- function(dat,\n                               test_frac = 0.3,\n                               lambda_grid = 10^seq(-4, 4, length.out = 30),\n                               num.trees = 200,\n                               mtry_grid = NULL,\n                               B = 20,\n                               seed = 123) {\n\n  set.seed(seed)\n  X &lt;- dat$X; y &lt;- dat$y\n  n &lt;- nrow(X); p &lt;- ncol(X)\n\n  # one fixed split\n  idx &lt;- sample(seq_len(n), size = floor((1 - test_frac) * n))\n  Xtr &lt;- X[idx, , drop = FALSE]; ytr &lt;- y[idx]\n  Xte &lt;- X[-idx, , drop = FALSE]; yte &lt;- y[-idx]\n\n  if (is.null(mtry_grid)) {\n    mtry_grid &lt;- unique(round(c(1, 2, 3, 5, 8, 12, sqrt(p), p)))\n    mtry_grid &lt;- mtry_grid[mtry_grid &gt;= 1 & mtry_grid &lt;= p]\n  }\n\n  tau0 &lt;- 0.1 * sd(ytr)\n\n  # ---- Ridge (simple loop) ----\n  ridge_res &lt;- purrr::map_dfr(lambda_grid, function(lam) {\n    fit  &lt;- glmnet::glmnet(Xtr, ytr, alpha = 0, lambda = lam, standardize = TRUE)\n    pred &lt;- as.numeric(predict(fit, newx = Xte, s = lam))\n\n    tibble::tibble(\n      method    = \"Ridge\",\n      reg_param = lam,\n      df        = ridge_df(Xtr, lam),\n      test_mse  = mean((yte - pred)^2)\n    )\n  })\n\n  # ---- Random Forest (simple loop) ----\n  rf_res &lt;- purrr::map_dfr(mtry_grid, function(mtry_val) {\n    dat_tr &lt;- data.frame(y = ytr, Xtr)\n    dat_te &lt;- data.frame(Xte)\n\n    fit  &lt;- ranger::ranger(y ~ ., data = dat_tr,\n                           num.trees = num.trees, mtry = mtry_val)\n    pred &lt;- predict(fit, data = dat_te)$predictions\n\n    df_hat &lt;- rf_df_mc(\n      X = as.data.frame(Xtr), y = ytr,\n      mtry = mtry_val, num.trees = num.trees,\n      tau = tau0, B = B\n    )\n\n    tibble::tibble(\n      method    = \"Random Forest\",\n      reg_param = mtry_val,\n      df        = df_hat,\n      test_mse  = mean((yte - pred)^2)\n    )\n  })\n\n  dplyr::bind_rows(ridge_res, rf_res) %&gt;%\n    dplyr::mutate(setting = dat$name, p = p)\n}\n\n\n\n\n\n6.2.3 Regularization behavior\nWe examine how regularization parameters in ridge regression and random forests map to effective degrees of freedom, following the framework of Mentch and Zhou (2020).\n\n\n6.2.3.1 Ridge regression\nIn ridge regression, regularization is controlled by the penalty parameter \\(\\lambda &gt; 0\\).\nThe effective degrees of freedom are given by\n\\[\n\\mathrm{DoF}_{\\text{ridge}}(\\lambda)\n= \\sum_{j=1}^p \\frac{d_j^2}{d_j^2 + \\lambda},\n\\]\nwhere \\(d_1, \\dots, d_p\\) denote the singular values of the standardized design matrix.\nAs \\(\\lambda \\to 0\\), the penalty vanishes and\n\\[\n\\mathrm{DoF}_{\\text{ridge}}(\\lambda) \\to \\mathrm{rank}(X),\n\\]\nwhile as \\(\\lambda \\to \\infty\\),\n\\[\n\\mathrm{DoF}_{\\text{ridge}}(\\lambda) \\to 0.\n\\]\nThus, ridge regression spans a wide and continuous range of effective model complexity, enabling smooth traversal of the bias–variance trade-off.\n\n\n6.2.3.2 Random forests\nFor random forests, regularization is induced by restricting the number of candidate predictors available at each split. Following Mentch and Zhou (2020), we parameterize mtry as a proportion of eligible predictors,\n\\[\n\\text{mtry} = \\alpha p, \\quad \\alpha \\in (0,1],\n\\]\nwhere \\(p\\) denotes the total number of predictors and \\(\\alpha\\) controls the strength of regularization. Smaller values of \\(\\alpha\\) correspond to stronger regularization, while \\(\\alpha = 1\\) recovers the fully greedy splitting rule.\nDegrees of freedom for random forests are defined as the sensitivity of fitted values to perturbations in the response,\n\\[\n\\mathrm{DoF}_{\\text{RF}}\n= \\sum_{i=1}^n \\frac{\\partial \\hat{y}_i}{\\partial y_i}.\n\\]\nBecause this quantity is not available in closed form, it is estimated using a Monte Carlo perturbation approach,\n\\[\n\\mathrm{DoF}_{\\text{RF}}\n\\approx \\mathbb{E}\\left[\n\\frac{\\left(\\hat{f}(y + \\omega) - \\hat{f}(y)\\right)^\\top \\omega}{\\|\\omega\\|_2^2}\n\\right],\n\\]\nwhere \\(\\omega\\) is a small random perturbation vector and the expectation is approximated empirically.\n\n\nCode\ndat_lin  &lt;- gen_linear_medium(n = 500, p = 100, s = 5, snr = 1, seed = 1)\ndat_lin$name &lt;- \"Linear\"\n\ndat_mars &lt;- gen_marsadd(n = 500, snr = 1, seed = 1)\ndat_mars$name &lt;- \"MARS-additive\"\n\ndat_bc &lt;- gen_data(200)\ndat_bc$name &lt;- \"Block-correlated\"\nres_lin  &lt;- run_setting_simple(dat_lin,  seed = 123)\nres_mars &lt;- run_setting_simple(dat_mars, seed = 123)\nres_bc   &lt;- run_setting_simple(dat_bc,   seed = 123)\n\nall_res &lt;- dplyr::bind_rows(res_lin, res_mars, res_bc)\n\n\n\n\n6.2.3.3 Comparison of regularization effects\nAlthough both \\(\\lambda\\) and \\(\\alpha\\) act as regularization parameters, they induce fundamentally different complexity scales. The ridge penalty directly shrinks the linear coefficient vector and allows the model to access both low- and high-complexity regimes. In contrast, varying \\(\\alpha\\) modulates randomness and adaptivity in random forests but does not collapse the model into a globally smooth estimator. As a result, the range of achievable degrees of freedom for random forests is typically narrower and concentrated in a higher-complexity regime than that of ridge regression.\n\n\n\n6.2.4 Results\nWe summarize the relationship between degrees of freedom and out-of-sample predictive accuracy for ridge regression and random forests across the three data-generating settings.\n\n\nCode\nggplot(all_res, aes(x = df, y = test_mse, color = method, shape = method)) +\n  geom_point(size = 2.8) +\n  facet_wrap(~ setting, scales = \"free_x\") +\n  theme_minimal(base_size = 13) +\n  labs(\n    x = \"Degrees of freedom\",\n    y = \"Out-of-sample predictive error (Test MSE)\",\n    color = \"Method\",\n    shape = \"Method\"\n  )\n\n\n\n\n\n\n\n\n\nAcross all three settings, ridge regression and random forests exhibit distinct and largely non-overlapping degrees-of-freedom regimes. Ridge regression spans a wide range of complexity levels and adapts smoothly to the underlying structure when the linear model is appropriate. Random forests, even under strong regularization via mtry, operate in a higher-complexity regime and display a more limited range of effective degrees of freedom.\n\n\n6.2.5 Discussion & Conclusion\nComparing models at equivalent degrees of freedom is conceptually appealing because it aims to control for model complexity, i.e., \\[\n\\mathrm{DoF}_A \\approx \\mathrm{DoF}_B\n\\;\\Rightarrow\\;\n\\text{compare test error at “similar complexity.”}\n\\] In practice, our results suggest this is often not a meaningful comparison between ridge regression and random forests.\nFirst, matched degrees of freedom are frequently unattainable. Ridge spans a broad complexity range via \\(\\lambda\\), \\[\n\\mathrm{DoF}_{\\text{ridge}}(\\lambda)=\\sum_{j=1}^p \\frac{d_j^2}{d_j^2+\\lambda},\n\\qquad\n\\mathrm{DoF}_{\\text{ridge}}(\\lambda)\\in(0,\\mathrm{rank}(X)],\n\\] whereas random forests regularized through \\[\n\\text{mtry}=\\alpha p,\\quad \\alpha\\in(0,1]\n\\] tend to remain in a comparatively high degrees-of-freedom regime under the Mentch–Zhou definition \\[\n\\mathrm{DoF}_{\\text{RF}}=\\sum_{i=1}^n \\frac{\\partial \\hat{y}_i}{\\partial y_i}.\n\\] Across all three settings, the achievable DoF ranges show little overlap, making “matched-DoF” comparisons infeasible.\nSecond, even if DoF could be matched, it would not guarantee comparable expressiveness. Ridge remains linear, \\[\n\\hat f_{\\text{ridge}}(x)=x^\\top \\hat\\beta_\\lambda,\n\\] so under nonlinear data-generating mechanisms (e.g., MARSadd), increasing DoF within the linear class cannot remove misspecification: \\[\n\\inf_{\\beta}\\mathbb{E}\\big[(f^*(X)-X^\\top\\beta)^2\\big] &gt; 0.\n\\]\nOverall, degrees of freedom are a useful within-method complexity diagnostic, but they are not a generally reliable basis for cross-model comparison between ridge regression and random forests.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Final Exam</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "7  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  }
]