{
  "hash": "0ff6511f89209b3a4ba0ecda1ff0d1f3",
  "result": {
    "engine": "knitr",
    "markdown": "# 4 Models\n\n## Generalized Linear Model (GLM)\n\n### Motivating Ideas\n\nThe **Generalized Linear Model (GLM)** represents a major unifying step in the history of statistics.  \nBefore 1972, researchers used a collection of specialized methods for different kinds of data:\n\n- **Continuous data:** Multiple linear regression (Normal distribution, identity link)  \n- **Group mean comparisons:** ANOVA (Normal distribution, identity link)  \n- **Binary data:** Logistic or probit regression (Binomial distribution, logit/probit link)  \n- **Count data:** Poisson regression (Poisson distribution, log link)\n\nEach model had its own estimation rules and assumptions.  \nNelder and Wedderburn (1972) proposed GLMs as a **single framework** that could describe all of these models through three shared components:\n\n1. **A random component:**  \n   The response variable $Y_i$ follows a distribution from the *exponential family* (e.g., Normal, Binomial, Poisson, Gamma).\n\n2. **A systematic component:**  \n   Predictors enter linearly through  \n   $$ \n   \\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}.\n   $$\n\n3. **A link function:**  \n   Connects the expected value $\\mu_i = E[Y_i]$ to the linear predictor:  \n   $$\n   g(\\mu_i) = \\eta_i.\n   $$\n\nWith this formulation, the same estimation algorithm — **Iteratively Reweighted Least Squares (IRLS)** — can be used across models.  \nGLMs thus generalized linear regression to non-normal data while keeping the interpretability of regression coefficients.\n\n\n### Chronology of Key Ideas\n\nAdapted from Lindsey’s summary of *McCullagh & Nelder* (who themselves drew from *Stiegler*), the historical path toward GLMs developed gradually:\n\n| Period | Development | Distribution & Link | Key Contributors |\n|:-------|:-------------|:--------------------|:------------------|\n| **Early 19th century** | Multiple linear regression — foundation of least squares. | Normal, identity | Legendre, Gauss |\n| **1920s–1935** | ANOVA formalized — partitioning of variance. | Normal, identity | Fisher |\n| **1922** | Likelihood function introduced — general approach to inference. | Any | Fisher |\n| **1922** | Dilution assays for dose–response data. | Binomial, complementary log–log | Fisher |\n| **1934** | Exponential family identified — distributions with sufficient statistics. | — | Fisher |\n| **1935** | Probit analysis for quantal response data. | Binomial, probit | Bliss |\n| **1944–1952** | Logit model for proportions. | Binomial, logit | Berkson; Dyke & Patterson |\n| **1960** | Item response theory (Rasch model). | Bernoulli, logit | Rasch |\n| **1963** | Log-linear models for count data. | Poisson, log | Birch |\n| **1965–1967** | Regression models for survival data. | Exponential, log or reciprocal | Feigl & Zelen; Zippin & Armitage; Glasser |\n| **1966** | Inverse polynomials extended regression to Gamma data. | Gamma, reciprocal | Nelder |\n| **1972** | *Generalized Linear Models* unified all the above under one theory and algorithm. | Exponential family, general link | Nelder & Wedderburn |\n\n\n## Conceptual Unification\n\nBy the early 1970s, it became clear that many well-known models were specific cases of a broader principle.  \nNelder and Wedderburn (1972) formally expressed this as:\n\n$$\ng(\\mu_i) = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}, \\quad\nY_i \\sim \\text{Exponential Family}(\\mu_i, \\phi)\n$$\n\nTheir key insight:\n\n- All these models (linear, logistic, Poisson, Gamma, etc.) share the same likelihood structure.  \n- They can all be estimated through the **same maximum-likelihood algorithm**.  \n- Diagnostics, residuals, and hypothesis tests could be made consistent across model types.\n\nIn short, GLMs provided the long-sought **unification of statistical modeling**, bringing together 150 years of development—from **Gauss’s least squares** and **Fisher’s likelihood theory** to **modern regression frameworks**.\n\n\n## References\n\n- Legendre, A. M. (1805). *Nouvelles méthodes pour la détermination des orbites des comètes.*  \n- Gauss, C. F. (1809). *Theoria motus corporum coelestium in sectionibus conicis solem ambientium.*  \n- Fisher, R. A. (1922). *On the mathematical foundations of theoretical statistics.* *Philosophical Transactions of the Royal Society A.*  \n- Bliss, C. I. (1935). *The method of probits.* *Science*, 79(2037), 38–39.  \n- Berkson, J. (1944). *Application of the logistic function to bio-assay.* *JASA*, 39(227), 357–365.  \n- Rasch, G. (1960). *Probabilistic models for some intelligence and attainment tests.*  \n- Birch, M. W. (1963). *Maximum likelihood in three-way contingency tables.* *JASA*, 58, 1071–1081.  \n- Feigl, P., & Zelen, M. (1965). *Estimation of exponential survival probabilities with concomitant information.* *Biometrics*, 21, 826–838.  \n- Nelder, J. A., & Wedderburn, R. W. M. (1972). *Generalized Linear Models.* *JRSS A*, 135(3), 370–384.  \n- McCullagh, P., & Nelder, J. A. (1989). *Generalized Linear Models* (2nd ed.). Chapman & Hall.  \n- Lindsey, J. K. (1997). *Applying Generalized Linear Models.* Springer.  \n\n## Expressing the GLM Mathematically\n\nA **single-parameter exponential family** can be expressed as:\n\n$$\nf_X(x \\mid \\theta)\n= h(x)\\,\\exp\\!\\left[\\eta(\\theta)\\,T(x) - A(\\theta)\\right],\n$$\n\nor equivalently,\n\n$$\n\\log f_X(x \\mid \\theta)\n= \\eta(\\theta)\\,T(x) - A(\\theta) + B(x).\n$$\n\n### Components\n\n| Symbol | Role | GLM Interpretation |\n|:--------|:------|:------------------|\n| $$T(x)$$ | Sufficient statistic | Response variable $$Y_i$$ |\n| $$h(x)$$ or $$B(x)$$ | Base measure | Constant term $$c(y_i, \\phi)$$ |\n| $$\\eta(\\theta)$$ | Natural (canonical) parameter | Linear predictor $$\\eta_i = X_i^\\top \\beta$$ under canonical link |\n| $$A(\\theta)$$ | Cumulant (log-partition) function | Determines mean and variance |\n| $$\\theta$$ | Model parameter | Canonical parameter of the distribution |\n\nFor any exponential-family member:\n\n$$\n\\mathbb{E}[T(X)] = A'(\\theta),\n$$\n\n$$\n\\mathrm{Var}[T(X)] = A''(\\theta).\n$$\n\nThus, for GLMs:\n\n$$\n\\mu_i = \\mathbb{E}[Y_i] = A'(\\theta_i),\n$$\n\n$$\n\\mathrm{Var}(Y_i) = a(\\phi) A''(\\theta_i),\n$$\n\nand the **link function** connects the mean to the predictors:\n\n$$\ng(\\mu_i) = \\eta_i = X_i^\\top \\beta.\n$$\n\nIf the link is canonical, then:\n\n$$\ng(\\mu_i) = \\theta_i.\n$$\n\n---\n\n### Binomial Distribution (Logistic GLM)\n\n$$\nY_i \\sim \\text{Binomial}(n_i, p_i)\n$$\n\nFor simplicity, we often take $$n_i = 1$$ so that:\n\n$$\nY_i \\sim \\text{Binomial}(1, p_i).\n$$\n\nThe probability mass function is:\n\n$$\nf_Y(y_i \\mid p_i) = \\binom{n_i}{y_i} p_i^{y_i} (1 - p_i)^{n_i - y_i}.\n$$\n\nThis can be written in exponential-family form:\n\n$$\nf_Y(y_i \\mid \\theta_i)\n= h(y_i)\\,\\exp\\!\\left[y_i \\theta_i - A(\\theta_i)\\right],\n$$\n\nwhere:\n\n$$\n\\theta_i = \\log\\!\\frac{p_i}{1 - p_i},\n$$\n\n$$\nA(\\theta_i) = n_i \\log(1 + e^{\\theta_i}),\n$$\n\n$$\nh(y_i) = \\binom{n_i}{y_i}.\n$$\n\nThen the mean and variance follow from derivatives of $$A(\\theta_i)$$:\n\n$$\n\\mu_i = A'(\\theta_i) = n_i \\frac{e^{\\theta_i}}{1 + e^{\\theta_i}} = n_i p_i,\n$$\n\n$$\n\\mathrm{Var}(Y_i) = A''(\\theta_i) = n_i p_i (1 - p_i).\n$$\n\nThe **canonical link function** is the logit:\n\n$$\ng(p_i) = \\log\\!\\frac{p_i}{1 - p_i} = \\eta_i = X_i^\\top \\beta,\n$$\n\nand the **inverse link** (mean function) is:\n\n$$\np_i = \\frac{e^{\\eta_i}}{1 + e^{\\eta_i}}.\n$$\n\n---\n\n### Poisson Distribution (Log-linear GLM)\n\n$$\nY_i \\sim \\text{Poisson}(\\lambda_i)\n$$\n\nThe probability mass function is:\n\n$$\nf_Y(y_i \\mid \\lambda_i) = \\frac{\\lambda_i^{y_i} e^{-\\lambda_i}}{y_i!}.\n$$\n\nThis can be expressed in exponential-family form:\n\n$$\nf_Y(y_i \\mid \\theta_i)\n= h(y_i)\\,\\exp\\!\\left[y_i \\theta_i - A(\\theta_i)\\right],\n$$\n\nwhere:\n\n$$\n\\theta_i = \\log \\lambda_i,\n$$\n\n$$\nA(\\theta_i) = e^{\\theta_i},\n$$\n\n$$\nh(y_i) = \\frac{1}{y_i!}.\n$$\n\nFrom these we derive:\n\n$$\n\\mu_i = A'(\\theta_i) = e^{\\theta_i} = \\lambda_i,\n$$\n\n$$\n\\mathrm{Var}(Y_i) = A''(\\theta_i) = e^{\\theta_i} = \\lambda_i.\n$$\n\nThe **canonical link** is the log link:\n\n$$\ng(\\mu_i) = \\log \\mu_i = \\eta_i = X_i^\\top \\beta,\n$$\n\nand the **inverse link** is:\n\n$$\n\\mu_i = e^{\\eta_i}.\n$$\n\nIf we model **rates** with exposure $$E_i$$:\n\n$$\n\\log \\lambda_i = \\log E_i + X_i^\\top \\beta,\n$$\n\nor equivalently:\n\n$$\n\\lambda_i = E_i\\,e^{X_i^\\top \\beta}.\n$$\n\n---\n\n### Summary Table\n\n| Distribution | Canonical Parameter $$\\theta$$ | Cumulant $$A(\\theta)$$ | Mean $$A'(\\theta)$$ | Variance $$A''(\\theta)$$ | Canonical Link |\n|---------------|--------------------------------|-------------------------|---------------------|---------------------------|----------------|\n| $$Y_i \\sim \\text{Binomial}(1, p_i)$$ | $$\\log\\frac{p_i}{1-p_i}$$ | $$\\log(1 + e^{\\theta})$$ | $$p_i$$ | $$p_i(1 - p_i)$$ | Logit |\n| $$Y_i \\sim \\text{Poisson}(\\lambda_i)$$ | $$\\log \\lambda_i$$ | $$e^{\\theta}$$ | $$\\lambda_i$$ | $$\\lambda_i$$ | Log |\n\n---\n\n### What We Will Demonstrate in Code\n\n1. **Simulate Data**\n\n   - Binomial: $$Y_i \\sim \\text{Binomial}(1, p_i)$$  \n   - Poisson: $$Y_i \\sim \\text{Poisson}(\\lambda_i)$$\n\n2. **Fit GLMs with Canonical Links**\n\n   - Logistic GLM: $$g(p_i) = \\log\\frac{p_i}{1-p_i}$$  \n   - Poisson GLM: $$g(\\lambda_i) = \\log \\lambda_i$$\n\n3. **Verify Theoretical Relationships**\n\n   - Mean: $$\\mu_i = A'(\\theta_i)$$  \n   - Variance: $$\\mathrm{Var}(Y_i) = A''(\\theta_i)$$  \n   - Canonical link linearity: $$g(\\mu_i) = X_i^\\top \\beta$$\n\n4. **Visualize Results**\n\n   - Predicted vs observed $$Y_i$$  \n   - Inverse-link response curves $$g^{-1}(\\eta_i)$$  \n   - Deviance and residual diagnostics\n\n## Code\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nn  <- 500\nx1 <- rnorm(n)\nx2 <- rnorm(n)\neta <- -0.5 + 1.1*x1 - 0.8*x2\np   <- plogis(eta)\ny   <- rbinom(n, size = 1, prob = p)\n\ndf <- data.frame(y, x1, x2)\n\nm_logit <- glm(y ~ x1 + x2, data = df, family = binomial())\nsummary(m_logit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = y ~ x1 + x2, family = binomial(), data = df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -0.3858     0.1078  -3.579 0.000344 ***\nx1            1.1572     0.1318   8.780  < 2e-16 ***\nx2           -0.7521     0.1107  -6.794 1.09e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 685.93  on 499  degrees of freedom\nResidual deviance: 530.03  on 497  degrees of freedom\nAIC: 536.03\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n\n```{.r .cell-code}\n# Effect curve for x1 (x2 fixed at 0), with 95% CI on response scale\nlibrary(ggplot2)\nxgrid <- data.frame(x1 = seq(-3, 3, length.out = 200), x2 = 0)\npred  <- predict(m_logit, newdata = xgrid, type = \"link\", se.fit = TRUE)\neta_hat <- pred$fit\nse_eta  <- pred$se.fit\np_hat   <- plogis(eta_hat)\nlo      <- plogis(eta_hat - 1.96*se_eta)\nhi      <- plogis(eta_hat + 1.96*se_eta)\n\nggplot() +\n  geom_point(aes(x = x1, y = y), data = df, alpha = 0.25) +\n  geom_line(aes(x = xgrid$x1, y = p_hat), linewidth = 1) +\n  geom_ribbon(aes(x = xgrid$x1, ymin = lo, ymax = hi), alpha = 0.2) +\n  labs(title = \"Binomial GLM (logit): effect of x1 | x2=0\",\n       x = \"x1\", y = \"P(Y=1)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](03-4-models_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Quick residual check\nplot(fitted(m_logit), residuals(m_logit, type = \"deviance\"),\n     xlab = \"Fitted probability\", ylab = \"Deviance residual\",\n     main = \"Logistic GLM: residuals vs fitted\")\nabline(h = 0, lty = 2)\n```\n\n::: {.cell-output-display}\n![](03-4-models_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2)\nn  <- 500\nx1 <- rnorm(n)\nx2 <- rnorm(n)\nE  <- runif(n, 0.5, 2.5)                # exposure\neta <- 0.2 + 0.5*x1 + 0.6*x2 + log(E)   # include offset in DGP\nlambda <- exp(eta)\ny  <- rpois(n, lambda)\n\ndfp <- data.frame(y, x1, x2, E)\n\nm_pois <- glm(y ~ x1 + x2 + offset(log(E)), data = dfp, family = poisson())\nsummary(m_pois)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = y ~ x1 + x2 + offset(log(E)), family = poisson(), \n    data = dfp)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  0.24655    0.03577   6.892  5.5e-12 ***\nx1           0.50714    0.02559  19.814  < 2e-16 ***\nx2           0.59661    0.02793  21.359  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1337.16  on 499  degrees of freedom\nResidual deviance:  498.35  on 497  degrees of freedom\nAIC: 1634.3\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n\n```{.r .cell-code}\n# Effect curve for x1 (x2 = 0, E = 1), with 95% CI on mean scale\nlibrary(ggplot2)\nxgrid <- data.frame(x1 = seq(-3, 3, length.out = 200), x2 = 0, E = 1)\npred  <- predict(m_pois, newdata = xgrid, type = \"link\", se.fit = TRUE)\neta_hat <- pred$fit\nse_eta  <- pred$se.fit\nmu_hat  <- exp(eta_hat)\nlo      <- exp(eta_hat - 1.96*se_eta)\nhi      <- exp(eta_hat + 1.96*se_eta)\n\nggplot() +\n  geom_point(aes(x = x1, y = y/E), data = dfp, alpha = 0.25) +\n  geom_line(aes(x = xgrid$x1, y = mu_hat), linewidth = 1) +\n  geom_ribbon(aes(x = xgrid$x1, ymin = lo, ymax = hi), alpha = 0.2) +\n  labs(title = \"Poisson GLM (log): effect of x1 | x2=0, E=1\",\n       x = \"x1\", y = \"Rate (λ)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](03-4-models_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Quick residual check + overdispersion diagnostic\nplot(fitted(m_pois), residuals(m_pois, type = \"deviance\"),\n     xlab = \"Fitted mean λ\", ylab = \"Deviance residual\",\n     main = \"Poisson GLM: residuals vs fitted\")\nabline(h = 0, lty = 2)\n```\n\n::: {.cell-output-display}\n![](03-4-models_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n:::\n\n\n## Ordinal Regression (Proportional Odds)\n\nWe observe a response $ Y $ ordinal, or a finely discretized/continuous outcome) and predictors $ x \\in \\mathbb{R}^p$ .  \nWe want to model the **conditional cumulative distribution function (CDF)**\n\n$$\nF(y \\mid x) = P(Y \\le y \\mid x).\n$$\n\nFollowing Liu et al. (2017), we use a **cumulative probability model (CPM)**, which is the same mathematical structure as an ordinal / proportional odds regression.\n$$\ng\\big( P(Y \\le y \\mid x) \\big)\n= \\alpha(y) - x^\\top \\beta,\n\\tag{1}\n$$\nwhere\n\n$$ g(\\cdot) $$ is a link function (logit, probit, loglog, cloglog),\n$$ \\alpha(y) $$ is a nondecreasing function of the cutpoint $$ y $$ (it plays the role of ordered thresholds),\n$$ \\beta $$ is a single vector of regression coefficients shared across all cutpoints (this is the **proportional** / **parallel slopes** assumption).\n\nThis is the form used in Liu et al. (2017), *Modeling continuous response variables using ordinal regression*.\n\nSuppose $$Y \\in \\{1,2,\\dots,K\\}.$$  \nDefine $$\\theta_k = \\alpha(k)$$ for $$k=1,\\dots,K-1.$$  \nThen (1) becomes the standard cumulative link / proportional odds model\n\n$$\ng\\big( P(Y \\le k \\mid x) \\big)\n= \\theta_k - x^\\top \\beta, \\quad k = 1, \\dots, K-1.\n$$\n\n### Choice of link \n\n- **logit:**  \n  $$\n  g(p) = \\log\\frac{p}{1-p}, \\qquad\n  g^{-1}(y) = \\frac{e^{y}}{1+e^{y}}\n  $$\n- **probit:**  \n  $$\n  g(p) = \\Phi^{-1}(p), \\qquad\n  g^{-1}(y) = \\Phi(y)\n  $$\n- **loglog:**  \n  $$\n  g(p) = -\\log\\big(-\\log(p)\\big), \\qquad\n  g^{-1}(y) = \\exp\\big[-\\exp(-y)\\big]\n  $$\n- **cloglog:**  \n  $$\n  g(p) = \\log\\big[-\\log(1-p)\\big], \\qquad\n  g^{-1}(y) = 1 - \\exp\\big[-\\exp(y)\\big]\n  $$\n\nHere, $$\\Phi(\\cdot)$$ is the CDF of the standard normal distribution.\n\n\nLet\n$$\n\\pi_k(x) = P(Y = k \\mid x), \\quad k=1,\\dots,K.\n$$\n\nFrom the cumulative probabilities:\n$$\n\\begin{aligned}\nP(Y = 1 \\mid x) &= P(Y \\le 1 \\mid x), \\\\\nP(Y = k \\mid x) &= P(Y \\le k \\mid x) - P(Y \\le k-1 \\mid x), \\quad k=2,\\dots,K-1, \\\\\nP(Y = K \\mid x) &= 1 - P(Y \\le K-1 \\mid x).\n\\end{aligned}\n$$\n\nSo with the logit link, the full model is\n\n$$\n\\log \\frac{P(Y \\le k \\mid x)}{1 - P(Y \\le k \\mid x)}\n= \\theta_k - x^\\top \\beta, \\quad k=1,\\dots,K-1,\n$$\nand the $$\\pi_k(x)$$ are obtained by the above expressions.\n\n### Interpretation\n\n$\\beta$ describes how a one-unit change in a predictor shifts the **entire** conditional distribution of $Y$. $\\theta_k$ locate the cutpoints between ordered categories. Using different links corresponds to assuming different latent error distributions, as listed in the paper.\n\n## Neural Net\n\n### McCulloch–Pitts Neuron (1943)\n\n#### Model Description\nA neuron fires if it receives enough excitatory input and no inhibitory input.\n\nLet  \n- $v_i(t) \\in \\{0,1\\}$$ activation (firing) state of neuron $i$ at time $t$,  \n- $E_i$: set of excitatory inputs,  \n- $I_i$: set of inhibitory inputs,  \n- $T_i$: excitatory threshold.\n\n#### Model Equation\n$$\nv_i(t+1) =\n\\begin{cases}\n1, & \\text{if } \\displaystyle \\sum_{j \\in E_i} v_j(t) \\ge T_i \\text{ and } \\sum_{k \\in I_i} v_k(t) = 0, \\\\[6pt]\n0, & \\text{otherwise.}\n\\end{cases}\n$$\n\nThis is a logical threshold model — the first formal mathematical neuron, but without a learning rule.\n\n### Rosenblatt Perceptron (1958)\n\n#### Model Description\nA single-layer learnable threshold unit for classification.\n\nLet  \n- Input vector $x \\in \\mathbb{R}^d$,  \n- Weight vector $w \\in \\mathbb{R}^d$,  \n- Bias $b$,  \n- Target label $t \\in \\{0,1\\}$.\n\n#### Forward Function\n$$\ny =\n\\begin{cases}\n1, & \\text{if } w^\\top x + b \\ge 0, \\\\[4pt]\n0, & \\text{if } w^\\top x + b < 0.\n\\end{cases}\n$$\n\nor equivalently\n\n$$\ny = \\mathbf{1}\\{w^\\top x + b \\ge 0\\}.\n$$\n\n#### Learning Rule\nGiven a learning rate \\(\\eta > 0\\):\n\n$$\nw \\leftarrow w + \\eta (t - y) x, \\qquad\nb \\leftarrow b + \\eta (t - y).\n$$\n\nIf the perceptron predicts incorrectly, the weights shift toward or away from the input vector to correct the error.\n\n### Rumelhart–Hinton–Williams Backpropagation Network (1986)\nA multilayer feedforward network trained by gradient descent.\n\nLet  \n- Layer index $\\ell = 1, 2, \\dots, L$,  \n- Activations $a_j^{(\\ell)}$,  \n- Net inputs $z_j^{(\\ell)}$,  \n- Weights $w_{ij}^{(\\ell)}$,  \n- Biases $b_j^{(\\ell)}$.\n\n#### Forward Pass\nFor the input layer:\n$$\na_j^{(1)} = x_j.\n$$\n\nFor each hidden or output layer:\n$$\nz_j^{(\\ell)} = \\sum_i w_{ij}^{(\\ell)} a_i^{(\\ell-1)} + b_j^{(\\ell)}, \\qquad\na_j^{(\\ell)} = f(z_j^{(\\ell)}),\n$$\nwhere \\(f\\) is a differentiable activation function (e.g. sigmoid).\n\nThe total loss (mean-squared error):\n$$\nE = \\frac{1}{2} \\sum_k (t_k - a_k^{(L)})^2.\n$$\n\n#### Backward Pass (Error Terms)\nFor output layer units:\n$$\n\\delta_k^{(L)} = (t_k - a_k^{(L)}) f'(z_k^{(L)}).\n$$\n\nFor hidden layer units:\n$$\n\\delta_j^{(\\ell)} = f'(z_j^{(\\ell)}) \\sum_k w_{jk}^{(\\ell+1)} \\delta_k^{(\\ell+1)}.\n$$\n\n#### Weight and Bias Updates\n$$\n\\Delta w_{ij}^{(\\ell)} = \\eta \\, a_i^{(\\ell-1)} \\, \\delta_j^{(\\ell)}, \\qquad\nw_{ij}^{(\\ell)} \\leftarrow w_{ij}^{(\\ell)} + \\Delta w_{ij}^{(\\ell)}.\n$$\n\n$$\n\\Delta b_j^{(\\ell)} = \\eta \\, \\delta_j^{(\\ell)}, \\qquad\nb_j^{(\\ell)} \\leftarrow b_j^{(\\ell)} + \\Delta b_j^{(\\ell)}.\n$$\n\n### Summary Table\n\n| Model | Output Function | Learning Rule |\n|--------|-----------------|----------------|\n| **McCulloch–Pitts (1943)** | $$v_i(t+1)=1$$ if enough excitation and no inhibition | None (fixed logic) |\n| **Rosenblatt (1958)** | $$y=\\mathbf{1}\\{w^\\top x + b \\ge 0\\}$$ | $$w \\leftarrow w + \\eta (t-y)x$$ |\n| **Rumelhart et al. (1986)** | $$a_j=f\\!\\left(\\sum_i w_{ij}a_i+b_j\\right)$$ | $$\\Delta w_{ij}=\\eta a_i \\delta_j$$ with backpropagation |\n\n\nThese three equations trace the mathematical evolution of neural networks:  \nfrom **logical firing** (McCulloch–Pitts) → **learnable thresholds** (Rosenblatt) → **differentiable, multilayer learning** (Rumelhart–Hinton–Williams).\n\n## Create visualizations of the prediction relationship\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(nnet)           # for neural net\nlibrary(randomForest)   # for random forest\nlibrary(MASS)           # for polr (ordinal regression)\nset.seed(42)\nn  <- 2000\nX1 <- rnorm(n)\nX2 <- rnorm(n)\nX3 <- rbinom(n, 1, 0.4)\n\nsigmoid <- function(x) 1 / (1 + exp(-x))\nmu <- sqrt(5) * sigmoid(X1 + X3) + sqrt(5) * sigmoid(X2) * X3\nY  <- mu + rnorm(n, 0, 1)\n\ndat <- tibble(X1, X2, X3 = factor(X3), Y)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(nnet)\nlibrary(randomForest)\nlibrary(MASS)\n\nset.seed(42)\n\n# --- models -------------------------------------------------\nglm_fit <- glm(Y ~ X1 + X2 + X3, data = dat, family = gaussian())\n\ndat <- dat %>%\n  mutate(pred_glm = predict(glm_fit))\n\nnn_fit <- nnet(\n  Y ~ X1 + X2 + X3,\n  data   = dat,\n  size   = 5,\n  linout = TRUE,\n  maxit  = 1000,\n  decay  = 1e-4,\n  trace  = FALSE\n)\n\ndat <- dat %>%\n  mutate(pred_nn = as.numeric(predict(nn_fit, dat)))\n\nrf_fit <- randomForest(\n  Y ~ X1 + X2 + X3,\n  data = dat,\n  ntree = 300,\n  importance = TRUE\n)\n\ndat <- dat %>%\n  mutate(pred_rf = predict(rf_fit, dat))\n\n# --- ordinal target -----------------------------------------\ndat <- dat %>%\n  mutate(\n    Y_ord = cut(\n      Y,\n      breaks = quantile(Y, probs = c(0, 0.33, 0.66, 1)),\n      labels = c(\"Low\", \"Medium\", \"High\"),\n      include.lowest = TRUE,\n      ordered_result = TRUE\n    )\n  )\n\nord_fit <- polr(Y_ord ~ X1 + X2 + X3, data = dat, method = \"logistic\")\n\ndat <- dat %>%\n  mutate(pred_ord = predict(ord_fit, type = \"class\"))\n\n# --- long format for compare plot ---------------------------\ndat_long <- dat %>%\n  dplyr::select(Y, pred_glm, pred_nn, pred_rf) %>%\n  tidyr::pivot_longer(\n    cols = c(pred_glm, pred_nn, pred_rf),\n    names_to = \"model\",\n    values_to = \"prediction\"\n  ) %>%\n  mutate(\n    model = dplyr::recode(\n      model,\n      pred_glm = \"GLM\",\n      pred_nn  = \"Neural Net\",\n      pred_rf  = \"Random Forest\"\n    )\n  )\n\n# --- compare scatter ----------------------------------------\nggplot(dat_long, aes(x = Y, y = prediction)) +\n  geom_point(alpha = 0.25) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  facet_wrap(~ model, nrow = 1) +\n  labs(\n    title = \"Observed vs Predicted: Model Comparison\",\n    x = \"Observed Y\",\n    y = \"Predicted Y\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](03-4-models_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# --- residual density ---------------------------------------\ndat_long %>%\n  mutate(resid = prediction - Y) %>%\n  ggplot(aes(x = resid, fill = model)) +\n  geom_density(alpha = 0.4) +\n  labs(\n    title = \"Residual distributions by model\",\n    x = \"Prediction error (pred - Y)\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](03-4-models_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n:::\n",
    "supporting": [
      "03-4-models_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}