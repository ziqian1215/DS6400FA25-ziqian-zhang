{
  "hash": "dde3bb6cee579059c8031c0c54cffa99",
  "result": {
    "engine": "knitr",
    "markdown": "# Measures of model performance\n\n## Random Forest\n> **Note.** Random Forest can be applied to both classification and regression tasks.  \n> In this chapter, we focus **only on the classification setting**, where the target variable $y_i \\in \\{0,1\\}$ and each tree outputs class probabilities. The formulas and evaluation metrics described below are all for classification.\n\nLet the dataset be\n\n$$\nD=\\{(x_i,y_i)\\}_{i=1}^N,\\quad x_i\\in\\mathbb{R}^p,\\ y_i\\in\\{0,1\\}.\n$$\n\nA Random Forest is an ensemble of $B$ classification trees $\\{T_b\\}_{b=1}^B$. Each tree is trained using:\n\n### Bootstrap sampling\n\nDraw a bootstrap sample $D_b^*$ from $D$ by sampling $N$ observations **with replacement**.\n\n$$\nD_b^* = \\{(x_{i_1},y_{i_1}),\\dots,(x_{i_N},y_{i_N})\\},\\quad i_j \\sim \\text{Unif}\\{1,\\dots,N\\}.\n$$\n\n### Random feature selection\n\nAt each internal node, randomly select $m_{\\text{try}}$ features from the full set $\\{1,\\dots,p\\}$.  \nAmong the possible splits on these features, choose the one that maximizes the impurity decrease:\n\n$$\n\\Delta \\mathcal{I} \n= \\mathcal{I}(\\text{parent}) \n-\\sum_{c\\in\\{\\text{left},\\text{right}\\}}\n\\frac{n_c}{n_{\\text{parent}}}\\mathcal{I}(c).\n$$\n\nA common impurity measure is the Gini impurity:\n\n$$\n\\mathcal{I}_{\\text{Gini}}(\\text{node}) = 1 - \\sum_{k\\in\\{0,1\\}}\\hat{\\pi}_k^2,\n\\qquad \n\\hat{\\pi}_k = \\frac{1}{n_{\\text{node}}}\\sum_{i\\in\\text{node}}\\mathbb{1}(y_i=k).\n$$\n\n### Prediction\n\nEach tree $T_b$ produces a probability estimate for the positive class:\n\n$$\nT_b(x) = \\hat{p}_b(y=1\\mid x).\n$$\n\nThe Random Forest averages these:\n\n$$\n\\hat{p}(x) = \\frac{1}{B}\\sum_{b=1}^B T_b(x),\n\\qquad\n\\hat{y}(x) = \\mathbb{1}\\{\\hat{p}(x) \\ge 0.5\\}.\n$$\n\nThis reduces variance and improves generalization.\n\n## Ten-Fold Cross-Validation\n\nWe split $D$ into 10 disjoint folds of (approximately) equal size:\n\n$$\nD = \\bigcup_{k=1}^{10} D_k,\\qquad D_i\\cap D_j = \\varnothing \\ (i\\neq j).\n$$\n\nFor each fold $k$:\n\n- Training set: $D^{(-k)} = D \\setminus D_k$  \n- Test set: $D_k$\n\nTrain the Random Forest on $D^{(-k)}$ to obtain a fitted model $f_k(\\cdot)$.  \nUse it to produce predictions on all samples in $D_k$:\n\n$$\n\\{(\\hat{p}_k(x_i),\\hat{y}_k(x_i),y_i)\\}_{(x_i,y_i)\\in D_k}.\n$$\n\nLet $M(\\cdot)$ be a performance metric (Accuracy, F1, etc).  \nDefine the fold-wise performance as\n\n$$\nM_k = M\\big(\\{(\\hat{y}_k(x_i),y_i)\\}_{(x_i,y_i)\\in D_k}\\big).\n$$\n\nThe cross-validated estimate is\n\n$$\n\\overline{M} = \\frac{1}{10}\\sum_{k=1}^{10} M_k.\n$$\n\n### Standard error of the 10-fold CV mean (derivation)\n\nLet $M_1,\\dots,M_n$ be the performance values (e.g., accuracy, F1, AUC, or MSE) computed on each of the $n$ CV folds. For 10-fold CV, $n=10$.\n\n**Step 1 (fold mean).**  \nDefine the mean across folds\n$$\n\\overline{M} \\;=\\; \\frac{1}{n}\\sum_{k=1}^n M_k \\, .\n$$\n\n**Step 2 (sample variance across folds).**  \nUse the unbiased sample variance with Bessel’s correction\n$$\ns^2 \\;=\\; \\frac{1}{n-1}\\sum_{k=1}^n \\big(M_k-\\overline{M}\\big)^2 \\, .\n$$\n\n**Step 3 (standard error of the mean).**  \nThe standard error of the mean is the standard deviation of $\\overline{M}$.  \nWe estimate it by plugging in the sample standard deviation $s$:\n$$\n\\widehat{\\mathrm{SE}}(\\overline{M}) \\;=\\; \\frac{s}{\\sqrt{n}} \n\\;=\\; \\sqrt{\\frac{s^2}{n}} \\, .\n$$\n\n**Step 4 (combine Steps 2 and 3).**  \nSubstitute $s^2$ from Step 2:\n$$\n\\widehat{\\mathrm{SE}}(\\overline{M})\n\\;=\\;\n\\sqrt{ \\frac{1}{n}\\cdot \\frac{1}{n-1} \\sum_{k=1}^n \\big(M_k-\\overline{M}\\big)^2 }\n\\;=\\;\n\\sqrt{ \\frac{1}{n(n-1)} \\sum_{k=1}^n \\big(M_k-\\overline{M}\\big)^2 } \\, .\n$$\n\nFor 10-fold CV ($n=10$),\n$$\n\\widehat{\\mathrm{SE}}(\\overline{M})\n\\;=\\;\n\\sqrt{ \\frac{1}{10(10-1)} \\sum_{k=1}^{10} \\big(M_k-\\overline{M}\\big)^2 } \\, .\n$$\n\n---\n\n## Performance Metrics\n\nOn a given test set, define confusion matrix counts:\n\n$$\nTP=\\sum \\mathbb{1}\\{y_i=1,\\hat{y}_i=1\\},\\quad\nFP=\\sum \\mathbb{1}\\{y_i=0,\\hat{y}_i=1\\},\\quad\nTN=\\sum \\mathbb{1}\\{y_i=0,\\hat{y}_i=0\\},\\quad\nFN=\\sum \\mathbb{1}\\{y_i=1,\\hat{y}_i=0\\}.\n$$\n\nThen:\n\n$$\n\\mathrm{Accuracy}=\\frac{TP+TN}{TP+TN+FP+FN},\n\\qquad\n\\mathrm{Precision}=\\frac{TP}{TP+FP},\n\\qquad\n\\mathrm{Recall}=\\frac{TP}{TP+FN},\n$$\n\n$$\n\\mathrm{F1}=2\\frac{\\mathrm{Precision}\\cdot\\mathrm{Recall}}{\\mathrm{Precision}+\\mathrm{Recall}}.\n$$\n\nAUC is the area under the ROC curve:\n\n$$\n\\mathrm{AUC} = \\Pr(S^+ > S^-)\n$$\n\nwhere $S^+$ and $S^-$ are the scores from randomly drawn positive and negative examples.\n\n---\n\n## Combining RF and CV\n\nFor each fold $k$, compute the metrics on the test set $D_k$.  \nFor example, cross-validated accuracy is\n\n$$\n\\overline{\\mathrm{Acc}} = \\frac{1}{10}\\sum_{k=1}^{10}\n\\frac{TP_k+TN_k}{TP_k+TN_k+FP_k+FN_k}.\n$$\n\nDo the same for Precision, Recall, F1, and AUC.  \nThese $\\overline{M}$ values represent the estimated generalization performance of the Random Forest model.\n\n\n## Simulation study\n\nThe following part shows the simulation study comparing LOO vs 10-fold CV for LDA & Random Forest.\n\n## Data generation model\nWe simulate i.i.d. data $\\{(\\mathbf{T}_i, Y_i)\\}_{i=1}^n$ as:\n\n$$\nY_i \\sim \\mathrm{Bin}(1, 0.5), \\qquad\n\\mathbf{T}_i \\mid Y_i = y_i \\;\\sim\\; N_2\\!\\left(\n\\begin{bmatrix}\ny_i - \\tfrac{1}{2} \\\\\n0\n\\end{bmatrix},\n\\, I_2\n\\right),\n$$\n\nwhere $I_2$ is the $2\\times2$ identity and $N_2$ denotes the bivariate normal.\nEquivalently, conditional on $Y_i$, the two features are independent with unit variance;  \nthe first feature’s mean shifts by $y_i-0.5$, making it informative for $Y$.\n\n### R implementation of data generation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data generation function\ngenerate_data <- function(N, seed = NULL) {\n  if (!is.null(seed)) set.seed(seed)\n  y  <- rbinom(N, size = 1, prob = 0.5)                 # Bernoulli(0.5)\n  x1 <- rnorm(N, mean = y - 0.5, sd = 1)                # mean depends on y\n  x2 <- rnorm(N, mean = 0,       sd = 1)                # mean 0, unit sd\n  data.frame(\n    x1 = x1,\n    x2 = x2,\n    y  = factor(y, levels = c(0,1))\n  )\n}\n```\n:::\n\n\n## Methods evaluated\n\nWe evaluate two classifiers:\n\n- **Linear Discriminant Analysis (LDA)**\n- **Random Forest (RF)**\n\nEach model’s **generalization error** is estimated using two cross-validation approaches:\n\n- **Leave-one-out cross-validation (LOO-CV):**  \n  $$\n  \\widehat{R}_{\\text{LOO}} = \\frac{1}{n} \\sum_{i=1}^n \\ell(y_i, \\widehat{f}^{(-i)}(x_i)),\n  $$\n  where $\\widehat{f}^{(-i)}$ is trained without observation $i$.\n\n- **10-fold cross-validation:**  \n  $$\n  \\widehat{R}_{10\\text{-fold}} =\n  \\frac{1}{10}\\sum_{k=1}^{10}\\frac{1}{|\\mathcal{I}_k|}\\sum_{i\\in\\mathcal{I}_k} \\ell(y_i,\\widehat{f}^{(-k)}(x_i)),\n  $$\n  where folds $\\mathcal{I}_1,\\dots,\\mathcal{I}_{10}$ partition the sample.\n\nThe loss function $\\ell$ is **0–1 loss** (misclassification error).  \nWe then compare each estimator to the **true error** (approximated using a very large independent test set) and summarize performance using:\n\n- **Bias:** mean difference between estimated CV error and true error.  \n- **Variance:** variability of CV estimates across Monte Carlo replications.  \n- **MSE:** mean squared error, decomposed as  \n  $$\n  \\text{MSE} = (\\text{Bias})^2 + \\text{Variance}.\n  $$\n\n### Libraries\n\n::: {.cell}\n\n```{.r .cell-code}\nsuppressPackageStartupMessages({\n  library(MASS)          # for LDA\n  library(randomForest)  # for Random Forest\n  library(dplyr)\n  library(tibble)\n  library(purrr)\n  library(ggplot2)\n})\n```\n:::\n\n### Stratified folds, model wrappers, and error computation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmake_stratified_folds <- function(y, K = 10, seed = 1) {\n  set.seed(seed)\n  y <- factor(y, levels = c(\"0\",\"1\"))\n  folds <- integer(length(y))\n  for (lvl in levels(y)) {\n    idx <- which(y == lvl)\n    idx <- sample(idx)\n    splits <- cut(seq_along(idx), breaks = K, labels = FALSE)\n    folds[idx] <- splits\n  }\n  folds\n}\n\npredict_model <- function(model_type, train_df, test_df, y_col = \"y\",\n                          rf_ntree = 500, rf_mtry = NULL) {\n  if (!is.factor(train_df[[y_col]])) train_df[[y_col]] <- factor(train_df[[y_col]], levels = c(\"0\",\"1\"))\n  if (!is.factor(test_df[[y_col]]))  test_df[[y_col]]  <- factor(test_df[[y_col]],  levels = c(\"0\",\"1\"))\n\n  if (model_type == \"lda\") {\n    fit <- MASS::lda(as.formula(paste(y_col, \"~ .\")), data = train_df)\n    post <- predict(fit, newdata = test_df)\n    pred <- factor(ifelse(post$posterior[, \"1\"] >= 0.5, \"1\", \"0\"), levels = c(\"0\",\"1\"))\n  } else if (model_type == \"rf\") {\n    if (is.null(rf_mtry)) rf_mtry <- floor(sqrt(ncol(train_df) - 1))\n    fit <- randomForest::randomForest(as.formula(paste(y_col, \"~ .\")),\n                                      data = train_df, ntree = rf_ntree, mtry = rf_mtry)\n    pred <- predict(fit, newdata = test_df, type = \"response\")\n  } else stop(\"Unknown model_type\")\n\n  pred\n}\n\ncompute_error <- function(y_true, pred) {\n  mean(pred != y_true)\n}\n```\n:::\n\n### Cross-validation evaluators\n\n::: {.cell}\n\n```{.r .cell-code}\nevaluate_loo <- function(df, model_type, rf_ntree = 500, rf_mtry = NULL) {\n  n <- nrow(df)\n  errs <- numeric(n)\n  for (i in seq_len(n)) {\n    test_idx  <- i\n    train_idx <- setdiff(seq_len(n), i)\n    train_df  <- df[train_idx, , drop = FALSE]\n    test_df   <- df[test_idx,  , drop = FALSE]\n    pred <- predict_model(model_type, train_df, test_df, rf_ntree = rf_ntree, rf_mtry = rf_mtry)\n    errs[i] <- compute_error(test_df$y, pred)\n  }\n  mean(errs)\n}\n\nevaluate_kfold <- function(df, model_type, K = 10, seed = 1, rf_ntree = 500, rf_mtry = NULL) {\n  folds <- make_stratified_folds(df$y, K = K, seed = seed)\n  errs <- numeric(K)\n  for (k in seq_len(K)) {\n    test_idx  <- which(folds == k)\n    train_idx <- which(folds != k)\n    train_df  <- df[train_idx, , drop = FALSE]\n    test_df   <- df[test_idx,  , drop = FALSE]\n    pred <- predict_model(model_type, train_df, test_df, rf_ntree = rf_ntree, rf_mtry = rf_mtry)\n    errs[k] <- compute_error(test_df$y, pred)\n  }\n  mean(errs)\n}\n```\n:::\n\n## True error and Monte Carlo evaluation\n\nTo evaluate the accuracy of cross-validation estimators, we need a benchmark:  \nthe **true generalization error** of each model.  \n\nFor a fitted classifier $f$, the true error is defined as\n$$\nR(f) = \\Pr\\{ f(\\mathbf{X}) \\neq Y \\},\n$$\nwhere $(\\mathbf{X}, Y)$ is a new independent draw from the same distribution.\n\nSince this probability cannot be computed exactly, we approximate it using a **very large independent test set** (e.g., $N_\\text{test} = 50{,}000$).  \nWe train the model on the entire observed dataset, then evaluate its misclassification rate on the test set.\n\n---\n\n### Function to estimate true error\n\n\n::: {.cell}\n\n```{.r .cell-code}\nestimate_true_error <- function(model_type, train_df, N_test = 50000,\n                                rf_ntree = 500, rf_mtry = NULL) {\n  # Generate large independent test set\n  test_df <- generate_data(N_test)\n  pred <- predict_model(model_type, train_df, test_df, rf_ntree = rf_ntree, rf_mtry = rf_mtry)\n  compute_error(test_df$y, pred)\n}\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\none_replication <- function(N, seed = NULL) {\n  if (!is.null(seed)) set.seed(seed)\n  df <- generate_data(N)\n\n  # LDA\n  lda_loo   <- evaluate_loo(df, \"lda\")\n  lda_k10   <- evaluate_kfold(df, \"lda\", K = 10, seed = 1)\n  lda_true  <- estimate_true_error(\"lda\", df)\n\n  # RF\n  rf_loo    <- evaluate_loo(df, \"rf\")\n  rf_k10    <- evaluate_kfold(df, \"rf\", K = 10, seed = 1)\n  rf_true   <- estimate_true_error(\"rf\", df)\n\n  tibble::tibble(\n    N        = N,\n    model    = c(\"LDA\",\"LDA\",\"RF\",\"RF\"),\n    method   = c(\"LOO\",\"10-fold\",\"LOO\",\"10-fold\"),\n    est_err  = c(lda_loo, lda_k10, rf_loo, rf_k10),\n    true_err = c(lda_true, lda_true, rf_true, rf_true)\n  )\n}\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20250925)\none_replication(40)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n      N model method  est_err true_err\n  <dbl> <chr> <chr>     <dbl>    <dbl>\n1    40 LDA   LOO       0.325    0.323\n2    40 LDA   10-fold   0.285    0.323\n3    40 RF    LOO       0.425    0.333\n4    40 RF    10-fold   0.38     0.333\n```\n\n\n:::\n:::\n\n## Simulation Setup\n\nThe code below runs a Monte Carlo simulation across multiple sample sizes. It defines a set of sample sizes (`Ns`) and the number of replications (`R=20`) to run for each, sets a random seed for reproducibility, and initializes an empty list to store results. For each sample size \\(N\\) and replication \\(r\\), the function `one_replication(N)` is called to perform a single experiment, the output is tagged with the replication index, and then appended to the results list. In the end, `results_list` contains all simulated outcomes across every combination of sample size and replication, providing the data needed for further summarization and analysis.\n\n::: {.cell}\n\n```{.r .cell-code}\nNs <- c( 20, 30, 40, 50, 75, 100)\nR  <- 20  # increase for tighter Monte Carlo precision\nset.seed(123)\nresults_list <- list()\n\nfor (N in Ns) {\n  for (r in 1:R) {\n    res <- one_replication(N)\n    res$rep <- r\n    results_list <- append(results_list, list(res))\n  }\n}\n```\n:::\n\n\n## Summary\n\n::: {.cell}\n\n```{.r .cell-code}\nresults <- dplyr::bind_rows(results_list)\nggplot(results, aes(x = factor(N), y = est_err, color = method)) +\n  geom_boxplot(outlier.alpha = 0.3, position = position_dodge(width = 0.8)) +\n  stat_summary(fun = mean, geom = \"line\", aes(group = method),\n               position = position_dodge(width = 0.8)) +\n  stat_summary(fun = mean, geom = \"point\", aes(group = method),\n               position = position_dodge(width = 0.8), size = 2) +\n  facet_wrap(~ model) +\n  labs(\n    title = \"Distribution of CV Estimates vs. True Error\",\n    x = \"Sample size N\",\n    y = \"Estimated error rate\",\n    color = \"CV Method\"\n  ) +\n  theme_minimal(base_size = 12)\n```\n\n::: {.cell-output-display}\n![](01-performance-measures_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n",
    "supporting": [
      "01-performance-measures_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}