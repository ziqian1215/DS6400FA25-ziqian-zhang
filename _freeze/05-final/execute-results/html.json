{
  "hash": "51dab44f96b4263b7fb6b8e34832861b",
  "result": {
    "engine": "knitr",
    "markdown": "# Final Exam\n\n## Question 1\n\n### Data Generating Process\n\nWe generate synthetic data from a linear regression model with a finite sample size of **N = 200** observations and **p = 50** predictor variables. The response variable is generated according to\n\n$$\nY = X\\beta + \\varepsilon, \\qquad \\varepsilon \\sim \\mathcal{N}(0_N, I_N),\n$$\n\nwhere $X \\in \\mathbb{R}^{N \\times p}$ is the design matrix and $\\beta \\in \\mathbb{R}^p$ is the vector of true regression coefficients.\n\n#### Block structure\n\nThe predictor matrix is constructed by concatenating four blocks with heterogeneous correlation structures:\n\n$$\nX = [\\, U \\;\\; V \\;\\; W \\;\\; Z \\,],\n$$\n\nwith dimensions\n\n- $U \\in \\mathbb{R}^{N \\times 10}$\n- $V \\in \\mathbb{R}^{N \\times 10}$\n- $W \\in \\mathbb{R}^{N \\times 10}$\n- $Z \\in \\mathbb{R}^{N \\times 20}$\n\nThe coefficient vector is partitioned accordingly as\n\n$$\n\\beta = (\\alpha^\\top, \\gamma^\\top, \\delta^\\top, \\zeta^\\top)^\\top ,\n$$\n\nso that the data generating model can be written in block form as\n\n$$\nY = U\\alpha + V\\gamma + W\\delta + Z\\zeta + \\varepsilon .\n$$\n\nEach block is designed to represent a distinct regime in terms of signal strength, sparsity, and predictor correlation.\n\n| Block | Dimension | Nonzero Coefficients | Signal Strength | Correlation Structure | Purpose |\n|------|-----------|----------------------|-----------------|-----------------------|---------|\n| U | 10 | 1 (α₁ = 3) | Strong, sparse | Equicorrelated (ρ = 0.95) | Assess behavior under severe multicollinearity |\n| V | 10 | 3 (γ₁–γ₃ = 1.5) | Moderate, sparse | Equicorrelated (ρ = 0.50) | Compare methods with multiple correlated signals |\n| W | 10 | 5 (δ₁–δ₅ = 0.3) | Weak, dense | Equicorrelated (ρ = 0.20) | Examine bias–variance trade-offs under weak signals |\n| Z | 20 | 0 | None | Independent (ρ = 0) | Evaluate false positives and shrinkage of noise |\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmake_equicorr <- function(p, rho){\n  S <- matrix(rho, p, p); diag(S) <- 1\n  S\n}\n\ngen_data <- function(n=200){\n  # block sizes\n  pU <- 10; pV <- 10; pW <- 10; pZ <- 20\n  p  <- pU+pV+pW+pZ\n  \n  SU <- make_equicorr(pU, 0.95)\n  SV <- make_equicorr(pV, 0.50)\n  SW <- make_equicorr(pW, 0.20)\n  SZ <- diag(pZ)\n  \n  U <- mvtnorm::rmvnorm(n, sigma=SU)\n  V <- mvtnorm::rmvnorm(n, sigma=SV)\n  W <- mvtnorm::rmvnorm(n, sigma=SW)\n  Z <- mvtnorm::rmvnorm(n, sigma=SZ)\n  \n  X <- cbind(U,V,W,Z)\n  colnames(X) <- paste0(\"X\", seq_len(ncol(X)))\n\n  beta <- rep(0, p)\n  # alpha\n  beta[1] <- 3\n  # gamma\n  beta[(pU+1):(pU+3)] <- 1.5\n  # delta\n  beta[(pU+pV+1):(pU+pV+5)] <- 0.3\n  # zeta already 0\n  \n  eps <- rnorm(n, 0, 1)\n  y <- as.numeric(X %*% beta + eps)\n  \n  block <- c(rep(\"U\",pU), rep(\"V\",pV), rep(\"W\",pW), rep(\"Z\",pZ))\n  list(X=X, y=y, beta=beta, block=block)\n}\n```\n:::\n\n### Models and Tuning Procedures\n\nAll models are fit within a single train–test split using a unified workflow. All tuning and model fitting are performed using training data only, while final performance is evaluated on held-out test data.\n\nPrior to model fitting, predictors are standardized using statistics computed from the training data and applied to both training and test sets to prevent data leakage.\n\n\n#### Ordinary Least Squares (OLS)\n\nOrdinary least squares is fit on the standardized training data without regularization:\n\n$$\n\\hat{\\beta}^{\\text{OLS}} = \\arg\\min_{\\beta}\\|Y - X\\beta\\|_2^2.\n$$\n\nOLS provides an unregularized reference point with minimal bias but potentially high variance, particularly in the presence of correlated predictors. Comparisons with OLS help isolate the impact of regularization.\n\n\n#### Ridge, Lasso, and Elastic Net\n\nRegularized regression models are fit using the **glmnet** framework. For ridge, lasso, and elastic net, the tuning parameter $\\lambda$ is selected by cross-validation on the training data to minimize mean squared prediction error.\n\nThe estimators take the form\n\n$$\n\\hat{\\beta}\n= \\arg\\min_{\\beta}\\|Y - X\\beta\\|_2^2\n+ \\lambda\\left(\n\\alpha\\|\\beta\\|_1 + (1-\\alpha)\\|\\beta\\|_2^2\n\\right),\n$$\n\nwith the following special cases:\n\n- Ridge regression: $\\alpha = 0$\n- Lasso regression: $\\alpha = 1$\n- Elastic net: $\\alpha \\in (0,1)$, fixed at a predefined value\n\nFor each method, the value of $\\lambda$ corresponding to the minimum cross-validated error ($\\lambda_{\\text{min}}$) is selected, and the model is refit on the full training set at this value.\n\n#### Principal Components Regression (PCR)\n\nPrincipal components regression is fit by first computing the principal components of the standardized training predictors. A linear regression model is then fit using the first $k$ components.\n\nThe number of retained components $k$ is selected via cross-validation by minimizing the cross-validated root mean squared error. To reduce variance, the smallest $k$ within one standard error of the minimum is chosen.\n\n#### Implementation details\n\nFor each fitted model, coefficient estimates and test-set predictions are stored for downstream evaluation. Tuning parameters (selected $\\lambda$ or number of components $k$) are recorded to facilitate comparison across methods.\n\nAll subsequent analyses—including prediction accuracy, coefficient estimation error, variable selection stability, and block-wise diagnostics—are based on these fitted models.\n\n### Evaluation Metrics\n\nModel performance is evaluated using repeated random train–test splits. Specifically, for each repetition\n$r = 1,\\dots,R$ with $R=60$, a new dataset is generated, randomly split into a training set (70%) and a test set (30%), and all models are fit and evaluated independently. This yields empirical distributions for all metrics, allowing uncertainty and variability to be assessed.\n\n---\n\n#### Out-of-sample predictive accuracy\n\nFor each method and each repetition, predictive accuracy is measured using the test-set mean squared error (MSE):\n\n$$\n\\text{MSE}^{(r)} = \\frac{1}{n_{\\text{test}}}\n\\sum_{i \\in \\mathcal{T}_r} (y_i - \\hat{y}_i)^2,\n$$\n\nwhere $\\mathcal{T}_r$ denotes the test set for repetition $r$.\n\nTo assess block-wise predictive contributions, we compute a drop-one-block increase in test MSE. For block\n$b \\in \\{U,V,W,Z\\}$, let $\\hat{y}_i^{(-b)}$ denote predictions obtained after setting coefficients in block $b$ to zero. The marginal contribution of block $b$ is defined as\n\n$$\n\\Delta\\text{MSE}_b^{(r)}\n= \\text{MSE}_b^{(r)} - \\text{MSE}^{(r)},\n$$\n\nwhere $\\text{MSE}_b^{(r)}$ is the test MSE using $\\hat{y}^{(-b)}$. Positive values indicate that the block contributes to predictive accuracy.\n\n\n#### Coefficient estimation error\n\nAccuracy of coefficient estimation is evaluated using the mean squared error between estimated and true coefficients:\n\n$$\n\\text{Coef-MSE}^{(r)} = \\frac{1}{p}\n\\sum_{j=1}^{p} \\bigl(\\hat{\\beta}_j^{(r)} - \\beta_j\\bigr)^2.\n$$\n\nBlock-specific coefficient error is computed analogously by restricting the summation to coefficients belonging to a given block $b$:\n\n$$\n\\text{Coef-MSE}_b^{(r)} =\n\\frac{1}{|b|}\n\\sum_{j \\in b} \\bigl(\\hat{\\beta}_j^{(r)} - \\beta_j\\bigr)^2.\n$$\n\n\n#### Variable selection stability\n\nFor methods with an explicit variable selection component (lasso and elastic net), selection is defined by nonzero estimated coefficients. Let $S^{(r)} \\subset \\{1,\\dots,p\\}$ denote the selected set in repetition $r$.\n\nStability of variable selection is quantified using the Jaccard similarity between selected sets from two repetitions $r$ and $r'$:\n\n$$\nJ\\bigl(S^{(r)}, S^{(r')}\\bigr)\n= \\frac{|S^{(r)} \\cap S^{(r')}|}\n{|S^{(r)} \\cup S^{(r')}|}.\n$$\n\nWe compute Jaccard similarities over many randomly chosen pairs of repetitions to obtain an empirical distribution. Block-wise selection stability is computed by restricting the selected sets to indices belonging to each block.\n\n\n#### Shrinkage magnitude and coefficient patterns\n\nTo summarize the degree of coefficient shrinkage, we compute the $\\ell_1$ and $\\ell_2$ norms of the estimated coefficient vector:\n\n$$\n\\|\\hat{\\beta}^{(r)}\\|_1 = \\sum_{j=1}^{p} |\\hat{\\beta}_j^{(r)}|,\n\\qquad\n\\|\\hat{\\beta}^{(r)}\\|_2 =\n\\left(\\sum_{j=1}^{p} (\\hat{\\beta}_j^{(r)})^2\\right)^{1/2}.\n$$\n\nIn addition, the full set of estimated coefficients $\\hat{\\beta}^{(r)}$ is retained across repetitions, enabling visualization of shrinkage patterns and comparisons across blocks with differing correlation structures.\n\n---\n\nAcross all metrics, comparisons are based on the empirical distributions obtained from repeated resampling rather than single-point estimates, allowing uncertainty and variability to be explicitly assessed.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nscale_train_test <- function(Xtr, Xte){\n  mu <- colMeans(Xtr)\n  sd <- apply(Xtr, 2, sd)\n  sd[sd == 0] <- 1\n\n  Xtr_s <- sweep(sweep(Xtr, 2, mu, \"-\"), 2, sd, \"/\")\n  Xte_s <- sweep(sweep(Xte, 2, mu, \"-\"), 2, sd, \"/\")\n\n  colnames(Xtr_s) <- colnames(Xtr)\n  colnames(Xte_s) <- colnames(Xtr)\n\n  list(Xtr = Xtr_s, Xte = Xte_s, mu = mu, sd = sd)\n}\n\n\nblock_mse <- function(beta_hat, beta_true, block){\n  tibble(\n    block = c(\"U\",\"V\",\"W\",\"Z\"),\n    coef_mse = c(\n      mean((beta_hat[block==\"U\"]-beta_true[block==\"U\"])^2),\n      mean((beta_hat[block==\"V\"]-beta_true[block==\"V\"])^2),\n      mean((beta_hat[block==\"W\"]-beta_true[block==\"W\"])^2),\n      mean((beta_hat[block==\"Z\"]-beta_true[block==\"Z\"])^2)\n    )\n  )\n}\n\ndrop_block_delta_mse <- function(Xte, yte, beta_hat, block){\n  yhat_full <- as.numeric(Xte %*% beta_hat)\n  mse_full  <- mean((yte - yhat_full)^2)\n  deltas <- map_dbl(c(\"U\",\"V\",\"W\",\"Z\"), function(b){\n    beta2 <- beta_hat\n    beta2[block==b] <- 0\n    mse2 <- mean((yte - as.numeric(Xte %*% beta2))^2)\n    mse2 - mse_full\n  })\n  tibble(block=c(\"U\",\"V\",\"W\",\"Z\"), delta_mse=deltas)\n}\n\nfit_one_split <- function(dat, test_frac=0.3, alpha_en=0.5){\n  X <- dat$X; y <- dat$y; beta_true <- dat$beta; block <- dat$block\n  n <- nrow(X)\n  id_te <- sample.int(n, size=floor(test_frac*n))\n  id_tr <- setdiff(seq_len(n), id_te)\n  \n  Xtr0 <- X[id_tr,]; ytr <- y[id_tr]\n  Xte0 <- X[id_te,]; yte <- y[id_te]\n  \n  sc <- scale_train_test(Xtr0, Xte0)\n  Xtr <- sc$Xtr; Xte <- sc$Xte\n  \n  out <- list()\n  \n  # ----- OLS -----\n  ols_fit <- lm(ytr ~ Xtr)\n  beta_ols <- coef(ols_fit)[-1]\n  yhat <- as.numeric(cbind(1,Xte) %*% coef(ols_fit))\n  out$ols <- list(beta=beta_ols, yhat=yhat, tune=NA)\n  \n  # ----- Ridge -----\n  cv_r <- cv.glmnet(Xtr, ytr, alpha=0, standardize=FALSE)\n  b_r <- as.numeric(coef(cv_r, s=\"lambda.min\"))[-1]\n  yhat_r <- as.numeric(predict(cv_r, newx=Xte, s=\"lambda.min\"))\n  out$ridge <- list(beta=b_r, yhat=yhat_r, tune=cv_r$lambda.min)\n  \n  # ----- Lasso -----\n  cv_l <- cv.glmnet(Xtr, ytr, alpha=1, standardize=FALSE)\n  b_l <- as.numeric(coef(cv_l, s=\"lambda.min\"))[-1]\n  yhat_l <- as.numeric(predict(cv_l, newx=Xte, s=\"lambda.min\"))\n  out$lasso <- list(beta=b_l, yhat=yhat_l, tune=cv_l$lambda.min)\n  \n  # ----- Elastic Net (alpha fixed) -----\n  cv_e <- cv.glmnet(Xtr, ytr, alpha=alpha_en, standardize=FALSE)\n  b_e <- as.numeric(coef(cv_e, s=\"lambda.min\"))[-1]\n  yhat_e <- as.numeric(predict(cv_e, newx=Xte, s=\"lambda.min\"))\n  out$enet <- list(beta=b_e, yhat=yhat_e, tune=cv_e$lambda.min)\n  \n  # ----- PCR -----\n  df_tr <- data.frame(y=ytr, Xtr)\n\n  pcr_fit <- pcr(y ~ ., data=df_tr, scale=FALSE, validation=\"CV\", segments=10)\n  # choose k by CV RMSEP\n  rmsep <- RMSEP(pcr_fit, estimate=\"CV\")\n\n  rm <- as.numeric(rmsep$val[1,1,-1, drop=TRUE])\n  se <- as.numeric(rmsep$se[1,1,-1, drop=TRUE])\n  \n  k_min <- which.min(rm)\n  thr <- rm[k_min] + se[k_min]\n  idx <- which(rm <= thr)\n  \n  k_best <- if(length(idx)==0) k_min else idx[1]\n  print(c(k_min=k_min, k_best=k_best))\n\n  df_te <- as.data.frame(Xte)  # must have same column names as training predictors\n  yhat_p <- as.numeric(predict(pcr_fit, newdata = df_te, ncomp = k_best))\n  \n  # coefficients in original X space (no intercept)\n  b_p <- as.numeric(coef(pcr_fit, ncomp = k_best, intercept = FALSE))\n  out$pcr <- list(beta = b_p, yhat = yhat_p, tune = k_best)\n\n  \n  # ---- assemble metrics tidy ----\n  methods <- names(out)\n  res_main <- map_dfr(methods, function(m){\n    bh <- out[[m]]$beta\n    mse <- mean((yte - out[[m]]$yhat)^2)\n    coefmse <- mean((bh - beta_true)^2)\n    tibble(method=m, test_mse=mse, coef_mse=coefmse,\n           tune=out[[m]]$tune,\n           l1=sum(abs(bh)), l2=sqrt(sum(bh^2)))\n  })\n  \n  res_coef_block <- map_dfr(methods, function(m){\n    block_mse(out[[m]]$beta, beta_true, block) %>%\n      mutate(method=m)\n  })\n  \n  res_drop <- map_dfr(methods, function(m){\n    drop_block_delta_mse(Xte, yte, out[[m]]$beta, block) %>%\n      mutate(method=m)\n  })\n  \n  # selection info (lasso + enet)\n  sel <- map_dfr(c(\"lasso\",\"enet\"), function(m){\n    bh <- out[[m]]$beta\n    tibble(method=m,\n           n_selected=sum(bh!=0),\n           nU=sum(bh[block==\"U\"]!=0),\n           nV=sum(bh[block==\"V\"]!=0),\n           nW=sum(bh[block==\"W\"]!=0),\n           nZ=sum(bh[block==\"Z\"]!=0),\n           selected=list(as.integer(bh!=0)))\n  })\n  \n    # ---- store beta_hat for all methods (for shrinkage + correlation behavior) ----\n  beta_hat_long <- map_dfr(methods, function(m){\n    tibble(\n      method = m,\n      j = seq_along(out[[m]]$beta),\n      beta_hat = out[[m]]$beta\n    )\n  })\n\n  list(main=res_main, coef_block=res_coef_block, drop=res_drop, sel=sel, beta_hat_long=beta_hat_long)\n\n}\n\njaccard <- function(a,b){\n  inter <- sum(a==1 & b==1)\n  uni   <- sum(a==1 | b==1)\n  if(uni==0) return(1)\n  inter/uni\n}\n\njaccard_restrict <- function(a, b, idx){\n  a <- a[idx]; b <- b[idx]\n  inter <- sum(a==1 & b==1)\n  uni   <- sum(a==1 | b==1)\n  if(uni==0) return(1)\n  inter/uni\n}\n\nget_jaccard_df <- function(sel_df_method, n_pairs=2000){\n  mats <- do.call(rbind, sel_df_method$selected)  # R x p\n  Rn <- nrow(mats)\n  pairs <- replicate(n_pairs, sample.int(Rn, 2))\n  js <- apply(pairs, 2, \\(ij) jaccard(mats[ij[1],], mats[ij[2],]))\n  tibble::tibble(jaccard = js)\n}\n\nget_jaccard_block_df <- function(sel_df_method, block_vec, n_pairs=2000){\n  mats <- do.call(rbind, sel_df_method$selected)  # R x p\n  Rn <- nrow(mats)\n  pairs <- replicate(n_pairs, sample.int(Rn, 2))\n  \n  blocks <- unique(block_vec)\n  purrr::map_dfr(blocks, function(bk){\n    idx <- which(block_vec == bk)\n    js <- apply(pairs, 2, \\(ij) jaccard_restrict(mats[ij[1],], mats[ij[2],], idx))\n    tibble::tibble(block=bk, jaccard=js)\n  })\n}\n```\n:::\n\n\n### Results\n\n\n\n#### Predictive performance and bias–variance trade-off\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\np_pred_overall <- ggplot(main_df, aes(x=method, y=test_mse, fill=method, color=method)) +\n  geom_violin(trim=FALSE, alpha=0.25, linewidth=0) +\n  geom_boxplot(width=0.15, outlier.shape=NA, alpha=0.55, color=\"black\") +\n  geom_point(alpha=0.25, position=position_jitter(width=0.10, height=0), size=1) +\n  scale_fill_manual(values=pal, labels=method_labels) +\n  scale_color_manual(values=pal, labels=method_labels) +\n  scale_x_discrete(labels=method_labels) +\n  labs(x=NULL, y=\"Test MSE\", title=\"Out-of-sample predictive accuracy (overall)\") +\n  theme_pub\n\np_pred_overall\n```\n\n::: {.cell-output-display}\n![](05-final_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\ndrop_df$block <- factor(drop_df$block, levels=c(\"U\",\"V\",\"W\",\"Z\"))\n\np_pred_block <- ggplot(drop_df, aes(x=method, y=delta_mse, fill=method, color=method)) +\n  geom_violin(trim=FALSE, alpha=0.25, linewidth=0) +\n  geom_boxplot(width=0.15, outlier.shape=NA, alpha=0.55, color=\"black\") +\n  facet_wrap(~block, nrow=1) +\n  scale_fill_manual(values=pal, labels=method_labels) +\n  scale_color_manual(values=pal, labels=method_labels) +\n  scale_x_discrete(labels=method_labels) +\n  labs(x=NULL, y=expression(Delta~\"MSE when dropping block\"),\n       title=\"Out-of-sample predictive contribution by block (drop-one-block ΔMSE)\") +\n  theme_pub\n\np_pred_block\n```\n\n::: {.cell-output-display}\n![](05-final_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n:::\n\nRegularized methods outperform OLS in out-of-sample prediction, illustrating the bias–variance trade-off in the presence of correlated predictors. While OLS is unbiased, its high variance leads to inferior generalization. Ridge reduces variance through uniform shrinkage but offers limited gains when noise variables are present. Lasso and Elastic Net achieve the best predictive performance by reducing effective model complexity through shrinkage and sparsity. PCR provides weaker improvements, indicating that dimension reduction alone is insufficient when signal is sparse at the variable level.\n\n#### Coefficient estimation accuracy\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\np_coef_overall <- ggplot(main_df, aes(x=method, y=coef_mse, fill=method, color=method)) +\n  geom_violin(trim=FALSE, alpha=0.25, linewidth=0) +\n  geom_boxplot(width=0.15, outlier.shape=NA, alpha=0.55, color=\"black\") +\n  geom_point(alpha=0.25, position=position_jitter(width=0.10, height=0), size=1) +\n  scale_fill_manual(values=pal, labels=method_labels) +\n  scale_color_manual(values=pal, labels=method_labels) +\n  scale_x_discrete(labels=method_labels) +\n  labs(x=NULL, y=\"Coefficient MSE\", title=\"Coefficient estimation error (overall)\") +\n  theme_pub\n\np_coef_overall\n```\n\n::: {.cell-output-display}\n![](05-final_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\ncoef_block_df$block <- factor(coef_block_df$block, levels=c(\"U\",\"V\",\"W\",\"Z\"))\n\np_coef_block <- ggplot(coef_block_df, aes(x=method, y=coef_mse, fill=method, color=method)) +\n  geom_violin(trim=FALSE, alpha=0.25, linewidth=0) +\n  geom_boxplot(width=0.15, outlier.shape=NA, alpha=0.55, color=\"black\") +\n  facet_wrap(~block, nrow=1) +\n  scale_fill_manual(values=pal, labels=method_labels) +\n  scale_color_manual(values=pal, labels=method_labels) +\n  scale_x_discrete(labels=method_labels) +\n  labs(x=NULL, y=\"Blockwise coefficient MSE\", title=\"Coefficient estimation error by block\") +\n  theme_pub\n\np_coef_block\n```\n\n::: {.cell-output-display}\n![](05-final_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n:::\n\nCoefficient estimation accuracy improves substantially under regularization. Lasso yields the lowest estimation error by eliminating weak and null coefficients, whereas Ridge introduces bias through uniform shrinkage of all coefficients. Elastic Net balances these effects, achieving moderate estimation error while improving stability. PCR performs poorly in coefficient recovery due to misalignment between principal components and the true sparse structure.\n\n#### Variable selection, sparsity, and stability\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\np_sel_stab_overall <- ggplot(jac_df, aes(x=method, y=jaccard, fill=method, color=method)) +\n  geom_violin(trim=FALSE, alpha=0.25, linewidth=0) +\n  geom_boxplot(width=0.15, outlier.shape=NA, alpha=0.55, color=\"black\") +\n  scale_fill_manual(values=pal_sel, labels=c(lasso=\"Lasso\", enet=\"Elastic Net\")) +\n  scale_color_manual(values=pal_sel, labels=c(lasso=\"Lasso\", enet=\"Elastic Net\")) +\n  scale_x_discrete(labels=c(lasso=\"Lasso\", enet=\"Elastic Net\")) +\n  labs(x=NULL, y=\"Jaccard similarity\",\n       title=\"Selection stability across resamples (overall)\") +\n  theme_pub\n\np_sel_stab_overall\n```\n\n::: {.cell-output-display}\n![](05-final_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nsel_long <- sel_df %>%\n  select(rep, method, n_selected, nU, nV, nW, nZ) %>%\n  pivot_longer(cols=c(n_selected,nU,nV,nW,nZ), names_to=\"block\", values_to=\"count\") %>%\n  mutate(block = recode(block, n_selected=\"Overall\", nU=\"U\", nV=\"V\", nW=\"W\", nZ=\"Z\"),\n         block = factor(block, levels=c(\"Overall\",\"U\",\"V\",\"W\",\"Z\")))\n\np_sel_stab_block <- ggplot(jac_block_df, aes(x=method, y=jaccard, fill=method, color=method)) +\n  geom_violin(trim=FALSE, alpha=0.25, linewidth=0) +\n  geom_boxplot(width=0.15, outlier.shape=NA, alpha=0.55, color=\"black\") +\n  facet_wrap(~block, nrow=1) +\n  scale_fill_manual(values=pal_sel, labels=c(lasso=\"Lasso\", enet=\"Elastic Net\")) +\n  scale_color_manual(values=pal_sel, labels=c(lasso=\"Lasso\", enet=\"Elastic Net\")) +\n  scale_x_discrete(labels=c(lasso=\"Lasso\", enet=\"Elastic Net\")) +\n  labs(x=NULL, y=\"Jaccard similarity\",\n       title=\"Selection stability across resamples (by block)\") +\n  theme_pub\n\np_sel_stab_block\n```\n\n::: {.cell-output-display}\n![](05-final_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\np_sel_count <- ggplot(sel_long, aes(x=method, y=count, fill=method, color=method)) +\n  geom_violin(trim=FALSE, alpha=0.25, linewidth=0) +\n  geom_boxplot(width=0.15, outlier.shape=NA, alpha=0.55, color=\"black\") +\n  facet_wrap(~block, nrow=1) +\n  scale_fill_manual(values=pal_sel, labels=c(lasso=\"Lasso\", enet=\"Elastic Net\")) +\n  scale_color_manual(values=pal_sel, labels=c(lasso=\"Lasso\", enet=\"Elastic Net\")) +\n  scale_x_discrete(labels=c(lasso=\"Lasso\", enet=\"Elastic Net\")) +\n  labs(x=NULL, y=\"# selected variables\",\n       title=\"Selection sparsity (counts) overall and by block\") +\n  theme_pub\n\np_sel_count\n```\n\n::: {.cell-output-display}\n![](05-final_files/figure-html/unnamed-chunk-8-3.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\njac_block_df$block <- factor(jac_block_df$block, levels=c(\"U\",\"V\",\"W\",\"Z\"))\n```\n:::\n\nLasso produces sparse but unstable variable selection, particularly in highly correlated predictor blocks. Elastic Net selects larger but more stable sets of variables by encouraging grouped selection among correlated predictors. This highlights a fundamental trade-off between sparsity and stability in penalized regression.\n\n#### Shrinkage behavior\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\np_shrink_overall <- ggplot(main_df, aes(x=method, y=l2, fill=method, color=method)) +\n  geom_violin(trim=FALSE, alpha=0.25, linewidth=0) +\n  geom_boxplot(width=0.15, outlier.shape=NA, alpha=0.55, color=\"black\") +\n  scale_fill_manual(values=pal, labels=method_labels) +\n  scale_color_manual(values=pal, labels=method_labels) +\n  scale_x_discrete(labels=method_labels) +\n  labs(x=NULL, y=expression(paste(\"||\", hat(beta), \"||\"[2])),\n       title=\"Shrinkage magnitude (overall L2 norm)\") +\n  theme_pub\n\np_shrink_overall\n```\n\n::: {.cell-output-display}\n![](05-final_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nblock_l2_df$block <- factor(block_l2_df$block, levels=c(\"U\",\"V\",\"W\",\"Z\"))\nblock_l2_df$method <- factor(block_l2_df$method, levels=method_levels)\n\np_shrink_block <- ggplot(block_l2_df, aes(x=method, y=block_l2, fill=method, color=method)) +\n  geom_violin(trim=FALSE, alpha=0.25, linewidth=0) +\n  geom_boxplot(width=0.15, outlier.shape=NA, alpha=0.55, color=\"black\") +\n  facet_wrap(~block, nrow=1) +\n  scale_fill_manual(values=pal, labels=method_labels) +\n  scale_color_manual(values=pal, labels=method_labels) +\n  scale_x_discrete(labels=method_labels) +\n  labs(x=NULL, y=expression(paste(\"||\", hat(beta)[block], \"||\"[2])),\n       title=\"Shrinkage magnitude by block (L2 norm)\") +\n  theme_pub\n\np_shrink_block\n```\n\n::: {.cell-output-display}\n![](05-final_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n# need beta_hat_df + true beta vector and block vector\nbeta_true_df <- tibble(j=seq_along(dat$beta), beta_true=dat$beta, block=dat$block)\n\nbeta_mean_df <- beta_hat_df %>%\n  group_by(method, j, block) %>%\n  summarise(beta_hat_mean = mean(beta_hat), .groups=\"drop\") %>%\n  left_join(beta_true_df, by=c(\"j\",\"block\"))\n\np_pattern_scatter <- ggplot(beta_mean_df, aes(x=beta_true, y=beta_hat_mean, color=block)) +\n  geom_hline(yintercept=0, linewidth=0.3, alpha=0.5) +\n  geom_vline(xintercept=0, linewidth=0.3, alpha=0.5) +\n  geom_point(alpha=0.7, size=1.4) +\n  facet_wrap(~method, nrow=1, labeller=as_labeller(method_labels)) +\n  labs(x=expression(beta[j]~\"(true)\"),\n       y=expression(bar(hat(beta))[j]~\"(mean over resamples)\"),\n       title=\"Shrinkage pattern: true vs estimated coefficients (colored by block)\") +\n  theme_pub\n\np_pattern_scatter\n```\n\n::: {.cell-output-display}\n![](05-final_files/figure-html/unnamed-chunk-9-3.png){width=672}\n:::\n:::\n\nShrinkage behavior differs markedly across methods: Ridge applies strong global shrinkage, Lasso induces exact zeros, Elastic Net combines both mechanisms, and PCR lacks direct shrinkage in the original predictor space. In highly correlated blocks, OLS and PCR exhibit substantial coefficient instability, Ridge stabilizes estimates via shrinkage, and Elastic Net mitigates instability through grouped selection, whereas Lasso remains sensitive to correlation structure.\n\n#### Effects of correlated predictors\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbeta_sd_var$block <- factor(beta_sd_var$block, levels=c(\"U\",\"V\",\"W\",\"Z\"))\nbeta_sd_var$method <- factor(beta_sd_var$method, levels=method_levels)\n\np_corr_behavior <- ggplot(beta_sd_var, aes(x=block, y=sd_beta, fill=block)) +\n  geom_boxplot(outlier.shape=NA, alpha=0.55) +\n  geom_jitter(alpha=0.20, width=0.15, size=1) +\n  facet_wrap(~method, nrow=1, labeller=as_labeller(method_labels)) +\n  labs(x=\"Block\", y=expression(sd(hat(beta)[j])),\n       title=\"Effect of correlation: coefficient instability by block\") +\n  theme_pub +\n  theme(legend.position=\"none\")\n\np_corr_behavior\n```\n\n::: {.cell-output-display}\n![](05-final_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\nCorrelation among predictors has a pronounced impact on model behavior. In highly correlated blocks, OLS and PCR exhibit substantial coefficient instability due to near-nonidentifiability of the regression problem.\n\n### Summary\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(kableExtra)\nsummary_table <- main_df %>%\n  group_by(method) %>%\n  summarise(\n    `Test MSE` = mean(test_mse),\n    `Coef MSE` = mean(coef_mse),\n    `||β̂||₂` = mean(l2),\n    .groups = \"drop\"\n  )\nknitr::kable(\n  summary_table,\n  digits = 3,\n  caption = \"Overall predictive and estimation performance across methods\",\n  col.names = c(\"Method\", \"Test MSE\", \"Coefficient MSE\", \"L2 Norm of β̂\")\n)\n```\n\n::: {.cell-output-display}\n\n\nTable: Overall predictive and estimation performance across methods\n\n|Method | Test MSE| Coefficient MSE| L2 Norm of β̂|\n|:------|--------:|---------------:|------------:|\n|ols    |    1.570|           0.049|        4.267|\n|ridge  |    1.662|           0.085|        2.878|\n|lasso  |    1.223|           0.012|        3.660|\n|enet   |    1.277|           0.024|        3.416|\n|pcr    |    1.581|           0.055|        4.189|\n\n\n:::\n:::\n\n| Method        | Prediction | Coefficient Accuracy | Sparsity | Selection Stability | Correlated Predictors | Key Trade-off |\n|---------------|------------|----------------------|----------|---------------------|-----------------------|---------------|\n| OLS           | Poor       | Moderate             | None     | N/A                 | Very unstable         | Unbiased but high variance |\n| Ridge         | Moderate   | Poor–Moderate        | None     | N/A                 | Stable shrinkage      | Variance reduction without sparsity |\n| Lasso         | Best       | Best                 | High     | Low                 | Unstable selection    | Sparsity vs stability |\n| Elastic Net   | Best       | Moderate–Good        | Moderate | High                | Grouped selection     | Balanced regularization |\n| PCR           | Moderate   | Poor                 | Implicit | N/A                 | Depends on alignment  | Dimension reduction vs sparsity |\n\nThis exercise highlights how different regularization strategies trade off bias, variance, sparsity, and stability in a structured, correlated predictor setting.\n\nOverall, methods that explicitly impose sparsity—lasso and elastic net—achieve superior out-of-sample predictive accuracy and substantially lower coefficient estimation error compared to unregularized OLS and dimension-reduction-based PCR. In particular, lasso performs best in terms of both prediction and estimation when the true signal is sparse, as it effectively suppresses noise variables while retaining the dominant coefficients.\n\nRidge regression exhibits the strongest overall shrinkage and the most stable coefficient estimates under high within-block correlation, but this comes at the cost of higher estimation bias and weaker variable discrimination. Elastic net balances these behaviors by combining $\\ell_1$-induced sparsity with $\\ell_2$-based stabilization, leading to improved selection stability relative to lasso while maintaining competitive predictive performance.\n\nThe blockwise analyses further clarify the interaction between correlation structure and regularization. Highly correlated blocks benefit from shrinkage-based methods, while sparsity-driven methods more accurately identify truly active predictors within moderately correlated blocks. Blocks containing no signal are consistently downweighted, especially by lasso and elastic net, demonstrating effective regularization against overfitting.\n\nFinally, selection stability results show that elastic net produces more consistent variable selection across resamples than lasso, particularly in correlated blocks, underscoring the practical importance of combining sparsity and grouping effects.\n\nTaken together, these results emphasize that no single regularization method is uniformly optimal. Instead, the appropriate choice depends on the underlying signal structure, correlation patterns, and the relative importance of prediction accuracy, interpretability, and stability.\n\n\n## Question 2\n\n### Data and setup\n\nWe consider three data-generating settings to examine regularization and degrees of freedom under different modeling regimes. Two of the settings follow the simulation designs of Mentch and Zhou (2020), while the third corresponds exactly to the block-correlated linear model used in Q1. Model performance is evaluated using out-of-sample mean squared error (MSE).\n\n---\n\n#### Linear Medium setting (Mentch and Zhou, 2020)\n\nWe consider a sparse linear regression model with\n$$\nn = 500, \\quad p = 100, \\quad s = 5.\n$$\nThe response is generated according to\n$$\nY = X\\beta + \\varepsilon,\n$$\nwhere\n$$\n\\beta_j =\n\\begin{cases}\n1, & j = 1,\\dots,s, \\\\\n0, & j = s+1,\\dots,p,\n\\end{cases}\n\\quad\n\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2).\n$$\nThe noise variance $\\sigma^2$ is chosen to achieve a fixed signal-to-noise ratio,\n$$\n\\mathrm{SNR} = \\frac{\\mathrm{Var}(X\\beta)}{\\sigma^2}.\n$$\nThis setting represents a regime in which the linear model is correctly specified.\n\n---\n\n#### MARSadd setting (Mentch and Zhou, 2020)\n\nTo study a nonlinear data-generating mechanism, we consider the MARSadd model with\n$$\nn = 500, \\quad p = s = 5,\n$$\nwhere predictors are sampled independently from $\\mathrm{Unif}(0,1)$\\)$. The response is generated as\n$$\nY\n= 0.1 \\frac{e^{4X_1}}{1 + e^{-20(X_2 - 0.5)}}\n+ 3X_3 + 2X_4 + X_5\n+ \\varepsilon,\n\\quad\n\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2),\n$$\nwith $\\sigma^2$ chosen to control the signal-to-noise ratio. This setting introduces nonlinear and interaction effects that cannot be represented by linear models.\n\n\n#### Block-correlated linear setting (Q1)\n\nThe third setting follows exactly the block-correlated linear data-generating process used in **Q1**. Predictors are partitioned into blocks with different within-block correlation structures, and the response is generated according to a sparse linear model with additive Gaussian noise. All aspects of this setting are identical to Q1 and are not repeated here.\n\n\n#### Train–test split\n\nFor each setting, the data are randomly split into training and test sets. Models are fit on the training data, and predictive performance is evaluated on the test data using mean squared error. Results are averaged over repeated fits at each regularization level to reduce Monte Carlo variability.\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(glmnet)\nlibrary(ranger)\n\ngen_linear_medium <- function(n = 500, p = 100, s = 5, snr = 1, seed = NULL) {\n  if (!is.null(seed)) set.seed(seed)\n\n  X <- matrix(runif(n * p), n, p)   # same \"Unif(0,1)\" style\n  beta <- c(rep(1, s), rep(0, p - s))\n  f <- as.numeric(X %*% beta)\n\n  # choose sigma^2 to hit target SNR = Var(f)/sigma^2\n  sigma2 <- var(f) / snr\n  y <- f + rnorm(n, sd = sqrt(sigma2))\n\n  list(X = X, y = y, name = sprintf(\"Linear Medium (n=%d,p=%d,s=%d,SNR=%g)\", n, p, s, snr))\n}\n\ngen_marsadd <- function(n = 500, snr = 1, seed = NULL) {\n  if (!is.null(seed)) set.seed(seed)\n\n  p <- 5\n  X <- matrix(runif(n * p), n, p)\n\n  # One common \"MARSadd\" style nonlinear signal (matches the spirit of the paper)\n  f <- 0.1 * exp(4 * X[, 1]) / (1 + exp(-20 * (X[, 2] - 0.5))) +\n       3 * X[, 3] + 2 * X[, 4] + 1 * X[, 5]\n\n  sigma2 <- var(f) / snr\n  y <- f + rnorm(n, sd = sqrt(sigma2))\n\n  list(X = X, y = y, name = sprintf(\"MARSadd (n=%d,p=5,SNR=%g)\", n, snr))\n}\n```\n:::\n\n### Methods\n\nWe compare ridge regression and random forests under varying levels of regularization. For each method, we examine how regularization affects effective degrees of freedom and out-of-sample predictive accuracy.\n\n---\n\n#### Ridge regression\n\nRidge regression estimates regression coefficients by solving\n$$\n\\hat{\\beta}_\\lambda\n= \\arg\\min_{\\beta}\n\\left\\{\n\\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_2^2\n\\right\\},\n$$\nwhere $\\lambda > 0$ is the regularization parameter. Larger values of $\\lambda$ impose stronger shrinkage and reduce model complexity.\n\nFor ridge regression, the effective degrees of freedom admit a closed-form expression. Let $d_1,\\dots,d_p$ denote the singular values of the standardized design matrix $X$. The degrees of freedom are given by\n$$\n\\mathrm{DoF}_{\\text{ridge}}(\\lambda)\n= \\sum_{j=1}^p \\frac{d_j^2}{d_j^2 + \\lambda}.\n$$\nThis quantity can be interpreted as the effective number of parameters used by the model.\n\n#### Random forests\n\nRandom forests are ensembles of decision trees constructed using bootstrap samples and randomized splitting rules. Following Mentch and Zhou (2020), we treat the number of candidate predictors considered at each split, denoted `mtry`, as a regularization parameter. Smaller values of `mtry` restrict the set of allowable splits and lead to stronger regularization, while larger values increase model flexibility.\n\nDegrees of freedom for random forests are defined in terms of the sensitivity of fitted values to perturbations in the response:\n$$\n\\mathrm{DoF}_{\\text{RF}}\n= \\sum_{i=1}^n \\frac{\\partial \\hat{y}_i}{\\partial y_i}.\n$$\nBecause this quantity is not available in closed form, it is estimated using a Monte Carlo perturbation approach. Specifically, for a small perturbation vector $\\omega$,\n$$\n\\mathrm{DoF}_{\\text{RF}}\n\\approx \\mathbb{E}\\left[\n\\frac{(\\hat{f}(y+\\omega) - \\hat{f}(y))^\\top \\omega}{\\|\\omega\\|_2^2}\n\\right],\n$$\nwhere the expectation is approximated by repeated random perturbations.\n\n---\n\n#### Model evaluation\n\nFor each data-generating setting, models are fit on a training set and evaluated on a held-out test set. Predictive accuracy is measured using test mean squared error (MSE). To reduce Monte Carlo variability arising from random forest fitting and the estimation of degrees of freedom, model fitting is repeated multiple times at each regularization level, and results are summarized using average test MSE and average degrees of freedom.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nridge_df <- function(X, lambda) {\n  Xs <- scale(X, center = TRUE, scale = TRUE)\n  d  <- svd(Xs, nu = 0, nv = 0)$d\n  sum(d^2 / (d^2 + lambda))\n}\n\nrf_df_mc <- function(X, y, mtry, num.trees = 50, tau = 0.1 * sd(y), B = 15) {\n  dat0 <- data.frame(y = y, X)\n  fit0 <- ranger(y ~ ., data = dat0, num.trees = num.trees, mtry = mtry)\n  yhat0 <- predict(fit0, data = dat0)$predictions\n\n  vals <- replicate(B, {\n    omega <- rnorm(length(y), sd = tau)\n    datb <- data.frame(y = y + omega, X)\n    fitb <- ranger(y ~ ., data = datb, num.trees = num.trees, mtry = mtry)\n    yhatb <- predict(fitb, data = dat0)$predictions\n    sum((yhatb - yhat0) * omega) / (tau^2)\n  })\n\n  mean(vals)\n}\nrun_setting_simple <- function(dat,\n                               test_frac = 0.3,\n                               lambda_grid = 10^seq(-4, 4, length.out = 30),\n                               num.trees = 200,\n                               mtry_grid = NULL,\n                               B = 20,\n                               seed = 123) {\n\n  set.seed(seed)\n  X <- dat$X; y <- dat$y\n  n <- nrow(X); p <- ncol(X)\n\n  # one fixed split\n  idx <- sample(seq_len(n), size = floor((1 - test_frac) * n))\n  Xtr <- X[idx, , drop = FALSE]; ytr <- y[idx]\n  Xte <- X[-idx, , drop = FALSE]; yte <- y[-idx]\n\n  if (is.null(mtry_grid)) {\n    mtry_grid <- unique(round(c(1, 2, 3, 5, 8, 12, sqrt(p), p)))\n    mtry_grid <- mtry_grid[mtry_grid >= 1 & mtry_grid <= p]\n  }\n\n  tau0 <- 0.1 * sd(ytr)\n\n  # ---- Ridge (simple loop) ----\n  ridge_res <- purrr::map_dfr(lambda_grid, function(lam) {\n    fit  <- glmnet::glmnet(Xtr, ytr, alpha = 0, lambda = lam, standardize = TRUE)\n    pred <- as.numeric(predict(fit, newx = Xte, s = lam))\n\n    tibble::tibble(\n      method    = \"Ridge\",\n      reg_param = lam,\n      df        = ridge_df(Xtr, lam),\n      test_mse  = mean((yte - pred)^2)\n    )\n  })\n\n  # ---- Random Forest (simple loop) ----\n  rf_res <- purrr::map_dfr(mtry_grid, function(mtry_val) {\n    dat_tr <- data.frame(y = ytr, Xtr)\n    dat_te <- data.frame(Xte)\n\n    fit  <- ranger::ranger(y ~ ., data = dat_tr,\n                           num.trees = num.trees, mtry = mtry_val)\n    pred <- predict(fit, data = dat_te)$predictions\n\n    df_hat <- rf_df_mc(\n      X = as.data.frame(Xtr), y = ytr,\n      mtry = mtry_val, num.trees = num.trees,\n      tau = tau0, B = B\n    )\n\n    tibble::tibble(\n      method    = \"Random Forest\",\n      reg_param = mtry_val,\n      df        = df_hat,\n      test_mse  = mean((yte - pred)^2)\n    )\n  })\n\n  dplyr::bind_rows(ridge_res, rf_res) %>%\n    dplyr::mutate(setting = dat$name, p = p)\n}\n```\n:::\n\n\n### Regularization behavior\n\nWe examine how regularization parameters in ridge regression and random forests map to effective degrees of freedom, following the framework of Mentch and Zhou (2020).\n\n---\n\n#### Ridge regression\n\nIn ridge regression, regularization is controlled by the penalty parameter $\\lambda > 0$.  \nThe effective degrees of freedom are given by\n\n$$\n\\mathrm{DoF}_{\\text{ridge}}(\\lambda)\n= \\sum_{j=1}^p \\frac{d_j^2}{d_j^2 + \\lambda},\n$$\n\nwhere $d_1, \\dots, d_p$ denote the singular values of the standardized design matrix.\n\nAs $\\lambda \\to 0$, the penalty vanishes and\n\n$$\n\\mathrm{DoF}_{\\text{ridge}}(\\lambda) \\to \\mathrm{rank}(X),\n$$\n\nwhile as $\\lambda \\to \\infty$,\n\n$$\n\\mathrm{DoF}_{\\text{ridge}}(\\lambda) \\to 0.\n$$\n\nThus, ridge regression spans a wide and continuous range of effective model complexity, enabling smooth traversal of the bias–variance trade-off.\n\n#### Random forests\n\nFor random forests, regularization is induced by restricting the number of candidate predictors available at each split. Following Mentch and Zhou (2020), we parameterize `mtry` as a proportion of eligible predictors,\n\n$$\n\\text{mtry} = \\alpha p, \\quad \\alpha \\in (0,1],\n$$\n\nwhere $p$ denotes the total number of predictors and $\\alpha$ controls the strength of regularization. Smaller values of $\\alpha$ correspond to stronger regularization, while $\\alpha = 1$ recovers the fully greedy splitting rule.\n\nDegrees of freedom for random forests are defined as the sensitivity of fitted values to perturbations in the response,\n\n$$\n\\mathrm{DoF}_{\\text{RF}}\n= \\sum_{i=1}^n \\frac{\\partial \\hat{y}_i}{\\partial y_i}.\n$$\n\nBecause this quantity is not available in closed form, it is estimated using a Monte Carlo perturbation approach,\n\n$$\n\\mathrm{DoF}_{\\text{RF}}\n\\approx \\mathbb{E}\\left[\n\\frac{\\left(\\hat{f}(y + \\omega) - \\hat{f}(y)\\right)^\\top \\omega}{\\|\\omega\\|_2^2}\n\\right],\n$$\n\nwhere $\\omega$ is a small random perturbation vector and the expectation is approximated empirically.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndat_lin  <- gen_linear_medium(n = 500, p = 100, s = 5, snr = 1, seed = 1)\ndat_lin$name <- \"Linear\"\n\ndat_mars <- gen_marsadd(n = 500, snr = 1, seed = 1)\ndat_mars$name <- \"MARS-additive\"\n\ndat_bc <- gen_data(200)\ndat_bc$name <- \"Block-correlated\"\nres_lin  <- run_setting_simple(dat_lin,  seed = 123)\nres_mars <- run_setting_simple(dat_mars, seed = 123)\nres_bc   <- run_setting_simple(dat_bc,   seed = 123)\n\nall_res <- dplyr::bind_rows(res_lin, res_mars, res_bc)\n```\n:::\n\n\n#### Comparison of regularization effects\n\nAlthough both $\\lambda$ and $\\alpha$ act as regularization parameters, they induce fundamentally different complexity scales. The ridge penalty directly shrinks the linear coefficient vector and allows the model to access both low- and high-complexity regimes. In contrast, varying $\\alpha$ modulates randomness and adaptivity in random forests but does not collapse the model into a globally smooth estimator. As a result, the range of achievable degrees of freedom for random forests is typically narrower and concentrated in a higher-complexity regime than that of ridge regression.\n\n### Results\n\nWe summarize the relationship between degrees of freedom and out-of-sample predictive accuracy for ridge regression and random forests across the three data-generating settings.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(all_res, aes(x = df, y = test_mse, color = method, shape = method)) +\n  geom_point(size = 2.8) +\n  facet_wrap(~ setting, scales = \"free_x\") +\n  theme_minimal(base_size = 13) +\n  labs(\n    x = \"Degrees of freedom\",\n    y = \"Out-of-sample predictive error (Test MSE)\",\n    color = \"Method\",\n    shape = \"Method\"\n  )\n```\n\n::: {.cell-output-display}\n![](05-final_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\nAcross all three settings, ridge regression and random forests exhibit distinct and largely non-overlapping degrees-of-freedom regimes. Ridge regression spans a wide range of complexity levels and adapts smoothly to the underlying structure when the linear model is appropriate. Random forests, even under strong regularization via `mtry`, operate in a higher-complexity regime and display a more limited range of effective degrees of freedom.\n\n### Discussion & Conclusion\n\nComparing models at equivalent degrees of freedom is conceptually appealing because it aims to control for model complexity, i.e.,\n$$\n\\mathrm{DoF}_A \\approx \\mathrm{DoF}_B\n\\;\\Rightarrow\\;\n\\text{compare test error at “similar complexity.”}\n$$\nIn practice, our results suggest this is often not a meaningful comparison between ridge regression and random forests.\n\nFirst, matched degrees of freedom are frequently **unattainable**. Ridge spans a broad complexity range via $\\lambda$,\n$$\n\\mathrm{DoF}_{\\text{ridge}}(\\lambda)=\\sum_{j=1}^p \\frac{d_j^2}{d_j^2+\\lambda},\n\\qquad\n\\mathrm{DoF}_{\\text{ridge}}(\\lambda)\\in(0,\\mathrm{rank}(X)],\n$$\nwhereas random forests regularized through\n$$\n\\text{mtry}=\\alpha p,\\quad \\alpha\\in(0,1]\n$$\ntend to remain in a comparatively high degrees-of-freedom regime under the Mentch–Zhou definition\n$$\n\\mathrm{DoF}_{\\text{RF}}=\\sum_{i=1}^n \\frac{\\partial \\hat{y}_i}{\\partial y_i}.\n$$\nAcross all three settings, the achievable DoF ranges show little overlap, making “matched-DoF” comparisons infeasible.\n\nSecond, even if DoF could be matched, it would not guarantee comparable **expressiveness**. Ridge remains linear,\n$$\n\\hat f_{\\text{ridge}}(x)=x^\\top \\hat\\beta_\\lambda,\n$$\nso under nonlinear data-generating mechanisms (e.g., MARSadd), increasing DoF within the linear class cannot remove misspecification:\n$$\n\\inf_{\\beta}\\mathbb{E}\\big[(f^*(X)-X^\\top\\beta)^2\\big] > 0.\n$$\n\nOverall, degrees of freedom are a useful within-method complexity diagnostic, but they are not a generally reliable basis for cross-model comparison between ridge regression and random forests.\n",
    "supporting": [
      "05-final_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}